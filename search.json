[{"title":"docker(1)-基础","url":"/2024/12/31/docker-1--基础/","content":"docker 不多说就是拥有独立的存储空间的更加优秀的类虚拟机系统，他拥有许多优秀的思想，比如吸收了git的存储仓库的思想，使用image参照git中的仓库等。 docker 之分层存储Docker 设计时，就充分利用 Union FS 的技术，将其设计为分层存储的架构。所以严格来说，镜像并非是像一个 ISO 那样的打包文件，镜像只是一个虚拟的概念，其实际体现并非由一个文件组成，而是由一组文件系统组成，或者说，由多层文件系统联合组成。 镜像构建时，会一层层构建，前一层是后一层的基础。每一层构建完就不会再发生改变，后一层上的任何改变只发生在自己这一层。比如，删除前一层文件的操作，实际不是真的删除前一层的文件，而是仅在当前层标记为该文件已删除。在最终容器运行的时候，虽然不会看到这个文件，但是实际上该文件会一直跟随镜像。因此，在构建镜像的时候，需要额外小心，每一层尽量只包含该层需要添加的东西，任何额外的东西应该在该层构建结束前清理掉。 分层存储的特征还使得镜像的复用、定制变的更为容易。甚至可以用之前构建好的镜像作为基础层，然后进一步添加新的层，以定制自己所需的内容，构建新的镜像。 docker 之容器镜像（Image）和容器（Container）的关系，就像是面向对象程序设计中的 类 和 实例 一样，镜像是静态的定义，容器是镜像运行时的实体。容器可以被创建、启动、停止、删除、暂停等。 容器的实质是进程，但与直接在宿主执行的进程不同，容器进程运行于属于自己的独立的 命名空间。因此容器可以拥有自己的 root 文件系统、自己的网络配置、自己的进程空间，甚至自己的用户 ID 空间。容器内的进程是运行在一个隔离的环境里，使用起来，就好像是在一个独立于宿主的系统下操作一样。这种特性使得容器封装的应用比直接在宿主运行更加安全。也因为这种隔离的特性，很多人初学 Docker 时常常会混淆容器和虚拟机。 前面讲过镜像使用的是分层存储，容器也是如此。每一个容器运行时，是以镜像为基础层，在其上创建一个当前容器的存储层，我们可以称这个为容器运行时读写而准备的存储层为容器存储层。 容器存储层的生存周期和容器一样，容器消亡时，容器存储层也随之消亡。因此，任何保存于容器存储层的信息都会随容器删除而丢失。 按照 Docker 最佳实践的要求，容器不应该向其存储层内写入任何数据，容器存储层要保持无状态化。所有的文件写入操作，都应该使用 数据卷（Volume）、或者绑定宿主目录，在这些位置的读写会跳过容器存储层，直接对宿主（或网络存储）发生读写，其性能和稳定性更高。 数据卷的生存周期独立于容器，容器消亡，数据卷不会消亡。因此，使用数据卷后，容器删除或者重新运行之后，数据却不会丢失。 docker 之仓库docker仓库的功能和git仓库类似这里就不展开说明了，这里主要讨论一下docker仓库特殊的命名规则。 以 Ubuntu 镜像 为例，ubuntu 是仓库的名字，其内包含有不同的版本标签，如，14.04, 16.04。我们可以通过 ubuntu:14.04，或者 ubuntu:16.04 来具体指定所需哪个版本的镜像。如果忽略了标签，比如 ubuntu，那将视为 ubuntu:latest。 仓库名经常以 两段式路径 形式出现，比如 jwilder&#x2F;nginx-proxy，前者往往意味着 Docker Registry 多用户环境下的用户名，后者则往往是对应的软件名。但这并非绝对，取决于所使用的具体 Docker Registry 的软件或服务。","date":"2024-12-31","categories":["docker"]},{"title":"docker(2)-镜像使用和命令","url":"/2024/12/31/docker-2--镜像使用和命令/","content":"获取镜像 pull关键命令 命名格式 Docker 镜像仓库地址：地址的格式一般是 <域名&#x2F;IP>[:端口号]。默认地址是 Docker Hub。 仓库名：如之前所说，这里的仓库名是两段式名称，即 <用户名>&#x2F;<软件名>。对于 Docker Hub，如果不给出用户名，则默认为 library，也就是官方镜像。 运行镜像 run镜像格式 启动容器有两种方式，一种是基于镜像新建一个容器并启动，另外一个是将在终止状态（stopped）的容器重新启动。 因为 Docker 的容器实在太轻量级了，很多时候用户都是随时删除和新创建容器。 OPTIONS说明： -a stdin: 指定标准输入输出内容类型，可选 STDIN&#x2F;STDOUT&#x2F;STDERR 三项； -d: 后台运行容器，并返回容器ID； -i: 以交互模式运行容器，通常与 -t 同时使用； -t: 为容器重新分配一个伪输入终端，通常与 -i 同时使用； –name&#x3D;”nginx-lb”: 为容器指定一个名称； –dns 8.8.8.8: 指定容器使用的DNS服务器，默认和宿主一致； –dns-search example.com: 指定容器DNS搜索域名，默认和宿主一致； -h “mars”: 指定容器的hostname； -e username&#x3D;”ritchie”: 设置环境变量； –env-file&#x3D;[]: 从指定文件读入环境变量； –cpuset&#x3D;”0-2” or –cpuset&#x3D;”0,1,2”: 绑定容器到指定CPU运行； -m :设置容器使用内存最大值； –net&#x3D;”bridge”: 指定容器的网络连接类型，支持 bridge&#x2F;host&#x2F;none&#x2F;container: 四种类型； –link&#x3D;[]: 添加链接到另一个容器； –expose&#x3D;[]: 开放一个端口或一组端口； 当利用 docker run 来创建容器时，Docker 在后台运行的标准操作包括： 检查本地是否存在指定的镜像，不存在就从公有仓库下载 利用镜像创建并启动一个容器 分配一个文件系统，并在只读的镜像层外面挂载一层可读写层 从宿主主机配置的网桥接口中桥接一个虚拟接口到容器中去 从地址池配置一个 ip 地址给容器 执行用户指定的应用程序 执行完毕后容器被终止 docker container start列出镜像 image ls 打印输出所有的镜像 查看docker 磁盘的使用情况 注意：这里引申一下docker image 命令显示出来的各种信息，docker image ls 列表中的镜像体积总和并非是所有镜像实际硬盘消耗。由于 Docker 镜像是多层存储结构，并且可以继承、复用，因此不同镜像可能会因为使用相同的基础镜像，从而拥有共同的层。由于 Docker 使用 Union FS，相同的层只需要保存一份即可，因此实际镜像硬盘占用空间很可能要比这个列表镜像大小的总和要小的多。 特殊的玄虚镜像由于新旧镜像同名，旧镜像名称被取消等，出现仓库名、标签均为 的镜二维码像。这类无标签镜像也被称为 虚悬镜像(dangling image) 使用这个命令将会直接的看到这些镜像 一般来说，虚悬镜像已经失去了存在的价值，是可以随意删除的，可以用下面的命令删除。 中间层镜像默认的 docker image ls 列表中只会显示顶层镜像，如果希望显示包括中间层镜像在内的所有镜像的话，需要加 -a 参数。 这样会看到很多无标签的镜像，与之前的虚悬镜像不同，这些无标签的镜像很多都是中间层镜像，是其它镜像所依赖的镜像。这些无标签镜像不应该删除，否则会导致上层镜像因为依赖丢失而出错。实际上，这些镜像也没必要删除，因为之前说过，相同的层只会存一遍，而这些镜像是别的镜像的依赖，因此并不会因为它们被列出来而多存了一份，无论如何你也会需要它们。只要删除那些依赖它们的镜像后，这些依赖的中间层镜像也会被连带删除。 列出部分镜像不加任何参数的情况下，docker image ls 会列出所有顶级镜像，但是有时候我们只希望列出部分镜像。docker image ls 有好几个参数可以帮助做到这个事情。 根据仓库名列出镜像 显示镜像详细信息 这个方法将会显示出这个镜像的长id等信息 列出特定的某个镜像，也就是说指定仓库名和标签 docker image ls 还支持强大的过滤器参数 –filter，或者简写 -f 比如，我们希望看到在 mongo:3.2 之后建立的镜像，可以用下面的命令： 想查看某个位置之前的镜像也可以，只需要把 since 换成 before 即可。 利用 docker image ls 把所有的虚悬镜像的 ID 列出来 配合使用go 模板语法下面的命令会直接列出镜像结果，并且只包含镜像ID和仓库名： 或者打算以表格等距显示，并且有标题行，和默认一样，不过自己定义列： 删除本地镜像用 ID、镜像名、摘要删除镜像如果要删除本地的镜像，可以使用 docker image rm 命令，其格式为： <镜像> 可以是 镜像短 ID、镜像长 ID、镜像名(<仓库名>:<标签>) 或者 镜像摘要 引申： 短id 其实就是docker image ls 展示的相关的id Untagged 和 Deleted首先观察一个删除的命令和结果 删除行为分为两类，一类是 Untagged，另一类是 Deleted。我们之前介绍过，镜像的唯一标识是其 ID 和摘要，而一个镜像可以有多个标签。 因此当我们使用上面命令删除镜像的时候，实际上是在要求删除某个标签的镜像。所以首先需要做的是将满足我们要求的所有镜像标签都取消，这就是我们看到的 Untagged 的信息。因为一个镜像可以对应多个标签，因此当我们删除了所指定的标签后，可能还有别的标签指向了这个镜像，如果是这种情况，那么 Delete 行为就不会发生。所以并非所有的 docker image rm 都会产生删除镜像的行为，有可能仅仅是取消了某个标签而已。 当该镜像所有的标签都被取消了，该镜像很可能会失去了存在的意义，因此会触发删除行为。镜像是多层存储结构，因此在删除的时候也是从上层向基础层方向依次进行判断删除。镜像的多层结构让镜像复用变动非常容易，因此很有可能某个其它镜像正依赖于当前镜像的某一层。这种情况，依旧不会触发删除该层的行为。直到没有任何层依赖当前层时，才会真实的删除当前层。这就是为什么，有时候会奇怪，为什么明明没有别的标签指向这个镜像，但是它还是存在的原因，也是为什么有时候会发现所删除的层数和自己 docker pull 看到的层数不一样的源。 除了镜像依赖以外，还需要注意的是容器对镜像的依赖。如果有用这个镜像启动的容器存在（即使容器没有运行），那么同样不可以删除这个镜像。之前讲过，容器是以镜像为基础，再加一层容器存储层，组成这样的多层存储结构去运行的。因此该镜像如果被这个容器所依赖的，那么删除必然会导致故障。如果这些容器是不需要的，应该先将它们删除，然后再来删除镜像。 用 docker image ls 命令来配合像其它可以承接多个实体的命令一样，可以使用 docker image ls -q 来配合使用 docker image rm，这样可以成批的删除希望删除的镜像。我们在“镜像列表”章节介绍过很多过滤镜像列表的方式都可以拿过来使用。 比如，我们需要删除所有仓库名为 redis 的镜像： 或者删除所有在 mongo:3.2 之前的镜像： 充分利用你的想象力和 Linux 命令行的强大，你可以完成很多非常赞的功能。 docker commit 命令和相关的问题docker commit 的语法格式为： 可以使用如下的方法将指定的容器打包成镜像 引申：其中 –author 是指定修改的作者，而 –message 则是记录本次修改的内容。这点和 git 版本控制相似，不过这里这些信息可以省略留空。 使用 docker commit 命令虽然可以比较直观的帮助理解镜像分层存储的概念，但是实际环境中并不会这样使用。 首先，如果仔细观察之前的 docker diff webserver 的结果，你会发现除了真正想要修改的 &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html&#x2F;index.html 文件外，由于命令的执行，还有很多文件被改动或添加了。这还仅仅是最简单的操作，如果是安装软件包、编译构建，那会有大量的无关内容被添加进来，如果不小心清理，将会导致镜像极为臃肿。 此外，使用 docker commit 意味着所有对镜像的操作都是黑箱操作，生成的镜像也被称为黑箱镜像，换句话说，就是除了制作镜像的人知道执行过什么命令、怎么生成的镜像，别人根本无从得知。而且，即使是这个制作镜像的人，过一段时间后也无法记清具体在操作的。虽然 docker diff 或许可以告诉得到一些线索，但是远远不到可以确保生成一致镜像的地步。这种黑箱镜像的维护工作是非常痛苦的。 注意： 应该使用dockerfile 进行相关的镜像生成和维护工作","date":"2024-12-31","categories":["docker"]},{"title":"docker(4)-dockerfile定制镜像","url":"/2024/12/31/docker-4--dockerfile定制镜像/","content":"使用 Dockerfile 定制镜像从刚才的 docker commit 的学习中，我们可以了解到，镜像的定制实际上就是定制每一层所添加的配置、文件。如果我们可以把每一层修改、安装、构建、操作的命令都写入一个脚本，用这个脚本来构建、定制镜像，那么之前提及的无法重复的问题、镜像构建透明性的问题、体积的问题就都会解决。这个脚本就是 Dockerfile。 Dockerfile 是一个文本文件，其内包含了一条条的指令(Instruction)，每一条指令构建一层，因此每一条指令的内容，就是描述该层应当如何构建。 docker 指令FROM 指定基础镜像所谓定制镜像，那一定是以一个镜像为基础，在其上进行定制。就像我们之前运行了一个 nginx 镜像的容器，再进行修改一样，基础镜像是必须指定的。而 FROM 就是指定基础镜像，因此一个 Dockerfile 中 FROM 是必备的指令，并且必须是第一条指令。 From 命令可以制定特定的服务类的镜像如 nginx、redis、mongo、mysql、httpd、php、tomcat 等，或者编程语言镜像如 node、openjdk、python、ruby、golang 等，或者操作系统如 ubuntu、debian、centos、fedora、alpine 等，或者使用空白镜像scratch（这个镜像只能运行可执行文件） run 命令RUN 指令是用来执行命令行命令的。由于命令行的强大能力，RUN 指令在定制镜像时是最常用的指令之一。其格式有两种： shell 格式：RUN <命令>，就像直接在命令行中输入的命令一样。刚才写的 Dockerfile 中的 RUN 指令就是这种格式。 exec 格式：RUN [“可执行文件”, “参数1”, “参数2”]，这更像是函数调用中的格式。 注意：RUN 就像 Shell 脚本一样可以执行命令，那么我们不能就可以像 Shell 脚本一样把每个命令对应一个 RUN，比如这样 因为Dockerfile 中每一个指令都会建立一层，RUN 也不例外。每一个 RUN 的行为，就和刚才我们手工建立镜像的过程一样：新建立一层，在其上执行这些命令，执行结束后，commit 这一层的修改，构成新的镜像。上面的这种写法，创建了 7 层镜像。这是完全没有意义的，而且很多运行时不需要的东西，都被装进了镜像里，比如编译环境、更新的软件包等等。结果就是产生非常臃肿、非常多层的镜像，不仅仅增加了构建部署的时间，也很容易出错 正确的写法应该是这样： 并且，这里为了格式化还进行了换行。Dockerfile 支持 Shell 类的行尾添加 \\ 的命令换行方式，以及行首 # 进行注释的格式。良好的格式，比如换行、缩进、注释等，会让维护、排障更为容易，这是一个比较好的习惯。 此外，还可以看到这一组命令的最后添加了清理工作的命令，删除了为了编译构建所需要的软件，清理了所有下载、展开的文件，并且还清理了 apt 缓存文件。这是很重要的一步，我们之前说过，镜像是多层存储，每一层的东西并不会在下一层被删除，会一直跟随着镜像。因此镜像构建时，一定要确保每一层只添加真正需要添加的东西，任何无关的东西都应该清理掉。 docker build命令docker build 命令将会自动的将docker file 文件进行运行构建出镜像 下面使用的dockerfile 文件 从命令的输出结果中，我们可以清晰的看到镜像的构建过程。在 Step 2 中，如同我们之前所说的那样，RUN 指令启动了一个容器 9cdc27646c7b，执行了所要求的命令，并最后提交了这一层 44aa4490ce2c，随后删除了所用到的这个容器 9cdc27646c7b。 docker 使用dockerfile构建镜像的过程 首先我们要理解 docker build 的工作原理。Docker 在运行时分为 Docker 引擎（也就是服务端守护进程）和客户端工具。Docker 的引擎提供了一组 REST API，被称为 Docker Remote API，而如 docker 命令这样的客户端工具，则是通过这组 API 与 Docker 引擎交互，从而完成各种功能。因此，虽然表面上我们好像是在本机执行各种 docker 功能，但实际上，一切都是使用的远程调用形式在服务端（Docker 引擎）完成。也因为这种 C&#x2F;S 设计，让我们操作远程服务器的 Docker 引擎变得轻而易举。 当我们进行镜像构建的时候，并非所有定制都会通过 RUN 指令完成，经常会需要将一些本地文件复制进镜像，比如通过 COPY 指令、ADD 指令等。而 docker build 命令构建镜像，其实并非在本地构建，而是在服务端，也就是 Docker 引擎中构建的。 那么在这种客户端&#x2F;服务端的架构中，如何才能让服务端获得本地文件呢？ 这就引入了上下文的概念。当构建的时候，用户会指定构建镜像上下文的路径，docker build 命令得知这个路径后，会将路径下的所有内容打包，然后上传给 Docker 引擎。这样 Docker 引擎收到这个上下文包后，展开就会获得构建镜像所需的一切文件。— 这里简单地说就是系统将 制定的自己的目录的信息打包到docker中（注意并不是在容器）","date":"2024-12-31","categories":["docker"]},{"title":"docker(4)-dockerfile指令","url":"/2024/12/31/docker-4--dockerfile指令/","content":"COPY 复制文件格式： <源路径> 可以是多个，甚至可以是通配符，其通配符规则要满足 Go 的 filepath.Match 规则，如： <目标路径> 可以是容器内的绝对路径，也可以是相对于工作目录的相对路径（工作目录可以用 WORKDIR 指令来指定）。目标路径不需要事先创建，如果目录不存在会在复制文件前先行创建缺失目录。 此外，还需要注意一点，使用 COPY 指令，源文件的各种元数据都会保留。比如读、写、执行权限、文件变更时间等。这个特性对于镜像定制很有用。特别是构建相关文件都在使用 Git 进行管理的时候。 ADD 更高级的复制文件ADD 指令和 COPY 的格式和性质基本一致。但是在 COPY 基础上增加了一些功能。 比如 <源路径> 可以是一个 URL，这种情况下，Docker 引擎会试图去下载这个链接的文件放到 <目标路径> 去。下载后的文件权限自动设置为 600，如果这并不是想要的权限，那么还需要增加额外的一层 RUN 进行权限调整，另外，如果下载的是个压缩包，需要解压缩，也一样还需要额外的一层 RUN 指令进行解压缩。 注意: 如果 <源路径> 为一个 tar 压缩文件的话，压缩格式为 gzip, bzip2 以及 xz 的情况下，ADD 指令将会自动解压缩这个压缩文件到 <目标路径> 去。 在 Docker 官方的 Dockerfile 最佳实践文档 中要求，尽可能的使用 COPY，因为 COPY 的语义很明确，就是复制文件而已，而 ADD 则包含了更复杂的功能，其行为也不一定很清晰。最适合使用 ADD 的场合，就是所提及的需要自动解压缩的场合。 CMD 容器启动命令CMD 指令的格式和 RUN 相似，也是两种格式： shell 格式：CMD <命令> exec 格式：CMD [“可执行文件”, “参数1”, “参数2”…] 参数列表格式：CMD [“参数1”, “参数2”…]。在指定了 ENTRYPOINT 指令后，用 CMD 指定具体的参数。 之后使用docker run 的时候可以动态的添加命令到指定的数据中之前介绍容器的时候曾经说过，Docker 不是虚拟机，容器就是进程。既然是进程，那么在启动容器的时候，需要指定所运行的程序及参数。CMD 指令就是用于指定默认的容器主进程的启动命令的。 注意： 在指令格式上，一般推荐使用 exec 格式，这类格式在解析时会被解析为 JSON 数组，因此一定要使用双引号 “，而不要使用单引号。 如果使用 shell 格式的话，实际的命令会被包装为 sh -c 的参数的形式进行执行。比如： 在实际执行中，会将其变更为： 这就是为什么我们可以使用环境变量的原因，因为这些环境变量会被 shell 进行解析处理。 注意：Docker 不是虚拟机，容器中的应用都应该以前台执行，而不是像虚拟机、物理机里面那样，用 upstart&#x2F;systemd 去启动后台服务，容器内没有后台服务的概念。 比如 使用 CMD service nginx start 容器执行后就立即退出 使用 service nginx start 命令，则是希望 upstart 来以后台守护进程形式启动 nginx 服务。而刚才说了 CMD service nginx start 会被理解为 CMD [ “sh”, “-c”, “service nginx start”]，因此主进程实际上是 sh。那么当 service nginx start 命令结束后，sh 也就结束了，sh 作为主进程退出了，自然就会令容器退出。 正确的做法是直接执行 nginx 可执行文件，并且要求以前台形式运行。比如： 注意： 如果 docker run 指定了其他命令，CMD 指定的默认命令将被忽略。如果 Dockerfile 中有多个 CMD 指令，只有最后一个 CMD 有效。 ENTRYPOINT 入口点这个方法相当于默认容器启动的时候运行的命令 ENTRYPOINT 的格式和 RUN 指令格式一样，分为 exec 格式和 shell 格式。 ENTRYPOINT 的目的和 CMD 一样，都是在指定容器启动程序及参数。ENTRYPOINT 在运行时也可以替代，不过比 CMD 要略显繁琐，需要通过 docker run 的参数 –entrypoint 来指定。 当指定了 ENTRYPOINT 后，CMD 的含义就发生了改变，不再是直接的运行其命令，而是将 CMD 的内容作为参数传给 ENTRYPOINT 指令，换句话说实际执行时，将变为： 高级应用类似 redis 官方镜像，自定义启动效果 可以看到其中为了 redis 服务创建了 redis 用户，并在最后指定了 ENTRYPOINT 为 docker-entrypoint.sh 脚本。 该脚本的内容就是根据 CMD 的内容来判断，如果是 redis-server 的话，则切换到 redis 用户身份启动服务器，否则依旧使用 root 身份执行。比如： RUN vs CMD vs ENTRYPOINTRUN、CMD 和 ENTRYPOINT 这三个 Dockerfile 指令看上去很类似，很容易混淆。本节将通过实践详细讨论它们的区别。 简单的说： RUN 执行命令并创建新的镜像层，RUN 经常用于安装软件包。 CMD 设置容器启动后默认执行的命令及其参数，但 CMD 能够被 docker run 后面跟的命令行参数替换。 ENTRYPOINT 配置容器启动时运行的命令。 最佳实践 使用 RUN 指令安装应用和软件包，构建镜像。 如果 Docker 镜像的用途是运行应用程序或服务，比如运行一个 MySQL，应该优先使用 Exec 格式的 ENTRYPOINT 指令。CMD 可为 ENTRYPOINT 提供额外的默认参数，同时可利用 docker run 命令行替换默认参数。 如果想为容器设置默认的启动命令，可使用 CMD 指令。用户可在 docker run 命令行中替换此默认命令。 ENV 设置环境变量格式有两种： 这个指令很简单，就是设置环境变量而已，无论是后面的其它指令，如 RUN，还是运行时的应用，都可以直接使用这里定义的环境变量。 除此之外还能使用环境变量在编写docker文件的时候进行使用，专业术语变量展开 下列指令可以支持环境变量展开： ADD、COPY、ENV、EXPOSE、LABEL、USER、WORKDIR、VOLUME、STOPSIGNAL、ONBUILD。 可以从这个指令列表里感觉到，环境变量可以使用的地方很多，很强大。通过环境变量，我们可以让一份 Dockerfile 制作更多的镜像，只需使用不同的环境变量即可。 ARG 设置环境变量构建参数和ENV的效果一样,都是设置环境变量。但是arg还可以将相关的变量使用进编译过程中 VOLUME 定义匿名卷格式为： 之前我们说过，容器运行时应该尽量保持容器存储层不发生写操作，对于数据库类需要保存动态数据的应用，其数据库文件应该保存于卷(volume)中，后面的章节我们会进一步介绍 Docker 卷的概念。 之前我们说过，容器运行时应该尽量保持容器存储层不发生写操作，对于数据库类需要保存动态数据的应用，其数据库文件应该保存于卷(volume)中 EXPOSE 声明端口格式为 EXPOSE <端口1> [<端口2>…]。 在 Dockerfile 中写入这样的声明有两个好处： 帮助镜像使用者理解这个镜像服务的守护端口，以方便配置映射。 在运行时使用随机端口映射时，也就是 docker run -P 时，会自动随机映射 EXPOSE 的端口。 早期 Docker 版本中还有一个特殊的用处，解决以前所有容器都运行于默认桥接网络中，所有容器互相之间都可以直接访问导致的安全性问题。 于是有了一个 Docker 引擎参数 –icc&#x3D;false –icc&#x3D;false参数作用：当指定该参数后，容器间将默认无法互访，除非互相间使用了 –links 参数的容器才可以互通，并且只有镜像中 EXPOSE 所声明的端口才可以被访问。这个 –icc&#x3D;false 的用法，在引入了 docker network 后已经基本不用了，通过自定义网络可以很轻松的实现容器间的互联与隔离。 注意：要将 EXPOSE 和在运行时使用 -p <宿主端口>:<容器端口> 区分开来。-p，是映射宿主端口和容器端口，换句话说，就是将容器的对应端口服务公开给外界访问，而 EXPOSE 仅仅是声明容器打算使用什么端口而已，并不会自动在宿主进行端口映射。 WORKDIR 指定工作目录格式为 WORKDIR <工作目录路径>。 这个命令是为了解决，如下shell 命令会产生的问题 因为每一个run 命令都会产生一个容易所以会导致对应的文件目录不同，通过使用这个名利可以相关的文件目录统一 USER 指定当前用户和WORKDIR都是解决docker run类似的命令产生新的容器导致环境不同的问题，只不过WORKDIR针对目录USER针对用户 HEALTHCHECK 健康检查格式： HEALTHCHECK [选项] CMD <命令>：设置检查容器健康状况的命令 HEALTHCHECK NONE：如果基础镜像有健康检查指令，使用这行可以屏蔽掉其健康检查指令 HEALTHCHECK 指令是告诉 Docker 应该如何进行判断容器的状态是否正常，这是 Docker 1.12 引入的新指令。 HEALTHCHECK 支持下列选项： –interval&#x3D;<间隔>：两次健康检查的间隔，默认为 30 秒； –timeout&#x3D;<时长>：健康检查命令运行超时时间，如果超过这个时间，本次健康检查就被视为失败，默认 30 秒； –retries&#x3D;<次数>：当连续失败指定次数后，则将容器状态视为 unhealthy，默认 3 次。和 CMD, ENTRYPOINT 一样，HEALTHCHECK 只可以出现一次，如果写了多个，只有最后一个生效。 在 HEALTHCHECK [选项] CMD 后面的命令，格式和 ENTRYPOINT 一样，分为 shell 格式，和 exec 格式。命令的返回值决定了该次健康检查的成功与否：0：成功；1：失败；2：保留，不要使用这个值。 假设我们有个镜像是个最简单的 Web 服务，我们希望增加健康检查来判断其 Web 服务是否在正常工作，我们可以用 curl 来帮助判断，其 Dockerfile 的 HEALTHCHECK 可以这么写： 当运行该镜像后，可以通过 docker container ls 看到最初的状态为 (health: starting)： 在等待几秒钟后，再次 docker container ls，就会看到健康状态变化为了 (healthy)： ONBUILD 为他人做嫁衣裳格式：ONBUILD <其它指令>。 ONBUILD 是一个特殊的指令，它后面跟的是其它指令，比如 RUN, COPY 等，而这些指令，在当前镜像构建时并不会被执行。只有当以当前镜像为基础镜像，去构建下一级镜像的时候才会被执行。 Dockerfile 中的其它指令都是为了定制当前镜像而准备的，唯有 ONBUILD 是为了帮助别人定制自己而准备的。","date":"2024-12-31","categories":["docker"]},{"title":"git-基本使用教程","url":"/2024/12/31/git-基本使用教程/","content":"今天刚刚进行工作一个星期，感觉自己以前在大学里自己做项目的时候许多欠缺的地方，比如说版本控制上的欠缺，这里写几篇博客补充一下自己在版本控制上面的坑 将一个纳入git的控制之中 这个命令将一个文件夹新建成一个git仓库 将文件放入暂存区 这个方法可以一个或者多个方法放入文件的暂存区 克隆远程版本库 可以将指定的远程仓库克隆到本地 更新本的仓库中的文件 上面例子中代码的作用就是将哦origin指定的远程仓库中的next分支合并到本地分支中 在默认模式下，git pull是git fetch后跟git merge FETCH_HEAD的缩写 git branchgit branch 对命令进行分支进行操作 其他的一些说明","date":"2024-12-31","categories":["git"]},{"title":"git_在linux 环境下中文乱码问题解决","url":"/2024/12/31/git_在linux-环境下中文乱码问题解决/","content":"看了网上好多教程 其实一行命令就可以解决","date":"2024-12-31","categories":["git"]},{"title":"golang学习笔记(1)-包变量函数","url":"/2024/12/31/golang学习笔记-1--包变量函数/","content":"golang的包管理和变量管理都是非常容易的 golang 包golang在使用的时候可以通过import 关键字引入包 其他的包在定义的时候可以使用package 来定义一个包 注意 main包是一个特殊的包，这个包表示golang执行的入口 这里其实有一个小坑，就是golang的代码开发目录必须是是GOPATH环境变量下的src目录中，不然引用包的时候找不到代码 注意 在全局环境中同一个包名中不能有相同的方法接口声明 ， 除了main包，其他的包名称必须和他的上层文件夹的名称一致 goang 函数定义 golang的参数类型是放在后面的 golang的返回值是int类型 注意如果这个方法是可以导出的首字母要大写 golang 支持多个返回值,使用的是反解包模式 golang还支持命名返回直接使用返回的变量作为返回值 golang 变量声明方法 golang基本类型 golang 初始化值没有明确初始值的变量声明会被赋予它们的 零值。 零值是： 数值类型为 0， 布尔类型为 false， 字符串为 “”（空字符串）。 类型强制转化方法golang使用变量名称+() 强制转化 常量 注意 常量的声明与变量类似，只不过是使用 const 关键字。 常量可以是字符、字符串、布尔值或数值。 常量不能用 :&#x3D; 语法声明。也不用带上类型","date":"2024-12-31","categories":["golang"]},{"title":"golang学习笔记(2)-流程控制语句","url":"/2024/12/31/golang学习笔记-2--流程控制语句/","content":"for 简单来书就是没有花阔化的for循环 golang用 for 来代替while循环和死循环 for 循环的 range 形式可遍历切片或映射。 当使用 for 循环遍历切片时，每次迭代都会返回两个值。第一个值为当前元素的下标，第二个值为该下标所对应元素的一份副本。 if else swichgolang的switch 没有break ，而且所有可以使用&#x3D; 号进行判断的都可以放入其中 注意没有条件的swich 没有条件的 switch 同 switch true 一样。这种形式能将一长串 if-then-else 写得更加清晰。 deferdefer 语句会将函数推迟到外层函数返回之后执行。推迟调用的函数其参数会立即求值，但直到外层函数返回前该函数都不会被调用。 注意：推迟的函数调用会被压入一个栈中。当外层函数返回时，被推迟的函数会按照后进先出的顺序调用","date":"2024-12-31","categories":["golang"]},{"title":"golang学习笔记(3)-更多类型指针结构体数组map指针接口","url":"/2024/12/31/golang学习笔记-3--更多类型指针结构体数组map指针接口/","content":"指针Go 拥有指针。指针保存了值的内存地址。 类型 *T 是指向 T 类型值的指针。其零值为 nil。 & 操作符会生成一个指向其操作数的指针。 操作符表示指针指向的底层值。 这也就是通常所说的“间接引用”或“重定向”。与 C 不同，Go 没有指针运算。 结构体结构体基本使用 结构体指针 如果我们有一个指向结构体的指针 p，那么可以通过 (*p).X 来访问其字段 X。不过这么写太啰嗦了，所以语言也允许我们使用隐式间接引用，直接写 p.X 就可以。 结构体声明方法 顺序赋值 指定赋值 默认赋值 快速指针 结构体默认方法golang的结构体支持默认支持的方法，用来表示对象 注意这种方法是只读的，修改将不会生效，如果需要修改需要使用指针接收方法 使用指针接收者的原因有二：首先，方法能够修改其接收者指向的值。其次，这样可以避免在每次调用方法时复制该值。若值的类型为大型结构体时，这样做会更加高效。 数组 数组切片 注意是 [start:end) 这种方法是前开后闭的 注意切片其实是引用切片，数据修改会共享 注意:如果没有写数字将会以数组长度来衡量 以下切片是等价的： 切片的长度与容量切片拥有 长度 和 容量。 切片的长度就是它所包含的元素个数。 切片的容量是从它的第一个元素开始数，到其底层数组元素末尾的个数。 切片 s 的长度和容量可通过表达式 len(s) 和 cap(s) 来获取。 你可以通过重新切片来扩展一个切片，给它提供足够的容量。这样会形成一个窗口来审视原来的数组 注意：切片的零值是 nil 动态创建数组或者切片 切片追加数据 append 的第一个参数 s 是一个元素类型为 T 的切片，其余类型为 T 的值将会追加到该切片的末尾。 append 的结果是一个包含原切片所有元素加上新添加元素的切片。 当 s 的底层数组太小，不足以容纳所有给定的值时，它就会分配一个更大的数组。返回的切片会指向这个新分配的数组。 for 循环迭代for 循环的 range 形式可遍历切片或映射。 当使用 for 循环遍历切片时，每次迭代都会返回两个值。第一个值为当前元素的下标，第二个值为该下标所对应元素的一份副本 可以将下标或值赋予 _ 来忽略它。 若你只需要索引，忽略第二个变量即可。 mapmap 使用make 方法生成，两个参数 类型和值 golang支持初始化映射语法 映射的文法与结构体相似，不过必须有键名。 若顶级类型只是一个类型名，你可以在文法的元素中省略它。 在映射 m 中插入或修改元素： 获取元素： 删除元素： 通过双赋值检测某个键是否存在： 若 key 在 m 中，ok 为 true ；否则，ok 为 false。若 key 不在映射中，那么 elem 是该映射元素类型的零值。同样的，当从映射中读取某个不存在的键时，结果是映射的元素类型的零值。注 ：若 elem 或 ok 还未声明，你可以使用短变量声明： 函数值传参golang中函数也是顶级的变量 它们可以像其它值一样传递。可以用作函数的参数或返回值。 函数闭包 函数多态golang支持定义多个函数，每个函数要保证方法签名不同，名称可以相同 接口 其实 goalng的接口编程是针对类型方法来说的，比如使用golang的前缀函数声明绑定使用对象 golang的这种设计其实更像一种声明，这种声明是全局通用的，也就是说如果用接口声明了一种类型那么只要实现了相同的方法就可以重用了，golang支持方法重载所以这种方法其实更能提高通用性 注意：golang在这里又限制了一个条件就是，在全局环境中同一个包名中不能有相同的方法接口声明 引申 ： golang的type关键字Golang语言中存在一个关键字type，type又有两种使用方式，一种是类型别名，一种是类型定义 类型定义 golang 接口可以做变量传参，注意如果接口指向的一定是一个指针，因为他不可能需要复制的 如果接口被null 调用了如果接口被null 调用了，golang不会报错，而是继续以nil为入参运行，比如下面的例子，将会输出 nil 注意:如果接口没有找到对应的函数，将会报错 空接口指定了零个方法的接口值被称为 空接口： interface{}空接口可保存任何类型的值。（因为每个类型都至少实现了零个方法。） 空接口被用来处理未知类型的值。例如，fmt.Print 可接受类型为 interface{} 的任意数量的参数。 空接口承载接口类型判断类型断言类型断言 提供了访问接口值底层具体值的方式。 该语句断言接口值 i 保存了具体类型 T，并将其底层类型为 T 的值赋予变量 t。若 i 并未保存 T 类型的值，该语句就会触发一个恐慌。为了 判断 一个接口值是否保存了一个特定的类型，类型断言可返回两个值：其底层值以及一个报告断言是否成功的布尔值。 若 i 保存了一个 T，那么 t 将会是其底层值，而 ok 为 true。否则，ok 将为 false 而 t 将为 T 类型的零值，程序并不会产生恐慌。请注意这种语法和读取一个映射时的相同之处。 上面的问题的简化版类型选择类型选择 是一种按顺序从几个类型断言中选择分支的结构。 类型选择与一般的 switch 语句相似，不过类型选择中的 case 为类型（而非值）， 它们针对给定接口值所存储的值的类型进行比较。 类型选择中的声明与类型断言 i.(T) 的语法相同，只是具体类型 T 被替换成了关键字 type。 此选择语句判断接口值 i 保存的值类型是 T 还是 S。在 T 或 S 的情况下，变量 v 会分别按 T 或 S 类型保存 i 拥有的值。在默认（即没有匹配）的情况下，变量 v 与 i 的接口类型和值相同。 特殊接口Errorgolang有一个特殊的接口表示错误Error ，其实没啥鸟用。。。。","date":"2024-12-31","categories":["golang"]},{"title":"golang学习笔记(4)-reader和image","url":"/2024/12/31/golang学习笔记-4--reader和image/","content":"Reader文件操作","date":"2024-12-31","categories":["golang"]},{"title":"golang学习笔记(5)-golang并发","url":"/2024/12/31/golang学习笔记-5--golang并发/","content":"gorountineGo 程（goroutine）是由 Go 运行时管理的轻量级线程。 用一个go关键字就可以了 信道是带有类型的管道，你可以通过它用信道操作符 <- 来发送或者接收值。 （“箭头”就是数据流的方向。） 和映射与切片一样，信道在使用前必须创建： 默认情况下，发送和接收操作在另一端准备好之前都会阻塞。这使得 Go 程可以在没有显式的锁或竞态变量的情况下进行同步。 以下示例对切片中的数进行求和，将任务分配给两个 Go 程。一旦两个 Go 程完成了它们的计算，它就能算出最终的结果。 带缓冲的信道信道可以是 带缓冲的。将缓冲长度作为第二个参数提供给 make 来初始化一个带缓冲的信道： 仅当信道的缓冲区填满后，向其发送数据时才会阻塞。当缓冲区为空时，接受方会阻塞。 range 和 close发送者可通过 close 关闭一个信道来表示没有需要发送的值了。接收者可以通过为接收表达式分配第二个参数来测试信道是否被关闭：若没有值可以接收且信道已被关闭，那么在执行完 之后 ok 会被设置为 false。 循环 for i :&#x3D; range c 会不断从信道接收值，直到它被关闭。 注意： 只有发送者才能关闭信道，而接收者不能。向一个已经关闭的信道发送数据会引发程序恐慌（panic）。还要注意： 信道与文件不同，通常情况下无需关闭它们。只有在必须告诉接收者不再有需要发送的值时才有必要关闭，例如终止一个 range 循环。 select 语句select 语句使一个 Go 程可以等待多个通信操作。 select 会阻塞到某个分支可以继续执行为止，这时就会执行该分支。当多个分支都准备好时会随机选择一个执行。 例子 sync.Mutex lock unlock我们只是想保证每次只有一个 Go 程能够访问一个共享的变量，从而避免冲突？ 这里涉及的概念叫做 互斥（mutualexclusion）* ，我们通常使用 互斥锁（Mutex） 这一数据结构来提供这种机制。 Go 标准库中提供了 sync.Mutex 互斥锁类型及其两个方法： LockUnlock我们可以通过在代码前调用 Lock 方法，在代码后调用 Unlock 方法来保证一段代码的互斥执行。","date":"2024-12-31","categories":["golang"]},{"title":"golang-00-开发计划","url":"/2024/12/31/golang-00-开发计划/","content":"golang 配置工具包 golang 日志工具包 golang 异常处理工具包","date":"2024-12-31","categories":["go语言"]},{"title":"golang-01-golang_context用法和理念","url":"/2024/12/31/golang-01-golang_context用法和理念/","content":"Context是golang官方定义的一个package,它定义了Context类型,里面包含了Deadline&#x2F;Done&#x2F;Err方法以及绑定到Context上的成员变量值Value,具体定义如下： 那么到底什么Context？可以字面意思可以理解为上下文,比较熟悉的有进程&#x2F;线程上线文,关于golang中的上下文,一句话概括就是： goroutine的相关环境快照,其中包含函数调用以及涉及的相关的变量值.通过Context可以区分不同的goroutine请求,因为在golang Severs中,每个请求都是在单个goroutine中完成的. 最近在公司分析gRPC源码,proto文件生成的代码,接口函数第一个参数统一是ctx context.Context接口,公司不少同事都不了解这样设计的出发点是什么,其实我也不了解其背后的原理.今天趁着妮妲台风妹子正面登陆深圳,全市停工,停课,停业,在家休息找了一些资料研究把玩一把. Context通常被译作上下文,它是一个比较抽象的概念.在公司技术讨论时也经常会提到上下文.一般理解为程序单元的一个运行状态,现场,快照,而翻译中上下又很好地诠释了其本质,上下上下则是存在上下层的传递,上会把内容传递给下.在Go语言中,程序单元也就指的是Goroutine. 每个Goroutine在执行之前,都要先知道程序当前的执行状态,通常将这些执行状态封装在一个Context变量中,传递给要执行的Goroutine中. 上下文则几乎已经成为传递与请求同生存周期变量的标准方法.在网络编程下,当接收到一个网络请求Request,处理Request时,我们可能需要开启不同的Goroutine来获取数据与逻辑处理,即一个请求Request,会在多个Goroutine中处理. 而这些Goroutine可能需要共享Request的一些信息;同时当Request被取消或者超时的时候,所有从这个Request创建的所有Goroutine也应该被结束. 为什么使用context由于在golang severs中,每个request都是在单个goroutine中完成,并且在单个goroutine（不妨称之为A）中也会有请求其他服务（启动另一个goroutine（称之为B）去完成）的场景,这就会涉及多个Goroutine之间的调用.如果某一时刻请求其他服务被取消或者超时,则作为深陷其中的当前goroutine B需要立即退出,然后系统才可回收B所占用的资源.即一个request中通常包含多个goroutine,这些goroutine之间通常会有交互. 那么,如何有效管理这些goroutine成为一个问题（主要是退出通知和元数据传递问题）,Google的解决方法是Context机制,相互调用的goroutine之间通过传递context变量保持关联,这样在不用暴露各goroutine内部实现细节的前提下,有效地控制各goroutine的运行. 如此一来,通过传递Context就可以追踪goroutine调用树,并在这些调用树之间传递通知和元数据.虽然goroutine之间是平行的,没有继承关系,但是Context设计成是包含父子关系的,这样可以更好的描述goroutine调用之间的树型关系. 怎么使用context生成一个Context主要有两类方法： 顶层Context：Background要创建Context树,首先就是要创建根节点 该Context通常由接收request的第一个goroutine创建,它不能被取消,没有值,也没有过期时间,常作为处理request的顶层context存在. 下层Context：WithCancel&#x2F;WithDeadline&#x2F;WithTimeout有了根节点之后,接下来就是创建子孙节点.为了可以很好的控制子孙节点,Context包提供的创建方法均是带有第二返回值（CancelFunc类型）,它相当于一个Hook,在子goroutine执行过程中,可以通过触发Hook来达到控制子goroutine的目的（通常是取消,即让其停下来）.再配合Context提供的Done方法,子goroutine可以检查自身是否被父级节点Cancel： 注：父节点Context可以主动通过调用cancel方法取消子节点Context,而子节点Context只能被动等待.同时父节点Context自身一旦被取消（如其上级节点Cancel）,其下的所有子节点Context均会自动被取消. 有三种创建方法： 这里写一个列子 test1 的输出 ： test2 的输出: 注意一点 ： 老问题了， test2 最后如果不架上 time.Sleep( time.Second) 这行代码 ， main函数在运行完test2 函数之后将会退出进程， 这样goroute将强制退出了，done test2 将不会输出 context是一个优雅的设计吗?确实,通过引入Context包,一个request范围内所有goroutine运行时的取消可以得到有R效的控制.但是这种解决方式却不够优雅. context 像病毒一样扩散一旦代码中某处用到了Context,传递Context变量（通常作为函数的第一个参数）会像病毒一样蔓延在各处调用它的地方. 比如在一个request中实现数据库事务或者分布式日志记录, 创建的context,会作为参数传递到任何有数据库操作或日志记录需求的函数代码处. 即每一个相关函数都必须增加一个context.Context类型的参数,且作为第一个参数,这对无关代码完全是侵入式的. Context 不仅仅只是cancel信号Context机制最核心的功能是在goroutine之间传递cancel信号,但是它的实现是不完全的. Cancel可以细分为主动与被动两种,通过传递context参数,让调用goroutine可以主动cancel被调用goroutine.但是如何得知被调用goroutine什么时候执行完毕,这部分Context机制是没有实现的.而现实中的确又有一些这样的场景,比如一个组装数据的goroutine必须等待其他goroutine完成才可开始执行,这是context明显不够用了,必须借助sync.WaitGroup. context.valuecontext.Value相当于goroutine的TLS（Thread Local Storage）,但它不是静态类型安全的,任何结构体变量都必须作为字符串形式存储.同时,所有context都会在其中定义变量,很容易造成命名冲突. 总结 context包通过构建树型关系的Context,来达到上一层Goroutine能对传递给下一层Goroutine的控制.对于处理一个Request请求操作,需要采用context来层层控制Goroutine,以及传递一些变量来共享. Context对象的生存周期一般仅为一个请求的处理周期.即针对一个请求创建一个Context变量（它为Context树结构的根）;在请求处理结束后,撤销此ctx变量,释放资源. 每次创建一个Goroutine,要么将原有的Context传递给Goroutine,要么创建一个子Context并传递给Goroutine. Context能灵活地存储不同类型,不同数目的值,并且使多个Goroutine安全地读写其中的值. 当通过父Context对象创建子Context对象时,可同时获得子Context的一个撤销函数,这样父Context对象的创建环境就获得了对子Context将要被传递到的Goroutine的撤销权. 在子Context被传递到的goroutine中,应该对该子Context的Done信道（channel）进行监控,一旦该信道被关闭（即上层运行环境撤销了本goroutine的执行）,应主动终止对当前请求信息的处理,释放资源并返回.","date":"2024-12-31","categories":["go语言"]},{"title":"golang-02-golang流量控制方法","url":"/2024/12/31/golang-02-golang流量控制方法/","content":"问题描述解决go请求流量控制的问题，实现对于将并发控制在一定的范围内。我们遇到的场景：拉取数据时，数据源可能会有一定的QPS限制，防止访问失败，在请求时需要保证请求不超过接口限制。 方案调研 方案 优缺点 Waiting Group + Channel 控制并发数，但不控制QPS Go Ticker 简单，不能支持不均匀 Go 官方 RateLimiter 功能强大，使用较为复杂 Uber RateLimiter 使用方便，功能相对少一些 1. Waiting Group + Channel 保证任意时刻最大的并发数不会超过channel的容量，但不能保证QPS","date":"2024-12-31","categories":["go语言"]},{"title":"golang-03-go_linkname-隐式链接编译","url":"/2024/12/31/golang-03-go_linkname-隐式链接编译/","content":"什么是go:linkname指令的格式如下： go:linkname引导编译器将当前(私有)方法或者变量在编译时链接到指定的位置的方法或者变量,第一个参数表示当前方法或变量,第二个参数表示目标方法或变量,因为这关指令会破坏系统和包的模块化,因此在使用时必须导入unsafe 为什么要用go:linkname这个指令不经常用,最好也不要用,但理解这个指令可以帮助您理解核心包的很多代码.在标准库中是为了可以使用另一个包的unexported的方法或者变量,在敲代码的时候是不可包外访问的,但是运行时用这个命令hack了一下,就变得可以访问. 最大的作用就是 定向可访问. 示例 Greet()去访问一个没有方法体的方法hellofunc(),IDE一般会提示错误,看到这个之后您就会明白了,这一般是另外一个包有go:linkname的链接 我们再看链接的函数： 第一个参数表示当前方法或变量,第二个参数表示需要建立链接方法,变量的路径 在这里例子中hello()只能被hello.hellofunc这里作为链接调用,其他地方是无法访问到这个方法的,只能调用包装过的Greet方法.这个链接过程是在编译时完成的. 注意点go:linkname可以跨包使用 跨包使用时,目标方法或者变量必须导入有方法体的包,这个编译器才可以识别到链接 import _ “github.com&#x2F;lastsweetop&#x2F;testlinkname&#x2F;private” go build无法编译go:linkname,必须用单独的compile命令进行编译,因为go build会加上-complete参数,这个参数会检查到没有方法体的方法,并且不通过.","date":"2024-12-31","categories":["go语言"]},{"title":"golang-04-go_编译器","url":"/2024/12/31/golang-04-go_编译器/","content":"1. golang 编译器相关的处理逻辑 -2. golang 编译器优化1. 函数内连golang 会对简单的函数进行内连（没有复杂语句的情况下 ， for range go select 递归函数或者复杂函数） 如果在函数前有注释 &#x2F;&#x2F; go: noinline 表示默认不进行内连 , 或者编译的时候使用 -l 参数 引申 ： 打印内连信息的方法可以在编译的时候增加 -m &#x3D; 2 的标记 2. 逃逸分析逃逸分析是golang 重要的优化， 用户标识变量内存被分配在栈区还是堆区 在传统的c&#x2F;c++ 中 ， 如果返回一个栈上的对象指针，会导致函数执行完成线程被回收之后，对象被释放 2.1 golang 中栈堆分配原则a. 指向栈上的对象指针不能被存储到堆中b. 指向栈上对象的指针不能超过真对象的神明周期 具体的分析策略待补充 3. 闭包重写简单来说， 闭包如果调用了一次 ， 就是被初始化成一个简单函数 ， 如果多次调用或者后续调用，就会被创建闭包对象。 闭包中的值： 如果变量占用空间小于 2×sizeof(int) -> 会在内部创建局部变量来产生这个变量。 如果是通过指针或者值引用但是占用空间大，那么捕获的变量（var）将会转化成指针类型 &var。 4. 遍历函数闭包重写之后需要变量函数， 会对函数中的一些表达式操作转化成运行时函数, 比如 new 和map 这个过程可能还会进行重排序等","date":"2024-12-31","categories":["go语言"]},{"title":"golang-04-golang_浮点数和类型转化","url":"/2024/12/31/golang-04-golang_浮点数和类型转化/","content":"1. 浮点数陷阱 这种情况输出的是0.99999 而不是0.9 ， 这个其实是IEEE 标准的现实的陷阱 2. 浮点数和定点数INF -INF 分别描述最大值和最小值 golang 中简单的精度处理 ： 使用strconv 处理 输出结果 golang 大数math.big golang big标准库提供了3中数据类型 ： big.Int big.Float big.Rat big.Int 核心思想是使用uint切片来存储大整数 , go 大整数乘法用了 karatsuba算法（todo）执行时候使用了汇编代码 big.Float golang 实现原理简单粗暴 ， 小数先转成整数 ， 然后处理 ， 注意这么做同样有精度问题 。 因为有限位数无法表达无限小数，但是可以通过prec存储数字的位数来提高精度 big.Rat 分数运算 - 会进行一些优化， 尽量避免精度问题， 比如 1&#x2F;2+2&#x2F;3 会变成 (1×3+2×3)&#x2F;2×3 这样 类型转化golang 内置了几个类型转化的函数","date":"2024-12-31","categories":["go语言"]},{"title":"golang-05-字符串","url":"/2024/12/31/golang-05-字符串/","content":"golang 中字符串的结构 Data : 指向底层的字符数组 Len : 只想底层数组的长度 在golang中使用 rune 来表示一个字符 - print 使用 %#U 输出真实编码 输出 字符串运行时拼接 原理 ：找到一个更大的内存地址， 通过内存复制的方法将字符串复制到其中 注意： 如果拼接的时候字符串小于32字节时 ， 会有一个临时的缓存提供使用 ， 大于32 会在堆区开辟一个足够大的内存空间 字符串和字节数组转化 注意： 二者转化并不是简单的引用互转， 如果大小超过32 位 就会还需要申请内存执行copy才行 引申 黑魔法 0 成本转化","date":"2024-12-31","categories":["go语言"]},{"title":"golang-06-数组和切片","url":"/2024/12/31/golang-06-数组和切片/","content":"golang 数组声明的三种方法 printLn 使用 %T 打印数组类型 输出： golang 数组引用 输出 golang 数组赋值和函数引用都是 golang 数组内部数据格式 golang 切片 切片初始化方法 golang 切片截取 golang 截取语法 ， [1:2], 从1 ->2 不包括2 ， 长度和容量都是按照原来的数组的容量和长度截断 golang 切片类型的底层实现 golang make 初始化切片大小支持golang 在初始化的时候会动态检测是分配到栈区中还是分配到堆区中 ， 默认大小是64 KB 可以在在编译期指定 smallframes 标识进行更新 so make([]int64 ,1023) 和 make([]int64,1024)细节上是截然不同的 golang 复制 注意： 如果才用了协程调用的方法或者加入了 race检测， 就会运行 slicestringcopy或者slicecopy函数进行额外的检查， 默认使用的memmove 函数实现内存复制 引申： race 检测 -> go 工具链集成了 race 检测 ， 用于检测golang竞争的问题 golang 切片扩容规则 如果新申请的容量大于原来的经容量的2倍，则使用新申请的容量 如果接切片长度小于 1024 ， 则最终容量是就容量的2倍 如果接切片长度大于或等于1024 ， 则从旧容量开始的1&#x2F;4 循环增加直到大于等于新容量 如果最终容量过大导致溢出， 则最终容量就是新申请的容量","date":"2024-12-31","categories":["go语言"]},{"title":"golang-07-hash表go语言实现机制","url":"/2024/12/31/golang-07-hash表go语言实现机制/","content":"todo 底层实现原理 golang map 使用注意事项 golang 赋值的时候必须要初始化 golang 支持 nil 的map 取值 ， 虽然会返回nil golang map 实现底层原理","date":"2024-12-31","categories":["go语言"]},{"title":"golang-08-函数和栈","url":"/2024/12/31/golang-08-函数和栈/","content":"函数是程序中为了执行特定任务而存在的一系列执行代码。函数接受输入并返回输出，执行程序的过程可以看作一系列函数的调用过程。Go语言中最重要的函数为main函数，其是程序执行用户代码的入口，在每个程序中都需要存在。 golang 中函数是一等公民 函数是程序中为了执行特定任务而存在的一系列执行代码。函数接受输入并返回输出，执行程序的过程可以看作一系列函数的调用过程。Go语言中最重要的函数为main函数，其是程序执行用户代码的入口，在每个程序中都需要存在。 函数是程序中为了执行特定任务而存在的一系列执行代码。函数接受输入并返回输出，执行程序的过程可以看作一系列函数的调用过程。Go语言中最重要的函数为main函数，其是程序执行用户代码的入口，在每个程序中都需要存在。 golang 函数闭包和陷阱 这也是Go语言中一类非常经典的错误，被收录在了Go语言“共同的错误”[2]中，最终协程会打印出values切片的最后一个值。因为当前val值引用的是同一个地址的数据，所以在range循环的过程中，会不断在val地址中更新数据。而在闭包中，由于引用了外部变量val，所以在访问时会获取val地址中的值，可能会获取最后放入其中的值，而不是遍历所有值，从而导致严重的错误。 注意这种情况， 需要使用传参 来规避 ， 或者使用trace 工具来处理 golang 函数栈就是传统计算机理论中的函数栈 > 每个函数在执行过程中都使用一块栈内存来保存返回地址、局部变量、函数参数等，我们将这一块区域称为函数的栈帧（stack frame）。 golang 堆栈信息 -> todogolang 函数栈扩容和栈转移 -> todogolang 栈调试 -> todo","date":"2024-12-31","categories":["go语言"]},{"title":"golang-09-defer延迟调用","url":"/2024/12/31/golang-09-defer延迟调用/","content":"golang 中的defer 非常灵活， 用来替代类似java 中的try–catch Go语言中的defer有一些不同的特点及灵活性，包括程序中可以有多个defer、defer的调用可以存在于函数的任何位置等。defer可能不会被执行，例如，如果判断条件不成立则放置在if语句中的defer可能不会被执行。defer语句在使用时有许多陷阱，Go语言中defer的实现方式也经历了多次演进 golang defer 使用优势1. 用于资源释放defer一般用于资源的释放和异常的捕获，作为Go语言的特性之一，defer给Go代码的书写方式带来了很大的变化。下面的CopyFile函数用于将文件srcName的内容复制到文件dstName中。 如果不使用defer 则需要在每一个需要关闭的地方增加逻辑， 非常复杂 使用defer优化， 规范写法 2. panic 异常捕获defer的特性是无论后续函数的执行路径如何以及是否发生了panic，在函数结束后一定会得到执行，这为异常捕获提供了很好的时机。异常捕获通常结合recover函数一起使用 3. 延迟执行比如加锁延迟执行 4. 参数预执行 如上例所示，defer后的函数需要传递int参数，首先将a赋值为1，接着defer函数的参数传递为a+1，最后，在函数返回前a被赋值为99。那么最后defer函数打印出的b值是多少呢？答案是2。原因是传递到defer的参数是预执行的，因此在执行到defer语句时，执行了a+1并将其保留了起来，直到函数执行完成后才执行defer函数体内的语句 4. 多个 defer 后入先出运行 5. defer 配合返回值 输出 解释这个其实涉及到一个栈执行逻辑的问题 golang 栈的变量和返回值之间的执行逻辑如下 将返回值保存在栈上→执行defer函数→函数返回 但是要注意golang 返回之有变量的这种情况 , 如果返回值没有变量名称， 那么golang在返回的会生成一个默认变量来代表返回值的变量名称 ， 这个特性的差异导致了case 2 和case 3 返回值的不同 defer 底层原理 todo","date":"2024-12-31","categories":["go语言"]},{"title":"golang-范型","url":"/2024/12/31/golang-范型/","content":"golang 范型类型声明 golang 增加了一个 [] 符号的语法， 里面可以增加我们的约束，这个约束中的信息是低类型 ， 向上兼容所有派生类型比如 golang 初始化 复杂的嵌套类型 golang范型类型初始化方法和自定义类型初始化方法一样 ， 但是注意范型使用的时候要声明范型类型 ， golang暂时不提供推导能力 范型函数 约束类型 关键字 any -> 指定任意类型comparator -> 指实现了比较接口interface","date":"2024-12-31","categories":["go语言"]},{"title":"go语言的设计和坑","url":"/2024/12/31/go语言的设计和坑/","content":"Go 语言的设计和坑本文目的： 理解Go为什么X，摆脱原语言的思维 解决写代码时比较困惑和不满的点，对容易出错的语法有个印象 规范一些习惯性写法，保持代码strength & cleanGo学起来非常简单，但是这是语言设计者刻意为之，很多复杂的细节都藏在语言实现里，导致我们迅速学会Go之后不断踩坑，希望本文能让大家在工作中少踩坑，早下班！ Why Go2007年，Google设计Go，目的在于提高在并行编程（多核CPU越来越多）、分布式部署、大型代码库（以及维护他们的非常多的开发人员）的情况下的开发效率。设计时，在吸收C++优点的基础上，收集于很多工程师之间流传的的“不要像C++”的缺点。 Go like C++： 内存消耗少 执行速度快 启动快 Go not like C++： 程序编译时间短（按照作者过去的工程经验，一个C++大型项目即使make -j8也需要编译一个小时以上） 像动态语言一样灵活（提供了如runtime、interface、闭包、反射等很多增加灵活性的特性） 内置并发支持（C++的协程至少得等到std23才有，非常落后） 丰富的原生库（C++解析json，建立http服务器，使用redis这种都很难找到靠谱的库） 多语义（取消了指针运算、取消隐式类型转换、取消类型别名，取消重载，++和赋值作为表达式…） Go的优点： 面向工程：简单。只有25个关键字，代码风格统一，可读性高，go mod包丰富 自动垃圾回收：语言运行时内置垃圾回收 语言级并发：非常好用的routine和channel，更高层次的并发抽象 静态语言，动态特性 Go的缺点： runtime的性能还需要提高 没有泛型（这一项很快就可以克服了） 冗余的错误处理 Go mod不够完善（Go语⾔将⾛向何⽅?）https://chai2010.cn/static-public/talks/giac2018-go-talk.pdf（我为什么放弃Go语言）https://blog.csdn.net/liigo/article/details/23699459 Go的设计哲学 （创始人Rob Pike在SPLASH上的演讲，阐述了设计Go的初衷）https://talks.golang.org/2012/splash.article（许式伟，Go和Java在继承观念上的对比）https://www.infoq.cn/article/go-based-on-connection-combination-language-1（对面向对象的批评）https://studygolang.com/articles/2944（王垠：解密“设计模式”，对设计模式的批评）https://www.yinwang.org/blog-cn/2013/03/07/design-patterns 少即是多（less is more）：如果一个特性并不对解决任何问题有显著价值，那么go就不提供它；如果需要一个特性，那么只有一种方法去实现 面向接口编程：非侵入式接口，反对继承、反对虚函数和虚函数重载（多态）、删除构造和析构函数 正交+组合的语言特性：语言的特性之间相互独立，不相互影响。比如类型和方法是互相独立的，类型之间也是相互独立的，没有子类，包也没有子包。不同特性用组合的方式来松耦合 并发在语言层面支持：并发更好利用多核，有更强的表现力来模拟真实世界 在设计上，Go秉承了C的简单粗暴。 为什么没有继承？Go没有子类型的概念，只能把类型嵌入到另一个类型中，所以没有类型系统。Go的作者认为类型系统被过度使用了，应该在这个方向上退一步。 使用伸缩性良好的组合，而不是继承 数据和方法不再绑定在一起，数据的集合用struct，方法的集合用interface，保持正交 类似子类父类的系统造成非常脆弱的代码。类型的层次必须在早期进行设计，通常会是程序设计的第一步，但是一旦写出程序后，早期的决策就很难进行改变了。所以，类型层次结构会促成早期的过度设计，因为程序员要尽力对软件可能需要的各种可能的用法进行预测，不断地为了避免挂一漏万，不断的增加类型和抽象的层次。这种做法有点颠倒了，系统各个部分之间交互的方式本应该随着系统的发展而做出相应的改变，而不应该在一开始就固定下来。作者附了一个例子，是一些以接口为参数并且其返回结果也是一个接口的函数： 这种组合+函数的模式是相当灵活的。如果用继承，我们可能会多三个io.Reader的定义（或者若干个成员函数）；然后用多态去获得对应的功能为什么没有异常？ panic和recover这些函数是故意弄的不好用的，因为我们应该减少使用他们。不像Java库中使用异常那样，在go的库中这两个关键字几乎没有使用。 业务中的错误并不是真正的异常情况，if和return完全可以胜任，无需控制流 如果错误要使用特殊的控制结构，错误处理就会扭曲程序的控制流，非常复杂 显式的错误检查会迫使程序员在错误出现的时候对错误进行思考，并进行相应的处理，而不是推给前面的调用堆栈 毫无疑问这会使代码更长一些，但如此编码带来的清晰度和简单性可以弥补其冗长的缺点 为什么没有X？ 总结：Go的设计着眼于编程的便利性、编译的速度、概念的正交性以及支持并发和垃圾回收等功能。如果你在Go中找不到其他语言的X特性，那么只能说明这个特性不适合Go，比如它会影响编译速度或设计的清晰度，或者使得基础系统变得特别复杂。 保持Go的特性和风格 从Go的角度考虑问题，更加让人容易理解 用Go的方式写代码，more effective 容易出错的细节创建对象新建一个对象在go里面有好几种方法，让人迷惑，而且似乎和简洁这一设计原则违背。我们按照对象类型讨论一下： 对于结构体，new(T)和&T{}是等价的，都会给对象赋零值（一般人很少用new）。Note：直接var obj T;&T也是等价的，只不过变量有可能在堆上，有可能在栈上 对于slice、map、chan，make(map[string]int)和map[string]int{}等价，会对对象进行初始化。 看一个代码里的badcase： 零值零值和未初始化的值并不相同。不同类型的零值是什么？ 布尔类型是false，整型是0，字符串是”” 指针、函数、interface、slice、channel和map的零值都是nil 结构体的零值是递归生成的，每个成员都是对应的零值 我们来看一个例子。一个为nil的slice和map能做什么操作： 值传递Go语言中所有的传参都是值传递，都是原值的一个副本，或者说一个拷贝。传入的数据能不能在函数内被修改，取决于是不是指针或者含有指针的类型（指针被值传递复制后依然指向同一块地址）。这就让人很疑惑，什么时候传入的参数修改会生效，什么时候不会生效？slice类型在 值传递的时候len和cap不会变，所以函数内append没有用：type slice struct { array unsafe.Pointer len int cap int}&#x2F;&#x2F; badcasefunc appendMe(s []int){ s &#x3D; append(s, -1)} map 和 chan类型，本来就是个指针，所以函数内修改一定会生效：&#x2F;&#x2F; map实际上是一个 *hmapfunc makemap(t *maptype, hint int64, h *hmap, bucket unsafe.Pointer) *hmap { &#x2F;&#x2F;省略无关代码} &#x2F;&#x2F; chan实际上是个 *hchanfunc makechan(t *chantype, size int64) *hchan { &#x2F;&#x2F;省略无关代码} 再比如一个结构体作为参数：&#x2F;&#x2F; 这是一个典型的指针包裹类型type Person struct { name string age *int}func modify(x Person){ x.name &#x3D; “modified” *x.age &#x3D; 66} 这个结构体里的age是个指针类型，所以在函数内会被修改。这种含有指针的结构体类型，里面的指针指向了其他的内存。在发生拷贝的时候，只有结构体本身的内存会被拷贝，指向的内存是和原值共享的。（更多细节参考 值部：https://gfw.go101.org/article/value-part.html )但是我们一般希望的是，要么结构体的成员一起改变（这个简单，参数传person的指针），要么一起不改变（深拷贝）。那么另一个让人头疼的问题来了，那我如何深拷贝这个对象？深拷贝对于slice，go提供了似乎还不错的方式：&#x2F;&#x2F; 自己复制s1 :&#x3D; []int{1,2,3}s2 :&#x3D; append([]int{}, s1…)&#x2F;&#x2F; 效率更高的复制s1 :&#x3D; []int{1,2,3}s2 :&#x3D; make([]int, len(s1))copy(s2, s1) 如果你要拷贝一个map，只能用for循环依次把键值对赋值到新map里。切记：需要拷贝map一定要深拷贝，不然如果后续在不同的协程里操作map会panic如果有其他更复杂的结构体需要深拷贝呢？目前还没有很好的办法： 自己写一个复制值的函数 用序列化&#x2F;反序列化的方法来做，json，bson 用反射来做age :&#x3D; 22p :&#x3D; &Person{“Bob”, &age} v :&#x3D; reflect.ValueOf(p).Elem()vp2 :&#x3D; reflect.New(v.Type())vp2.Elem().Set(v) 小心interface判等go实现接口的时候有两个属性，type T和value V，判等的时候两个属性都要比较。比如一个interface存了3，那么T&#x3D;int，v&#x3D;3。只有当两个值都没有设置才等于nil。var pi *int &#x3D; nilvar pb *bool &#x3D; nilvar x interface{} &#x3D; pivar y interface{} &#x3D; pbvar z interface{} &#x3D; nil fmt.Println(x &#x3D;&#x3D; y) &#x2F;&#x2F; falsefmt.Println(x &#x3D;&#x3D; nil) &#x2F;&#x2F; falsefmt.Println(x &#x3D;&#x3D; z) &#x2F;&#x2F; false &#x2F;&#x2F; badcasetype error interface { Error() string}func returnsError() error { var p *MyError &#x3D; nil if bad() { p &#x3D; ErrBad } return p &#x2F;&#x2F; Will always return a non-nil error.} 还有一种常见的场景是我们容易漏掉的。int64和int的interface也不相等：var int1,int2 interface{}int1 &#x3D; int64(0)int2 &#x3D; int(0)fmt.Printf(“%v %v &#x3D; %v”, int1, int2, int1 &#x3D;&#x3D; int2) &#x2F;&#x2F; 0 0 false &#x2F;&#x2F; 如果函数参数用了interface，如果我们很容易犯错func (m *Map) Load(key, value interface{}) { if e, ok :&#x3D; read.m[key]; ok { … }}&#x2F;&#x2F; badcase 1: key的类型不一致导致缓存无法取出m :&#x3D; sync.Map{}m.Store(0, “ManualCache”)val, ok :&#x3D; m.Load(int64(0)) &#x2F;&#x2F; nil false&#x2F;&#x2F; badcase 2: value的类型不一致导致断言失败m.Store(“key”, 0)if val, ok :&#x3D; m.Load(“key”); ok { _ &#x3D; val.(int64) &#x2F;&#x2F; panic} 点点点…是个很常用的语法糖，能帮我们节省很多代码。用作展开：x :&#x3D; []int{1,2,3}y :&#x3D; []int{4,5,6}x &#x3D; append(x, y…) &#x2F;&#x2F;而不是for循环x &#x3D; append(x, 4, 5, 6) &#x2F;&#x2F;等价于上面的 用作可变参数列表：&#x2F;&#x2F; Println prints to the standard logger in the manner of fmt.Println.func Println(v …interface{}) { std.Output(2, fmt.Sprintln(v…)) &#x2F;&#x2F; Output takes parameters (int, string)} 用作简化数组声明：var _ &#x3D; […]language{ {“C”, 1972}, {“Python”, 1991}, {“Go”, 2009},}var b &#x3D; […]string{0: “foo”, 2: “foo”} &#x2F;&#x2F; [3]string{“foo”, “”, “foo”} 闭包里的局部变量是引用闭包里起的go协程里面引用的是变量i的地址。所有的go协程启动后等待调用，在下面的协程中，部分协程很可能在for循环完成之后才被调用，所以输出结果很多都是最后一个i的值&#x2F;&#x2F; bad casedone :&#x3D; make(chan bool)for i :&#x3D; 0; i < 5; i++ { go func() { println(i) done <- true }()}for i :&#x3D; 0; i < 5; i++{ <-done}&#x2F;&#x2F; 5 5 5 5 5 &#x2F;&#x2F; good sample 1for i :&#x3D; 0; i < 5; i++ { defer func(i int) { println(i) done <- true }(i)}&#x2F;&#x2F; good sample 2for i :&#x3D; 0; i < 5; i++ { i :&#x3D; i &#x2F;&#x2F; 新建变量 go func() { println(i) done <- true }()}&#x2F;&#x2F;1 3 5 4 2 不要引用大数组被切片引用的数据不会被释放（即使你仅仅引用了很小一部分），会大幅降低代码性能headerMap :&#x3D; make(map[string][]byte) for i :&#x3D; 0; i < 5; i++ { name :&#x3D; “&#x2F;path&#x2F;to&#x2F;file” data, err :&#x3D; ioutil.ReadFile(name) if err !&#x3D; nil { log.Fatal(err) } headerMap[name] &#x3D; data[:1] &#x2F;&#x2F; better: headerMap[name] &#x3D; append([]byte{}, data[:1]…)} 赋值不是原子操作在64位的机器上，赋值很可能被拆成mov两次的汇编代码，因此不是原子的。我们可以用atomic里的方法帮助我们做原子操作。考虑一个内存cache定时刷新的协程：因为随时有请求在读cache，所以刷新cache的时候需要保证cache的指针存取是原子操作。举例：mycache *map[string]*Cache&#x2F;&#x2F; 加载（读取）var _ &#x3D; (*T)(atomic.LoadPointer((*unsafe.Pointer)(unsafe.Pointer(mycache)))) &#x2F;&#x2F; 存储（修改）atomic.StorePointer( (*unsafe.Pointer)(unsafe.Pointer(mycache)), unsafe.Pointer(&newMycache)) 所有的操作，只要存在同时存在多个goroutine同时操作一个资源（临界区），除了带有sync，atomic，或者channel关键字的，都不安全。包括但不限于： 并发读写map 并发append切片 自增变量 赋值接收器用指针还是值Go的接收器可以传指针进来，也可以传值。注意传值的时候接收器不会被改变。官方推荐下面两种情况该用指针： MyStruct很大，需要拷贝的成本太高 方法需要修改MyStruct否则Go推荐使用值接收器Note：如果对象有可能并发执行方法，指针接收器中可能产生数据竞争，记得加锁func（s * MyStruct）pointerMethod（）{ &#x2F;&#x2F; 指针方法 s.Age &#x3D; -1 &#x2F;&#x2F; useful}func（s MyStruct）valueMethod（）{ &#x2F;&#x2F; 值方法 s.Age &#x3D; -1 &#x2F;&#x2F; no use} for循环里的变量都是副本for key, element &#x3D; range aContainer {…} 关于上面for循环有几个点： 实际遍历的aContainer是原始值的一个副本 element是遍历到的元素的原始值的一个副本 key和element整个循环都是同一个变量，而不是每次迭代都生成新变量这里涉及到几个问题。一个是aContainer和element的拷贝成本。aContainer是数组的时候的拷贝成本比较大，而切片和map的拷贝成本比较小。如果想要缩小拷贝成本，我们有几个建议： 遍历大数组时，可以先创建大数组的切片再放在range后面 element结构比较大的时候，直接用下标key遍历，舍弃element还有一个问题是遍历的时候修改，能不能生效？ 当aContainer是数组时，因为数组是整个复制，所以直接修改aContainer不会生效 直接修改key或者element，？ 因为切片和map是浅复制，在循环中操作aContainer或者aContainer[key]可以生效 因为循环里的副本和函数参数的副本非常类似，所以我们可以参考上面的“值传递”中的内容来判断修改副本是否会使得修改达到想要的效果。map的值不可取址map是哈希表实现的，所以值的地址在哈希表动态调整的时候可能会产生变化。因此。存着map值的地址是没有意义的，go中直接禁止了map的值的取地址。这些类型都不能取址： map元素 string的字节元素 常量（有名常量和字面量都不可以） 中间结果值（函数调用、显式值转换、各种操作）&#x2F;&#x2F; 下面这几行编译不通过。_ &#x3D; &[3]int{2, 3, 5}[0] &#x2F;&#x2F;字面量_ &#x3D; &map[int]bool{1: true}[1] &#x2F;&#x2F;字面量const pi &#x3D; 3.14_ &#x3D; &pi &#x2F;&#x2F;有名常量m :&#x3D; map[int]bool{1: true}_ &#x3D; &m[1] &#x2F;&#x2F;map的valuelt :&#x3D; [3]int{2, 3, 5}_ &#x3D; &lt[1:1] &#x2F;&#x2F;切片操作 一般来说，一个不可寻址的值的直接部分是不可修改的。但是map的元素是个例外。 map的元素虽然不可寻址，但是每个映射元素可以被整个修改（但不可以被部分修改）：type T struct{age int}mt :&#x3D; map[string]T{}mt[“John”] &#x3D; T{age: 29} &#x2F;&#x2F; 整体修改是允许的ma :&#x3D; map[int][5]int{}ma[1] &#x3D; [5]int{1: 789} &#x2F;&#x2F; 整体修改是允许的 &#x2F;&#x2F; 这两个赋值编译不通过，因为部分修改一个映射元素是非法的。这看上去确实有些反直觉。ma[1][1] &#x3D; 123 &#x2F;&#x2F; errormt[“John”].age &#x3D; 30 &#x2F;&#x2F; error &#x2F;&#x2F; 读取映射元素的元素或者字段是没问题的。fmt.Println(ma[1][1]) &#x2F;&#x2F; 789fmt.Println(mt[“John”].age) &#x2F;&#x2F; 29 逃逸分析关心变量在栈或者堆上有助于我们对变量的生命周期有所了解，写出更好性能的代码。比如一些短周期的变量的指针如果和长生命周期的变量绑定，就会使得这个变量迟迟不能回收，影响性能。Go在栈上的变量不会产生GC成本，因为变量会随着函数的退出一起销毁（当然这样性能也是最高的）。但是，变量是否在栈上，不能简单的通过是否局部变量或者是否使用new构建的引用类型来判断。有一个基本的判断原则：情况1：如果变量的引用被声明它的函数返回了，那么这个变量就会逃逸到堆上func ref(z S) *S { return &z}&#x2F;&#x2F; go run -gcflags ‘-m -l’ main.go.&#x2F;escape.go:10: moved to heap: z.&#x2F;escape.go:11: &z escapes to heap 情况2：返回的结构体引用的对象会逃逸func refStruct(y int) (z S) { z.M &#x3D; &y return z}&#x2F;&#x2F; go run -gcflags ‘-m -l’ main.go.&#x2F;escape.go:12: moved to heap: y.&#x2F;escape.go:13: &y escapes to heap 情况3：map、slice、chan引用的对象会逃逸func main() { a :&#x3D; make([]*int,1) b :&#x3D; 12 a[0] &#x3D; &b}&#x2F;&#x2F; go run -gcflags ‘-m -l’ maint.go.&#x2F;maint.go:5:2: moved to heap: b.&#x2F;maint.go:4:11: make([]*int, 1) does not escape 我们看一个例子，逃逸使得性能下降了不少：func BenchmarkHeap(b *testing.B) { b.ResetTimer() c :&#x3D; make(chan *T, b.N) &#x2F;&#x2F; c :&#x3D; make(chan T, b.N) for i :&#x3D; 0; i < b.N; i++ { b :&#x3D; T{a: 3, b: 5} c <- &b &#x2F;&#x2F; c <- b }}&#x2F;&#x2F; go test -bench&#x3D;. -run&#x3D;noneBenchmarkStack-12 32297865 32.1 ns&#x2F;opBenchmarkHeap-12 28062832 40.2 ns&#x2F;op routineGolang并发注意点goroutine性能探究和编程规范 最好确认routine任务的开销大于上下文切换的开销时，才使用routine。 要尽量控制routine的数量，不然会起到反效果 channel要注意缓冲区的大小和每次写入的数量，尽量打包写入防止泄漏如果routine在运行中被阻塞，或者速度很慢，就会发生泄漏（routine的数量会迅速线性增长） routinue卡死在读取chan却没数据理想情况下，我们设计的读取chan的routine会把所有的内容读取完毕后才会关闭。但是，一旦读取者在读取完成之前退出，写入方写满chan之后就会卡死。 routinue处理的速度过慢这个情况有点类似消息队列消费者的堆积，如果新起的routine处理速度比主协程还慢的话，堆积起来的routine会越来越多，最终打爆内存复用timer来替代timer.Aftertimer.After会创建很多的timer，引发很大的GC消耗。&#x2F;&#x2F; 如果有100w个msg推进来，就会有100w个timer被销毁func longRunning(messages <-chan string) { for { select { &#x2F;&#x2F; 消息间隔超过1min会return case <-time.After(time.Minute): return case msg :&#x3D; <-messages: fmt.Println(msg) } } }func longRunning(messages <-chan string) { timer :&#x3D; time.NewTimer(time.Minute) defer timer.Stop() for { select { case <-timer.C: &#x2F;&#x2F; 过期了 return case msg :&#x3D; <-messages: fmt.Println(msg) &#x2F;&#x2F; 此if代码块很重要。 if !timer.Stop() { <-timer.C } } &#x2F;&#x2F; 必须重置以复用。 timer.Reset(time.Minute) }} 我们在每次处理完消息后调用timer.Stop()以便于复用。如果timer已经过期，stop会返回false，C里面还有一条过期消息，我们需要把它取出来；如果timer没有过期，stop会返回true，继续执行循环 在一个Timer终止（stopped）之后并且在重置和重用此Timer值之前，我们应该确保此Timer的通道C中肯定不存在过期的通知常用的仓库sync和atomicstringsStrings库是重复造轮子的重灾区，很多人试图自己再写一遍前后处理var s &#x3D; “abaay森z众xbbab”o :&#x3D; fmt.Printlno(strings.TrimPrefix(s, “ab”)) &#x2F;&#x2F; aay森z众xbbabo(strings.TrimSuffix(s, “ab”)) &#x2F;&#x2F; abaay森z众xbbo(strings.TrimLeft(s, “ab”)) &#x2F;&#x2F; y森z众xbbabo(strings.TrimRight(s, “ab”)) &#x2F;&#x2F; abaay森z众xo(strings.Trim(s, “ab”)) &#x2F;&#x2F; y森z众xo(strings.TrimFunc(s, func(r rune) bool { return r < 128 &#x2F;&#x2F; trim all ascii chars })) &#x2F;&#x2F; 森z众 分割与合并&#x2F;&#x2F; “1 2 3” -> [“1”,”2”,”3”]func Fields(s string) []string &#x2F;&#x2F; 用空白字符分割字符串&#x2F;&#x2F; “1|2|3” -> [“1”,”2”,”3”]func Split(s, sep string) []string &#x2F;&#x2F; 用sep分割字符串，sep会被去掉&#x2F;&#x2F; [“1”,”2”,”3”] -> “1,2,3”func Join(a []string, sep string) string &#x2F;&#x2F; 将一系列字符串连接为一个字符串，之间用sep来分隔 &#x2F;&#x2F; Note:&#x2F;&#x2F; “1||3” -> [“1”,””,”3”] 演化中的错误处理满足下面的诉求： 可以把异常传递下去，并不丢失自己的类型 可以保存堆栈信息Go的错误处理一直在讨论和演进，目前官方已经有几种不同的方案。对于反复写错误处理代码的问题，有几种解决的设想，可以看看上面的（Go语⾔将⾛向何⽅?）import (“golang.org&#x2F;x&#x2F;xerrors”) func bar() error { if err :&#x3D; foo(); err !&#x3D; nil { return xerrors.Errorf(“bar failed: %w”, foo()) } return nil}func foo() error { return xerrors.Errorf(“foo failed: %w”, sql.ErrNoRows)} func main() { err :&#x3D; bar() if xerrors.Is(err, sql.ErrNoRows) { fmt.Printf(“data not found, %v\\n”, err) fmt.Printf(“%+v\\n”, err) return }}&#x2F;* Outputs:data not found, bar failed: foo failed: sql: no rows in result setbar failed: main.bar &#x2F;usr&#x2F;four&#x2F;main.go:12 foo failed:main.foo&#x2F;usr&#x2F;four&#x2F;main.go:18 sql: no rows in result set*&#x2F; 参考资料《Go Tour》（一个小时学会Go）https://tour.go-zh.org/welcome/1《The Go Programming Language Specification》（语法细节）https://golang.org/ref/spec#Introduction（中文版《Go语言编码规范》）《Go语言圣经》（语法细节）https://docs.hacknode.org/gopl-zh/《Effective Go》（适合刚学完Go的基础语法时候读）https://www.kancloud.cn/kancloud/effective/72199《Go语言设计和实现》（适合想了解Go某个特性实现原理的时候参考）https://draveness.me/golang/docs/part1-prerequisite/ch02-compile/golang-compile-intro/《Go Q&A 101》（可以和官方QA结合看）https://gfw.go101.org/article/unofficial-faq.html#time-sleep-after《Go 语言高级编程》https://chai2010.cn/advanced-go-programming-book/《Go语言原本》https://golang.design/under-the-hood/《Google Go代码规范》https://github.com/golang/go/wiki/CodeReviewComments《Uber Go规范》https://github.com/xxjwxc/uber_go_guide_cn","date":"2024-12-31","categories":["go语言"]},{"title":"dubbo-学习和使用(1)-dubbo架构设计和思想","url":"/2024/12/31/dubbo-学习和使用-1--dubbo架构设计和思想/","content":"决定使用dubbo 框架的原因考虑了很久，查了很对有关spring cloud 和 dubbo 的对比，最终决定好好研究一下dubbo框架，原因主要有一下的两个方面： dubbo 相比叫spring cloud 感觉rpc调用才是未来发展的框架和基础。 dubbo 相比较spring cloud 来说代码质量和抽象程度更高，研究和学习的点更多 目前dubbo存在的问题最大的问题，整合spring boot 其余的以后补充 dubbo 项目规范其实考察其他的一些rpc 调用框架，无一例外，都存在一个模板，在dubbo 中这种统一的模板被定义成一个统一的包，各个需要调用的接口都继承这个包来进行开发和部署 一个基础的dubbo项目结构 dubbo_information: 表示公共的接口类（rpc调用过程中的模板） dubbo_client : 表示公共接口的客户端 dubbo_service : 表示公共接口的服务端 dubbo的作用 作为服务注册中心，动态的注册和发现服务，使服务的位置透明并通过在消费方获取服务提供方地址列表，实现软负载均衡和 Failover（失效备份），降低对 F5 硬件负载均衡器的依赖 自动画出应用间的依赖关系图，以帮助架构师理清理关系。 将服务现在每天的调用量，响应时间，都统计出来，作为容量规划的参考指标，可以动态调整权重，在线上，将某台机器的权重一直加大，并在加大的过程中记录响应时间的变化，直到响应时间到达阀值，记录此时的访问量，再以此访问量乘以机器数反推总容量 dubbo的基本架构 节点 角色说明 Provider 暴露服务的服务提供方 Consumer 调用远程服务的服务消费方 Registry 服务注册与发现的注册中心 Monitor 统计服务的调用次调和调用时间的监控中心 Container 服务运行容器 调用关系说明 服务容器负责启动，加载，运行服务提供者。 服务提供者在启动时，向注册中心注册自己提供的服务。 服务消费者在启动时，向注册中心订阅自己所需的服务。 注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者。 服务消费者，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。 服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心。 分布式系统中一些特性Dubbo 架构具有以下几个特点，分别是连通性、健壮性、伸缩性、以及向未来架构的升级性。 连通性 注册中心负责服务地址的注册与查找，相当于目录服务，服务提供者和消费者只在启动时与注册中心交互，注册中心不转发请求，压力较小 监控中心负责统计各服务调用次数，调用时间等，统计先在内存汇总后每分钟一次发送到监控中心服务器，并以报表展示 服务提供者向注册中心注册其提供的服务，并汇报调用时间到监控中心，此时间不包含网络开销 服务消费者向注册中心获取服务提供者地址列表，并根据负载算法直接调用提供者，同时汇报调用时间到监控中心，此时间包含网络开销 注册中心，服务提供者，服务消费者三者之间均为长连接，监控中心除外 注册中心通过长连接感知服务提供者的存在，服务提供者宕机，注册中心将立即推送事件通知消费者 注册中心和监控中心全部宕机，不影响已运行的提供者和消费者，消费者在本地缓存了提供者列表 注册中心和监控中心都是可选的，服务消费者可以直连服务提供者 健壮性 监控中心宕掉不影响使用，只是丢失部分采样数据 数据库宕掉后，注册中心仍能通过缓存提供服务列表查询，但不能注册新服务 注册中心对等集群，任意一台宕掉后，将自动切换到另一台 注册中心全部宕掉后，服务提供者和服务消费者仍能通过本地缓存通讯 服务提供者无状态，任意一台宕掉后，不影响使用 服务提供者全部宕掉后，服务消费者应用将无法使用，并无限次重连等待服务提供者恢复 伸缩性 注册中心为对等集群，可动态增加机器部署实例，所有客户端将自动发现新的注册中心 服务提供者无状态，可动态增加机器部署实例，注册中心将推送新的服务提供者信息给消费者 升级性当服务集群规模进一步扩大，带动IT治理结构进一步升级，需要实现动态部署，进行流动计算，现有分布式服务架构不会带来阻力。下图是未来可能的一种架构： 节点角色说明 节点 角色说明 Deployer 自动部署服务的本地代理 Repository 仓库用于存储服务应用发布包 Scheduler 调度中心基于访问压力自动增减服务提供者 Admin 统一管理控制台 Registry 服务注册与发现的注册中心 Monitor 统计服务的调用次数和调用时间的监控中心 dubbo 的需求 在大规模服务化之前，应用可能只是通过 RMI 或 Hessian 等工具，简单的暴露和引用远程服务，通过配置服务的URL地址进行调用，通过 F5 等硬件进行负载均衡。 当服务越来越多时，服务 URL 配置管理变得非常困难，F5 硬件负载均衡器的单点压力也越来越大。 此时需要一个服务注册中心，动态的注册和发现服务，使服务的位置透明。并通过在消费方获取服务提供方地址列表，实现软负载均衡和 Failover，降低对 F5 硬件负载均衡器的依赖，也能减少部分成本。 当进一步发展，服务间依赖关系变得错踪复杂，甚至分不清哪个应用要在哪个应用之前启动，架构师都不能完整的描述应用的架构关系。 这时，需要自动画出应用间的依赖关系图，以帮助架构师理清理关系。 接着，服务的调用量越来越大，服务的容量问题就暴露出来，这个服务需要多少机器支撑？什么时候该加机器？ 为了解决这些问题，第一步，要将服务现在每天的调用量，响应时间，都统计出来，作为容量规划的参考指标。其次，要可以动态调整权重，在线上，将某台机器的权重一直加大，并在加大的过程中记录响应时间的变化，直到响应时间到达阀值，记录此时的访问量，再以此访问量乘以机器数反推总容量。 以上是 Dubbo 最基本的几个需求。","date":"2024-12-31","categories":["java"],"tags":["dubbo"]},{"title":"dubbo-学习和使用(8)-dubbo配置参考","url":"/2024/12/31/dubbo-学习和使用-8--dubbo配置参考/","content":"dubbo:service(dubbo:service)这个配置是针对服务端的，用来暴露服务端可以对外提供的服务信息 对应的配置类com.alibaba.dubbo.config.ServiceConfig 属性 对应URL 类型 是否必填 缺省值 作用 描述 兼容性 interface - class 必填 - 服务发现 服务接口名 1.0.0以上版本 ref - object 必填 - 服务发现 服务对象实现引用 1.0.0以上版本 version version string 可选 0.0.0 服务发现 服务版本，建议使用两位数字版本，如：1.0，通常在接口不兼容是版本号才需要升级 1.0.0以上版本 group group string 可选 - 服务发现 服务分组，当一个借口有多个实现，可以用分组区分 1.0.7以上版本 path string 可选 缺省为接口 服务发现 服务路径 1.0.12以上版本 delay delay int 可选 0 性能调优 延迟注册服务时间(毫秒)，设为-1时，表示延迟到Spring容器初始化完成时暴露服务 1.0.14以上版本 timeout timeout int 可选 1000 性能调优 远程服务调用超时时间(毫秒) 2.0.0以上版本 retries retries int 可选 2 性能调优 远程服务调用重试次数，不包括第一次调用，0为不需要重试 2.0.0以上版本 connections connections boolean 可选 100 性能调优 对每个提供者的最大连接数，rmi、http、hesslan等多连接协议表示限制连接数，dubbo等长链接协议表示建立的长连接个数 2.0.0以上版本 loadbalance loadbalance string 可选 random 性能调优 负载均衡策略，可选值：random，roundrobin，leastactice，分别表示：随机，轮询，最少活跃调用 2.0.0以上版本 async async boolean 可选 FALSE 性能调优 是否缺省异步执行，不可靠异步，只是忽略返回值，不阻塞执行线程 2.0.0以上版本 stub stub class&#x2F;boolean 可选 FALSE 服务治理 设为ture，表示使用缺省代理类名，即：接口名+Local后缀，服务接口客户端本地代理类名，用于在客户端执行本地逻辑，如本地缓存等。该本地代理类的构造函数必须允许传入远程代理对象，构造函数如：publicxxxServiceLocal(XxxServicexxxService) 2.0.0以上版本 mock mock class&#x2F;boolean 可选 FALSE 服务治理 设为true，表示使用缺省Mock类名，即：接口名+Mock后缀，服务接口调用失败Mock实现类，该Mock类必须有一个无参构造函数，与Local的区别在于，Local总是被执行，而Mock只在出现非业务异常(比如超时，网络异常等)时执行，Local在远程调用之前执行，Mock在远程调用后执行 2.0.0以上版本 token token string&#x2F;bollean 可选 FALSE 服务治理 令牌验证，为空表示不开启。如果为true，表示随机生成动态令牌，否则使用静态令牌。令牌的作用是防止消费者绕过注册中心直接访问，保证注册中心的授权功能有效。如果使用点对点调用，需要关闭令牌功能 2.0.0以上版本 registry - string 可选 缺省向所有registry注册 配置关联 向指定注册中心注册dubbo:registry的属性为id的value；在向多个注册中心注册时，多个id使用逗号分隔；如果不想讲该服务注册到任何registry，可将值设置N&#x2F;A 2.0.0以上版本 provider - string 可选 缺省使用第一个provider配置 配置关联 指定provider，值为dubbo:provider的id属性 2.0.0以上版本 deprecated deprecated boolean 可选 FALSE 服务治理 服务是否过时，如果设置为true，消费方引用时将打印服务过时警告error日志 2.0.5以上版本 dynamic dynamic boolean 可选 TRUE 服务治理 服务是否动态注册，如果设为false，注册后将显示disable状态，需要人工启用；并且服务提供者停止时，也不会自动取消注册，需要人工禁用 2.0.5以上版本 accesslog accesslog string&#x2F;boolean 可选 FALSE 服务治理 设为true，将向logger中输入访问日志，也可以填写访问日志文件路径，直接把访问日志输出到指定文件 2.0.5以上版本 owner owner string 可选 - 服务治理 服务责任人，用于服务治理，请填写公司负责人邮箱前缀 2.0.5以上版本 document document string 可选 - 服务治理 服务文档URL 1.0.5以上版本 weight weight int 可选 - 服务调优 服务权重 2.0.5以上版本 executes executes int 可选 0 性能调优 服务提供者每个服务方法最大可并行请求数 actives actives int 可选 0 性能调优 服务消费者每个服务方法最大并发调用数 proxy proxy string 可选 javassist 性能调优 省城动态代理方法，可选：jdk&#x2F;javassist 2.0.5以上版本 cluster cluster stirng 可选 failover 性能调优 集群方式，可选:failover&#x2F;failfast&#x2F;failsafe&#x2F;failback&#x2F;forking 2.0.5以上版本 filter service.filter string 可选 default 性能调优 服务提供方远程调用过程拦截器名称，多个名称用逗号分隔 2.0.5以上版本 listener exporter.listener string 可选 default 性能调优 服务提供方导出服务监听器名称，多个名称用逗号分隔 2.0.5以上版本 protocol - string 可选 - 配置关联 使用指定的协议暴露服务，在使用多协议时，dubbo:protocol的id属性的value,使用逗号分隔 2.0.5以上版本 layer layer string 可选 - 服务治理 服务提供者所在的分层。如：biz,dao,intl:web,china:acton 2.0.7以上版本 register register boolean 可选 TRUE 服务治理 该协议的服务是否注册到注册中心 2.0.8以上版本 注意：头四个是最重要的属性dubbo通过这些属性来唯一的标示一个service dubbo:reference这个配置是针对消费者端的，消费中通过这个类发起请求向服务端进行调用 对应的配置类com.alibaba.dubbo.config.ReferenceConfig 属性 对应URL 类型 是否必填 缺省值 作用 描述 兼容性 id - string 必填 - 配置关联 服务应用beanid 1.0.0以上版本 interface - class 必填 - 服务发现 服务接口名 1.0.0以上版本 version version string 可选 - 服务发现 服务版本，与服务提供者的版本保持一致 1.0.0以上版本 group group string 可选 - 服务发现 服务分组，当一个借口有多个实现，可以用分组区分，必须和服务提供方一致 1.0.7版本 timeout timeout long 可选 缺省使用dubbo:consumer的timeout 性能调优 服务方法调用超时时间(毫秒) 1.0.5以上版本 retries retries int 可选 缺省使用dubbo:consumer的retries 性能调优 远程服务调用重试次数，不包括第一次调用，不需要重试请设置为0 2.0.0以上版本 connections connections int 可选 缺省使用的connections 性能调优 对每个提供者的最大连接数，rmi、http、hessian等短连接协议表示限制连接数，dubbo等长连接协表示建立的长连接个数 2.0.0以上版本 loadbalance loadbalance string 可选 缺省使用的loadbalance 性能调优 负载均衡策略，可选值：random,roundrobin,leastactive，分别表示：随机，轮循，最少活跃调用 2.0.0以上版本 sync sync boolean 可选 缺省使用的async 性能调优 是否异步执行，不可靠异步，只是忽略返回值，不阻塞执行线程 2.0.0以上版本 generic generic boolean 可选 缺省使用的generic 服务治理 是否缺省泛化接口，如果为泛化接口，将返回GenericService 2.0.0以上版本 check check boolean 可选 缺省使用的check 服务治理 启动时检查提供者是否存在，true报错，false忽略 2.0.0以上版本 url string 可选 - 服务治理 点对点直连服务提供者地址，将绕过注册中心 1.0.6以上版本 stub stub class&#x2F;boolean 可选 - 服务治理 服务接口客户端本地代理类名，用于在客户端执行本地逻辑，如本地缓存等，该本地代理类的构造函数必须允许传入远程代理对象，构造函数如：publicXxxServiceLocal(XxxServicexxxService) 2.0.0以上版本 mock mock class&#x2F;boolean 可选 - 服务治理 服务接口调用失败Mock实现类名，该Mock类必须有一个无参构造函数，与Local的区别在于，Local总是被执行，而Mock只在出现非业务异常(比如超时，网络异常等)时执行，Local在远程调用之前执行，Mock在远程调用后执行 1.0.13以上版本 cache cache string&#x2F;boolean 可选 - 服务治理 以调用参数为key，缓存返回结果。可选：lru,threadlocal,jcache等 2.1.0以上版本 validation validation boolean 可选 - 服务治理 是否启用JSR303标准注解验证，如果启用，将对方法参数上的注解进行校验 2.1.0以上版本 proxy proxy boolean 可选 javassist 性能调优 选择动态代理实现策略，可选：javassist,jdk 2.0.2以上版本 client client string 可选 - 性能调优 客户端传输类型设置，如Dubbo协议的netty或mina 2.0.0以上版本 registry - string 可选 缺省将从所有注册中心获服务列表后合并结果 配置关联 从指定注册中心注册获取服务列表，在多个注册中心时使用，值为的id属性，多个注册中心ID用逗号分隔 2.0.0以上版本 owner owner string 可选 - 服务治理 调用服务负责人，用于服务治理，请填写负责人公司邮箱前缀 2.0.5以上版本 actives actives int 可选 0 性能调优 服务消费者每个服务方法最大并发调用数 2.0.5以上版本 cluster cluster string 可选 failover 性能优化 集群方式，可选：failover&#x2F;failfast&#x2F;failsafe&#x2F;failback&#x2F;forking 2.0.5以上版本 filter reference.filter string 可选 default 性能调优 服务消费方远程调用过程拦截器名称，多个名称用逗号分隔 2.0.5以上版本 listener invoker.listener string 可选 default 性能调优 服务消费方引用服务监听器名称，多个名称用逗号分隔 2.0.5以上版本 layer layer string 可选 - 服务治理 服务调用者所在的分层。如：biz、dao、intl:web、china:acton 2.0.7以上版本 init init boolean 可选 FALSE 性能调优 是否在afterPropertiesSet()时饥饿初始化引用，否则等到有人注入或引用该实例时再初始化 2.0.10以上版本 protocol protocol string 可选 - 服务治理 只调用指定协议的服务提供方，其他协议忽略 注意：头四个是最重要的属性dubbo通过这些属性来唯一的标示一个service dubbo:protocol(dubbo:protocol)服务提供者协议配置。对应的配置类：com.alibaba.dubbo.config.ProtocolConfig。同时，如果需要支持多协议，可以声明多个dubbo:protocol标签，并在dubbo:service中通过protocol属性指定使用的协议。 属性 对应URL参数 类型 是否必填 缺省值 作用 描述 兼容性 id string 可选 dubbo 配置关联 协议BeanId，可以在中引用此ID，如果ID不填，缺省和name属性值一样，重复则在name后加序号。 2.0.5以上版本 name string 必填 dubbo 性能调优 协议名称 2.0.5以上版本 port int 可选 dubbo协议缺省端口为20880，rmi协议缺省端口为1099，http和hessian协议缺省端口为80；如果没有配置port，则自动采用默认端口，如果配置为-1，则会分配一个没有被占用的端口。Dubbo2.4.0+，分配的端口在协议缺省端口的基础上增长，确保端口段可控。 服务发现 服务端口 2.0.5以上版本 host string 可选 自动查找本机IP 服务发现 -服务主机名，多网卡选择或指定VIP及域名时使用，为空则自动查找本机IP，-建议不要配置，让Dubbo自动获取本机IP 2.0.5以上版本 threadpool threadpool string 可选 fixed 性能调优 线程池类型，可选：fixed&#x2F;cached 2.0.5以上版本 threads threads int 可选 200 性能调优 服务线程池大小(固定大小) 2.0.5以上版本 iothreads threads int 可选 cpu个数+1 性能调优 io线程池大小(固定大小) 2.0.5以上版本 accepts accepts int 可选 0 性能调优 服务提供方最大可接受连接数 2.0.5以上版本 payload payload int 可选 8388608(&#x3D;8M) 性能调优 请求及响应数据包大小限制，单位：字节 2.0.5以上版本 codec codec string 可选 dubbo 性能调优 协议编码方式 2.0.5以上版本 serialization serialization string 可选 dubbo协议缺省为hessian2，rmi协议缺省为java，http协议缺省为json 性能调优 协议序列化方式，当协议支持多种序列化方式时使用，比如：dubbo协议的dubbo,hessian2,java,compactedjava，以及http协议的json等 2.0.5以上版本 accesslog accesslog string&#x2F;boolean 可选 服务治理 设为true，将向logger中输出访问日志，也可填写访问日志文件路径，直接把访问日志输出到指定文件 2.0.5以上版本 path string 可选 服务发现 提供者上下文路径，为服务path的前缀 2.0.5以上版本 transporter transporter string 可选 dubbo协议缺省为netty 性能调优 协议的服务端和客户端实现类型，比如：dubbo协议的mina,netty等，可以分拆为server和client配置 2.0.5以上版本 server server string 可选 dubbo协议缺省为netty，http协议缺省为servlet 性能调优 协议的服务器端实现类型，比如：dubbo协议的mina,netty等，http协议的jetty,servlet等 2.0.5以上版本 client client string 可选 dubbo协议缺省为netty 性能调优 协议的客户端实现类型，比如：dubbo协议的mina,netty等 2.0.5以上版本 dispatcher dispatcher string 可选 dubbo协议缺省为all 性能调优 协议的消息派发方式，用于指定线程模型，比如：dubbo协议的all,direct,message,execution,connection等 2.1.0以上版本 queues queues int 可选 0 性能调优 线程池队列大小，当线程池满时，排队等待执行的队列大小，建议不要设置，当线程程池时应立即失败，重试其它服务提供机器，而不是排队，除非有特殊需求。 2.0.5以上版本 charset charset string 可选 UTF-8 性能调优 序列化编码 2.0.5以上版本 buffer buffer int 可选 8192 性能调优 网络读写缓冲区大小 2.0.5以上版本 heartbeat heartbeat int 可选 0 性能调优 心跳间隔，对于长连接，当物理层断开时，比如拔网线，TCP的FIN消息来不及发送，对方收不到断开事件，此时需要心跳来帮助检查连接是否已断开 2.0.10以上版本 telnet telnet string 可选 服务治理 所支持的telnet命令，多个命令用逗号分隔 2.0.5以上版本 register register boolean 可选 TRUE 服务治理 该协议的服务是否注册到注册中心 2.0.8以上版本 contextpath contextpath String 可选 缺省为空串 服务治理 2.0.6以上版本 dubbo:registry(dubbo:registry)注册中心配置。对应的配置类： com.alibaba.dubbo.config.RegistryConfig。同时如果有多个不同的注册中心，可以声明多个 dubbo:registry 标签，并在 dubbo:service 或 dubbo:reference 的 registry 属性指定使用的注册中心。 属性 对应URL参数 类型 是否必填 缺省值 作用 描述 兼容性 id string 可选 配置关联 注册中心引用BeanId，可以在dubbo:serviceregistry=\"\"或dubbo:referenceregistry=\"\"中引用此ID 1.0.16以上版本 address host:port string 必填 服务发现 注册中心服务器地址，如果地址没有端口缺省为9090，同一集群内的多个地址用逗号分隔，如：ip:port,ip:port，不同集群的注册中心，请配置多个dubbo:registry标签 1.0.16以上版本 protocol string 可选 dubbo 服务发现 注同中心地址协议，支持dubbo,http,local三种协议，分别表示，dubbo地址，http地址，本地注册中心 2.0.0以上版本 port int 可选 9090 服务发现 注册中心缺省端口，当address没有带端口时使用此端口做为缺省值 2.0.0以上版本 username string 可选 服务治理 登录注册中心用户名，如果注册中心不需要验证可不填 2.0.0以上版本 password string 可选 服务治理 登录注册中心密码，如果注册中心不需要验证可不填 2.0.0以上版本 transport registry.transporter string 可选 netty 性能调优 网络传输方式，可选mina,netty 2.0.0以上版本 timeout registry.timeout int 可选 5000 性能调优 注册中心请求超时时间(毫秒) 2.0.0以上版本 session registry.session int 可选 60000 性能调优 注册中心会话超时时间(毫秒)，用于检测提供者非正常断线后的脏数据，比如用心跳检测的实现，此时间就是心跳间隔，不同注册中心实现不一样。 2.1.0以上版本 file registry.file string 可选 服务治理 使用文件缓存注册中心地址列表及服务提供者列表，应用重启时将基于此文件恢复，注意：两个注册中心不能使用同一文件存储 2.0.0以上版本 wait registry.wait int 可选 0 性能调优 停止时等待通知完成时间(毫秒) 2.0.0以上版本 check check boolean 可选 true 服务治理 注册中心不存在时，是否报错 2.0.0以上版本 register register boolean 可选 true 服务治理 是否向此注册中心注册服务，如果设为false，将只订阅，不注册 2.0.5以上版本 subscribe subscribe boolean 可选 true 服务治理 是否向此注册中心订阅服务，如果设为false，将只注册，不订阅 2.0.5以上版本 dynamic dynamic boolean 可选 true 服务治理 服务是否动态注册，如果设为false，注册后将显示后disable状态，需人工启用，并且服务提供者停止时，也不会自动取消册，需人工禁用。 2.0.5以上版本 dubbo:monitor监控中心配置。对应的配置类： com.alibaba.dubbo.config.MonitorConfig 属性 对应URL参数 类型 是否必填 缺省值 作用 描述 兼容性 protocol protocol string 可选 dubbo 服务治理 监控中心协议，如果为protocol&#x3D;”registry”，表示从注册中心发现监控中心地址，否则直连监控中心。 2.0.9以上版本 address string 可选 N&#x2F;A 服务治理 直连监控中心服务器地址，address&#x3D;”10.20.130.230:12080” 1.0.16以上版本 dubbo:applicationdubbo:application应用信息配置。对应的配置类：com.alibaba.dubbo.config.ApplicationConfig 属性 对应URL参数 类型 是否必填 缺省值 作用 描述 兼容性 name application string 必填 服务治理 当前应用名称，用于注册中心计算应用间依赖关系，注意：消费者和提供者应用名不要一样，此参数不是匹配条件，你当前项目叫什么名字就填什么，和提供者消费者角色无关，比如：kylin应用调用了morgan应用的服务，则kylin项目配成kylin，morgan项目配成morgan，可能kylin也提供其它服务给别人使用，但kylin项目永远配成kylin，这样注册中心将显示kylin依赖于morgan 1.0.16以上版本 version application.version string 可选 服务治理 当前应用的版本 2.2.0以上版本 owner owner string 可选 服务治理 应用负责人，用于服务治理，请填写负责人公司邮箱前缀 2.0.5以上版本 organization organization string 可选 服务治理 组织名称(BU或部门)，用于注册中心区分服务来源，此配置项建议不要使用autoconfig，直接写死在配置中，比如china,intl,itu,crm,asc,dw,aliexpress等 2.0.0以上版本 architecture architecture string 可选 服务治理 用于服务分层对应的架构。如，intl、china。不同的架构使用不同的分层。 2.0.7以上版本 environment environment string 可选 服务治理 应用环境，如：develop&#x2F;test&#x2F;product，不同环境使用不同的缺省值，以及作为只用于开发测试功能的限制条件 2.0.0以上版本 compiler compiler string 可选 javassist 性能优化 Java字节码编译器，用于动态类的生成，可选：jdk或javassist 2.1.0以上版本 logger logger string 可选 slf4j 性能优化 日志输出方式，可选：slf4j,jcl,log4j,jdk 2.2.0以上版本 dubbo:moduledubbo:module模块信息配置。对应的配置类 com.alibaba.dubbo.config.ModuleConfig 属性 对应URL参数 类型 是否必填 缺省值 作用 描述 兼容性 name module string 必填 服务治理 当前模块名称，用于注册中心计算模块间依赖关系 2.2.0以上版本 version module.version string 可选 服务治理 当前模块的版本 2.2.0以上版本 owner owner string 可选 服务治理 模块负责人，用于服务治理，请填写负责人公司邮箱前缀 2.2.0以上版本 organization organization string 可选 服务治理 组织名称(BU或部门)，用于注册中心区分服务来源，此配置项建议不要使用autoconfig，直接写死在配置中，比如china,intl,itu,crm,asc,dw,aliexpress等 2.2.0以上版本 dubbo:provider(<dubbo：provider>)服务提供者缺省值配置。对应的配置类： com.alibaba.dubbo.config.ProviderConfig。同时该标签为 dubbo:service 和 dubbo:protocol 标签的缺省值设置。（就是service和service对应的protocol的默认配置） 属性 对应URL参数 类型 是否必填 缺省值 作用 描述 兼容性 id string 可选 dubbo 配置关联 协议BeanId，可以在dubbo:serviceproivder=\"\"中引用此ID 1.0.16以上版本 protocol string 可选 dubbo 性能调优 协议名称 1.0.16以上版本 host string 可选 自动查找本机IP 服务发现 服务主机名，多网卡选择或指定VIP及域名时使用，为空则自动查找本机IP，建议不要配置，让Dubbo自动获取本机IP 1.0.16以上版本 threads threads int 可选 200 性能调优 服务线程池大小(固定大小) 1.0.16以上版本 payload payload int 可选 8388608(&#x3D;8M) 性能调优 请求及响应数据包大小限制，单位：字节 2.0.0以上版本 path string 可选 服务发现 提供者上下文路径，为服务path的前缀 2.0.0以上版本 server server string 可选 dubbo协议缺省为netty，http协议缺省为servlet 性能调优 协议的服务器端实现类型，比如：dubbo协议的mina,netty等，http协议的jetty,servlet等 2.0.0以上版本 client client string 可选 dubbo协议缺省为netty 性能调优 协议的客户端实现类型，比如：dubbo协议的mina,netty等 2.0.0以上版本 codec codec string 可选 dubbo 性能调优 协议编码方式 2.0.0以上版本 serialization serialization string 可选 dubbo协议缺省为hessian2，rmi协议缺省为java，http协议缺省为json 性能调优 协议序列化方式，当协议支持多种序列化方式时使用，比如：dubbo协议的dubbo,hessian2,java,compactedjava，以及http协议的json,xml等 2.0.5以上版本 default boolean 可选 FALSE 配置关联 是否为缺省协议，用于多协议 1.0.16以上版本 filter service.filter string 可选 性能调优 服务提供方远程调用过程拦截器名称，多个名称用逗号分隔 2.0.5以上版本 listener exporter.listener string 可选 性能调优 服务提供方导出服务监听器名称，多个名称用逗号分隔 2.0.5以上版本 threadpool threadpool string 可选 fixed 性能调优 线程池类型，可选：fixed&#x2F;cached 2.0.5以上版本 accepts accepts int 可选 0 性能调优 服务提供者最大可接受连接数 2.0.5以上版本 version version string 可选 0.0.0 服务发现 服务版本，建议使用两位数字版本，如：1.0，通常在接口不兼容时版本号才需要升级 2.0.5以上版本 group group string 可选 服务发现 服务分组，当一个接口有多个实现，可以用分组区分 2.0.5以上版本 delay delay int 可选 0 性能调优 延迟注册服务时间(毫秒)，设为1时，表示延迟到Spring容器初始化完成时暴露服务 2.0.5以上版本 timeout default.timeout int 可选 1000 性能调优 远程服务调用超时时间(毫秒) 2.0.5以上版本 retries default.retries int 可选 2 性能调优 远程服务调用重试次数，不包括第一次调用，不需要重试请设为0 2.0.5以上版本 connections default.connections int 可选 0 性能调优 对每个提供者的最大连接数，rmi、http、hessian等短连接协议表示限制连接数，dubbo等长连接协表示建立的长连接个数 2.0.5以上版本 loadbalance default.loadbalance string 可选 random 性能调优 负载均衡策略，可选值：random,roundrobin,leastactive，分别表示：随机，轮循，最少活跃调用 2.0.5以上版本 async default.async boolean 可选 FALSE 性能调优 是否缺省异步执行，不可靠异步，只是忽略返回值，不阻塞执行线程 2.0.5以上版本 stub stub boolean 可选 FALSE 服务治理 设为true，表示使用缺省代理类名，即：接口名+Local后缀。 2.0.5以上版本 mock mock boolean 可选 FALSE 服务治理 设为true，表示使用缺省Mock类名，即：接口名+Mock后缀。 2.0.5以上版本 token token boolean 可选 FALSE 服务治理 令牌验证，为空表示不开启，如果为true，表示随机生成动态令牌 2.0.5以上版本 registry registry string 可选 缺省向所有registry注册 配置关联 向指定注册中心注册，在多个注册中心时使用，值为dubbo:registry的id属性，多个注册中心ID用逗号分隔，如果不想将该服务注册到任何registry，可将值设为N&#x2F;A 2.0.5以上版本 dynamic dynamic boolean 可选 TRUE 服务治理 服务是否动态注册，如果设为false，注册后将显示后disable状态，需人工启用，并且服务提供者停止时，也不会自动取消册，需人工禁用。 2.0.5以上版本 accesslog accesslog string&#x2F;boolean 可选 FALSE 服务治理 设为true，将向logger中输出访问日志，也可填写访问日志文件路径，直接把访问日志输出到指定文件 2.0.5以上版本 owner owner string 可选 服务治理 服务负责人，用于服务治理，请填写负责人公司邮箱前缀 2.0.5以上版本 document document string 可选 服务治理 服务文档URL 2.0.5以上版本 weight weight int 可选 性能调优 服务权重 2.0.5以上版本 executes executes int 可选 0 性能调优 服务提供者每服务每方法最大可并行执行请求数 2.0.5以上版本 actives default.actives int 可选 0 性能调优 每服务消费者每服务每方法最大并发调用数 2.0.5以上版本 proxy proxy string 可选 javassist 性能调优 生成动态代理方式，可选：jdk&#x2F;javassist 2.0.5以上版本 cluster default.cluster string 可选 failover 性能调优 集群方式，可选：failover&#x2F;failfast&#x2F;failsafe&#x2F;failback&#x2F;forking 2.0.5以上版本 deprecated deprecated boolean 可选 FALSE 服务治理 服务是否过时，如果设为true，消费方引用时将打印服务过时警告error日志 2.0.5以上版本 queues queues int 可选 0 性能调优 线程池队列大小，当线程池满时，排队等待执行的队列大小，建议不要设置，当线程程池时应立即失败，重试其它服务提供机器，而不是排队，除非有特殊需求。 2.0.5以上版本 charset charset string 可选 UTF8 性能调优 序列化编码 2.0.5以上版本 buffer buffer int 可选 8192 性能调优 网络读写缓冲区大小 2.0.5以上版本 iothreads iothreads int 可选 CPU+1 性能调优 IO线程池，接收网络读写中断，以及序列化和反序列化，不处理业务，业务线程池参见threads配置，此线程池和CPU相关，不建议配置。 2.0.5以上版本 telnet telnet string 可选 服务治理 所支持的telnet命令，多个命令用逗号分隔 2.0.5以上版本 dubbo:service contextpath contextpath String 可选 缺省为空串 服务治理 layer layer string 可选 服务治理 服务提供者所在的分层。如：biz、dao、intl:web、china:acton。 2.0.7以上版本 dubbo:consumer(dubbo:consumer)服务消费者缺省值配置。配置类： com.alibaba.dubbo.config.ConsumerConfig 。同时该标签为 dubbo:reference 标签的缺省值设置。 属性 对应URL参数 类型 是否必填 缺省值 作用 描述 兼容性 timeout default.timeout int 可选 1000 性能调优 远程服务调用超时时间(毫秒) 1.0.16以上版本 retries default.retries int 可选 2 性能调优 远程服务调用重试次数，不包括第一次调用，不需要重试请设为0 1.0.16以上版本 loadbalance default.loadbalance string 可选 random 性能调优 负载均衡策略，可选值：random,roundrobin,leastactive，分别表示：随机，轮循，最少活跃调用 1.0.16以上版本 async default.async boolean 可选 FALSE 性能调优 是否缺省异步执行，不可靠异步，只是忽略返回值，不阻塞执行线程 2.0.0以上版本 connections default.connections int 可选 100 性能调优 每个服务对每个提供者的最大连接数，rmi、http、hessian等短连接协议支持此配置，dubbo协议长连接不支持此配置 1.0.16以上版本 generic generic boolean 可选 FALSE 服务治理 是否缺省泛化接口，如果为泛化接口，将返回GenericService 2.0.0以上版本 check check boolean 可选 TRUE 服务治理 启动时检查提供者是否存在，true报错，false忽略 1.0.16以上版本 proxy proxy string 可选 javassist 性能调优 生成动态代理方式，可选：jdk&#x2F;javassist 2.0.5以上版本 owner owner string 可选 服务治理 调用服务负责人，用于服务治理，请填写负责人公司邮箱前缀 2.0.5以上版本 actives default.actives int 可选 0 性能调优 每服务消费者每服务每方法最大并发调用数 2.0.5以上版本 cluster default.cluster string 可选 failover 性能调优 集群方式，可选：failover&#x2F;failfast&#x2F;failsafe&#x2F;failback&#x2F;forking 2.0.5以上版本 filter reference.filter string 可选 性能调优 服务消费方远程调用过程拦截器名称，多个名称用逗号分隔 2.0.5以上版本 listener invoker.listener string 可选 性能调优 服务消费方引用服务监听器名称，多个名称用逗号分隔 2.0.5以上版本 registry string 可选 缺省向所有registry注册 配置关联 向指定注册中心注册，在多个注册中心时使用，值为dubbo:registry的id属性，多个注册中心ID用逗号分隔，如果不想将该服务注册到任何registry，可将值设为N&#x2F;A 2.0.5以上版本 layer layer string 可选 服务治理 服务调用者所在的分层。如：biz、dao、intl:web、china:acton。 2.0.7以上版本 init init boolean 可选 FALSE 性能调优 是否在afterPropertiesSet()时饥饿初始化引用，否则等到有人注入或引用该实例时再初始化。 2.0.10以上版本 cache cache string&#x2F;boolean 可选 服务治理 以调用参数为key，缓存返回结果，可选：lru,threadlocal,jcache等 Dubbo2.1.0及其以上版本支持 validation validation boolean 可选 服务治理 是否启用JSR303标准注解验证，如果启用，将对方法参数上的注解进行校验 Dubbo2.1.0及其以上版本支持 timeout default.timeout int 可选 1000 性能调优 远程服务调用超时时间(毫秒) 2.0.5以上版本 retries default.retries int 可选 2 性能调优 远程服务调用重试次数，不包括第一次调用，不需要重试请设为0 2.0.5以上版本 connections default.connections int 可选 0 性能调优 对每个提供者的最大连接数，rmi、http、hessian等短连接协议表示限制连接数，dubbo等长连接协表示建立的长连接个数 2.0.5以上版本 loadbalance default.loadbalance string 可选 random 性能调优 负载均衡策略，可选值：random,roundrobin,leastactive，分别表示：随机，轮循，最少活跃调用 2.0.5以上版本 async default.async boolean 可选 FALSE 性能调优 是否缺省异步执行，不可靠异步，只是忽略返回值，不阻塞执行线程 2.0.5以上版本 stub stub boolean 可选 FALSE 服务治理 设为true，表示使用缺省代理类名，即：接口名+Local后缀。 2.0.5以上版本 mock mock boolean 可选 FALSE 服务治理 设为true，表示使用缺省Mock类名，即：接口名+Mock后缀。 2.0.5以上版本 token token boolean 可选 FALSE 服务治理 令牌验证，为空表示不开启，如果为true，表示随机生成动态令牌 2.0.5以上版本 registry registry string 可选 缺省向所有registry注册 配置关联 向指定注册中心注册，在多个注册中心时使用，值为dubbo:registry的id属性，多个注册中心ID用逗号分隔，如果不想将该服务注册到任何registry，可将值设为N&#x2F;A 2.0.5以上版本 dynamic dynamic boolean 可选 TRUE 服务治理 服务是否动态注册，如果设为false，注册后将显示后disable状态，需人工启用，并且服务提供者停止时，也不会自动取消册，需人工禁用。 2.0.5以上版本 accesslog accesslog string&#x2F;boolean 可选 FALSE 服务治理 设为true，将向logger中输出访问日志，也可填写访问日志文件路径，直接把访问日志输出到指定文件 2.0.5以上版本 owner owner string 可选 服务治理 服务负责人，用于服务治理，请填写负责人公司邮箱前缀 2.0.5以上版本 document document string 可选 服务治理 服务文档URL 2.0.5以上版本 weight weight int 可选 性能调优 服务权重 2.0.5以上版本 executes executes int 可选 0 性能调优 服务提供者每服务每方法最大可并行执行请求数 2.0.5以上版本 actives default.actives int 可选 0 性能调优 每服务消费者每服务每方法最大并发调用数 2.0.5以上版本 proxy proxy string 可选 javassist 性能调优 生成动态代理方式，可选：jdk&#x2F;javassist 2.0.5以上版本 cluster default.cluster string 可选 failover 性能调优 集群方式，可选：failover&#x2F;failfast&#x2F;failsafe&#x2F;failback&#x2F;forking 2.0.5以上版本 deprecated deprecated boolean 可选 FALSE 服务治理 服务是否过时，如果设为true，消费方引用时将打印服务过时警告error日志 2.0.5以上版本 queues queues int 可选 0 性能调优 线程池队列大小，当线程池满时，排队等待执行的队列大小，建议不要设置，当线程程池时应立即失败，重试其它服务提供机器，而不是排队，除非有特殊需求。 2.0.5以上版本 charset charset string 可选 UTF8 性能调优 序列化编码 2.0.5以上版本 buffer buffer int 可选 8192 性能调优 网络读写缓冲区大小 2.0.5以上版本 iothreads iothreads int 可选 CPU+1 性能调优 IO线程池，接收网络读写中断，以及序列化和反序列化，不处理业务，业务线程池参见threads配置，此线程池和CPU相关，不建议配置。 2.0.5以上版本 telnet telnet string 可选 服务治理 所支持的telnet命令，多个命令用逗号分隔 2.0.5以上版本 dubbo:service contextpath contextpath String 可选 缺省为空串 服务治理 layer layer string 可选 服务治理 服务提供者所在的分层。如：biz、dao、intl:web、china:acton。 2.0.7以上版本 dubbo:method (dubbo:method)方法级配置。对应的配置类： com.alibaba.dubbo.config.MethodConfig。同时该标签为 dubbo:service 或 dubbo:reference 的子标签，用于控制到方法级。 属性 对应URL参数 类型 是否必填 缺省值 作用 描述 兼容性 name string 必填 标识 方法名 1.0.8以上版本 timeout .timeout int 可选 缺省为的timeout 性能调优 方法调用超时时间(毫秒) 1.0.8以上版本 retries .retries int 可选 缺省为dubbo:reference的retries 性能调优 远程服务调用重试次数，不包括第一次调用，不需要重试请设为0 2.0.0以上版本 loadbalance .loadbalance string 可选 缺省为的loadbalance 性能调优 负载均衡策略，可选值：random,roundrobin,leastactive，分别表示：随机，轮循，最少活跃调用 2.0.0以上版本 async .async boolean 可选 缺省为dubbo:reference的async 性能调优 是否异步执行，不可靠异步，只是忽略返回值，不阻塞执行线程 1.0.9以上版本 sent .sent boolean 可选 TRUE 性能调优 异步调用时，标记sent&#x3D;true时，表示网络已发出数据 2.0.6以上版本 actives .actives int 可选 0 性能调优 每服务消费者最大并发调用限制 2.0.5以上版本 executes .executes int 可选 0 性能调优 每服务每方法最大使用线程数限制，此属性只在dubbo:method作为dubbo:service子标签时有效 2.0.5以上版本 deprecated .deprecated boolean 可选 FALSE 服务治理 服务方法是否过时，此属性只在dubbo:method作为dubbo:service子标签时有效 2.0.5以上版本 sticky .sticky boolean 可选 FALSE 服务治理 设置true该接口上的所有方法使用同一个provider.如果需要更复杂的规则，请使用用路由 2.0.6以上版本 return .return boolean 可选 TRUE 性能调优 方法调用是否需要返回值,async设置为true时才生效，如果设置为true，则返回future，或回调onreturn等方法，如果设置为false，则请求发送成功后直接返回Null 2.0.6以上版本 oninvoke attribute属性，不在URL中体现 String 可选 性能调优 方法执行前拦截 2.0.6以上版本 onreturn attribute属性，不在URL中体现 String 可选 性能调优 方法执行返回后拦截 2.0.6以上版本 onthrow attribute属性，不在URL中体现 String 可选 性能调优 方法执行有异常拦截 2.0.6以上版本 cache .cache string&#x2F;boolean 可选 服务治理 以调用参数为key，缓存返回结果，可选：lru,threadlocal,jcache等 Dubbo2.1.0及其以上版本支持 validation .validation boolean 可选 服务治理 是否启用JSR303标准注解验证，如果启用，将对方法参数上的注解进行校验 Dubbo2.1.0及其以上版本支持 代码实例 dubbo:argument(dubbo:argument)方法参数配置。对应的配置类： com.alibaba.dubbo.config.ArgumentConfig。该标签为 dubbo:method 的子标签，用于方法参数的特征描述，比如： 属性 对应URL参数 类型 是否必填 缺省值 作用 描述 兼容性 index int 必填 标识 方法名 2.0.6以上版本 type String 与index二选一 标识 通过参数类型查找参数的index 2.0.6以上版本 callback .retries boolean 可选 服务治理 参数是否为callback接口，如果为callback，服务提供方将生成反向代理，可以从服务提供方反向调用消费方，通常用于事件推送. 2.0.6以上版本 dubbo:parameter()选项参数配置。对应的配置类：java.util.Map。同时该标签为dubbo:protocol或dubbo:service或dubbo:provider或dubbo:reference或dubbo:consumer的子标签，用于配置自定义参数，该配置项将作为扩展点设置自定义参数使用。 属性 对应URL参数 类型 是否必填 缺省值 作用 描述 兼容性 key key string 必填 服务治理 路由参数键 2.0.0以上版本 value value string 必填 服务治理 路由参数值 2.0.0以上版本","date":"2024-12-31","categories":["java"],"tags":["dubbo"]},{"title":"dubbo-深入(1)-基本使用初探","url":"/2024/12/31/dubbo-深入-1--基本使用初探/","content":"深入dubbo（一）基本使用初探互联网架构升级过程图 单一应用架构(单机部署所有的应用)——>垂直应用架构(将单机的应用部署到相关的各种分开的服务器上，各个服务器相互独立)——>分布式服务架构(在之前的各个部分出现了交互的过程)——>流动计算架构soa(当系统变得复杂，各种交互混乱，玉树出现了基于中央调度的整合配置机制)——>微服务 dubbo的作用 作为服务注册中心，动态的注册和发现服务，使服务的位置透明并通过在消费方获取服务提供方地址列表，实现软负载均衡和 Failover（失效备份），降低对 F5 硬件负载均衡器的依赖 自动画出应用间的依赖关系图，以帮助架构师理清理关系。 将服务现在每天的调用量，响应时间，都统计出来，作为容量规划的参考指标，可以动态调整权重，在线上，将某台机器的权重一直加大，并在加大的过程中记录响应时间的变化，直到响应时间到达阀值，记录此时的访问量，再以此访问量乘以机器数反推总容量 dubbo的基本架构 节点 角色说明 Provider 暴露服务的服务提供方 Consumer 调用远程服务的服务消费方 Registry 服务注册与发现的注册中心 Monitor 统计服务的调用次调和调用时间的监控中心 Container 服务运行容器 基本使用方法引申，其实dubbo并没有那么难，首先他需要的一组公共的接口，作为对外面提供的服务的入口，dubbo框架处理了寻找服务提供者等过程，使用者可以就像使用spring框架那样进行dubbo的使用，见下面的例子 公共的接口 注意：这个公共的接口一定要有包名，否则dubbo内部的反射处理方法将会报空值错误 提供maven贡其他组件使用 provider服务提供者 xml配置文件，注意对外提供的服务一定要是接口 接口实现类 服务启动入口 注意：linux系统需要修改&#x2F;etc&#x2F;hosts文件将hostname对应的ip地址改成现在电脑对应网卡的ip地址 consumer 消费者 xml配置文件 消费者程序入口","date":"2024-12-31","categories":["java"],"tags":["dubbo"]},{"title":"dubbo-深入(3)-注册中心","url":"/2024/12/31/dubbo-深入-3--注册中心/","content":"深入dubbo（二）注册中心注册中心只要解决的问题就是，服务端和消费端的相互发现的问题，断开重链接的问题 基于广播实现注册的Multicast 注册中心Multicast 注册中心不需要启动任何中心节点，只要广播地址一样，就可以互相发现。 提供方启动时广播自己的地址 消费方启动时广播订阅请求 提供方收到订阅请求时，单播自己的地址给订阅者，如果设置了 unicast&#x3D;false，则广播给订阅者 消费方收到提供方地址时，连接该地址进行 RPC 调用。 配置方法 为了减少广播量，Dubbo 缺省使用单播发送提供者地址信息给消费者，如果一个机器上同时启了多个消费者进程，消费者需声明 unicast&#x3D;false，否则只会有一个消费者能收到消息 注意问题,hostname需要为本地的ip地址（不能是127.0.0.1），ubuntu系统需要修改hosts文件 使用基于zookeeper的注册中心注册依赖于zookeeper底层树实现 流程说明： 服务提供者启动时: 向 &#x2F;dubbo&#x2F;com.foo.BarService&#x2F;providers 目录下写入自己的 URL 地址 服务消费者启动时: 订阅 &#x2F;dubbo&#x2F;com.foo.BarService&#x2F;providers 目录下的提供者 URL 地址。并向 &#x2F;dubbo&#x2F;com.foo.BarService&#x2F;consumers 目录下写入自己的 URL 地址 监控中心启动时: 订阅 &#x2F;dubbo&#x2F;com.foo.BarService 目录下的所有提供者和消费者 URL 地址。 功能特点： 提供者出现断电等异常停机时，注册中心能自动删除提供者信息，注册中心重启时，能自动恢复注册数据，以及订阅请求，会话过期时，能自动恢复注册数据，以及订阅请求 <dubbo:registry check&#x3D;“false“ &#x2F;> 时，记录失败注册和订阅请求，后台定时重试 通过 <dubbo:registry username&#x3D;“admin“ password&#x3D;“1234“ &#x2F;> 设置 zookeeper 登录信息 <dubbo:registry group&#x3D;“dubbo“ &#x2F;> 设置 zookeeper 的根节点，不设置将使用无根树 号通配符 <dubbo:reference group&#x3D;“*“ version&#x3D;“*“ &#x2F;>，可订阅服务的所有分组和所有版本的提供者 注意：使用zk进行注册中心配置的时候，需要对依赖包尽进行配置 Zookeeper 单机配置: Zookeeper 集群配置： 同一 Zookeeper，分成多组注册中心: 使用redis作为配置中心 使用 Redis 的 Key&#x2F;Map 结构存储数据结构：主 Key 为服务名和类型，Map 中的 Key 为 URL 地址，Map 中的 Value 为过期时间，用于判断脏数据，脏数据由监控中心删除 具体流程将上图，因为使用很少，暂时不做讨论","date":"2024-12-31","categories":["java"],"tags":["dubbo"]},{"title":"dubbo-深入(3)-配置参考","url":"/2024/12/31/dubbo-深入-3--配置参考/","content":"深入dubbo（三）配置参考配置项分为三大类：服务发现：表示该配置项用于服务的注册与发现，目的是让消费方找到提供方。服务治理：表示该配置项用于治理服务间的关系，或为开发测试提供便利条件。性能调优：表示该配置项用于调优性能，不同的选项对性能会产生影响。 注意：只有 group，interface，version 是服务的匹配条件，三者决定是不是同一个服务，其它配置项均为调优和治理参数。 启动时检查参数 check 使用标签 reference consumer registry xml配置法 配置文件 Dubbo 缺省会在启动时检查依赖的服务是否可用，可以通过 check&#x3D;”false” 关闭检查（有些服务不关心，或者出现了循环依赖，必须有一方先启动， 比如Spring 容器是懒加载的，或者通过 API 编程延迟引用服务） 集群容错配置 默认Failover Cluster 失败自动切换，当出现失败，重试其它服务器 1。通常用于读操作，但重试会带来更长延迟。可通过 retries&#x3D;”2″ 来设置重试次数(不含第一次)。配置如下 Failfast Cluster 快速失败，只发起一次调用，失败立即报错。通常用于非幂等性的写操作，比如新增记录。 Failsafe Cluster 失败安全，出现异常时，直接忽略。通常用于写入审计日志等操作。 Failback Cluster 失败自动恢复，后台记录失败请求，定时重发。通常用于消息通知操作。 Forking Cluster 并行调用多个服务器，只要一个成功即返回。通常用于实时性要求较高的读操作，但需要浪费更多服务资源。可通过 forks&#x3D;”2″ 来设置最大并行数。 Broadcast Cluster广播调用所有提供者，逐个调用，任意一台报错则报错 2。通常用于通知所有提供者更新缓存或日志等本地资源信息。 集群模式配置以上六种按照以下示例在服务提供方和消费方配置集群模式 负载均衡配置 Random LoadBalance随机，按权重设置随机概率。在一个截面上碰撞的概率高，但调用量越大分布越均匀，而且按概率使用权重后也比较均匀，有利于动态调整提供者权重。 RoundRobin LoadBalance轮循，按公约后的权重设置轮循比率。存在慢的提供者累积请求的问题，比如：第二台机器很慢，但没挂，当请求调到第二台时就卡在那，久而久之，所有请求都卡在调到第二台上。 LeastActive LoadBalance最少活跃调用数，相同活跃数的随机，活跃数指调用前后计数差。使慢的提供者收到更少请求，因为越慢的提供者的调用前后计数差会越大。 ConsistentHash LoadBalance一致性 Hash，相同参数的请求总是发到同一提供者。 当某一台提供者挂时，原本发往该提供者的请求，基于虚拟节点，平摊到其它提供者，不会引起剧烈变动。 缺省只对第一个参数 Hash，如果要修改，请配置 <dubbo:parameter key&#x3D;”hash.arguments” value&#x3D;”0,1″ &#x2F;> 缺省用 160 份虚拟节点，如果要修改，请配置 <dubbo:parameter key&#x3D;”hash.nodes” value&#x3D;”320″ &#x2F;> 配置方法 服务端服务级别 客户端服务级别 服务端方法级别 客户端方法级别 线程池对于Dubbo的服务提供者，主要有两种线程池，一种是IO处理线程池，另一种是服务调用线程池。而作为IO处理线程池，由于Dubbo基于Mina、Grizzly和Netty框架做IO组件，IO线程池都是基于这些框架来配置，比如Netty中的boss和worker线程池，Dubbo选择的是“无边界”的CachedThreadPool，这意味着对所有服务请求先做到“来者不拒”，但它进一步限制了IO处理的线程数，默认是核数+1 如果事件处理的逻辑能迅速完成，并且不会发起新的 IO 请求，比如只是在内存中记个标识，则直接在 IO 线程上处理更快，如果事件处理逻辑较慢，或者需要发起新的IO请求，比如需要查询数据库，则必须派发到IO线程池中 Dispatcher all 所有消息都派发到线程池，包括请求，响应，连接事件，断开事件，心跳等。 direct 所有消息都不派发到线程池，全部在 IO 线程上直接执行。 message 只有请求响应消息派发到线程池，其它连接断开事件，心跳等消息，直接在 IO 线程上执行。 execution 只请求消息派发到线程池，不含响应，响应和其它连接断开事件，心跳等消息，直接在 IO 线程上执行。 connection 在 IO 线程上，将连接断开事件放入队列，有序逐个执行，其它消息派发到线程池。 ThreadPool fixed 固定大小线程池，启动时建立线程，不关闭，一直持有。(缺省) cached 缓存线程池，空闲一分钟自动删除，需要时重建。 limited 可伸缩线程池，但池中的线程数只会增长不会收缩。只增长不收缩的目的是为了避免收缩时突然来了大流量引起的性能问题。 直接链接和只订阅模式在开发及测试环境下，经常需要绕过注册中心，只测试指定服务提供者，这时候可能需要点对点直连 点对点，可在 dubbo:reference 中配置 url 指向提供者，将绕过注册中心，多个地址用分号隔开 注意：为了避免复杂化线上环境，不要在线上使用这个功能，只应在测试阶段使用。 只订阅模式：一般和直连连用，解决在测试环境下注册中心共用可能导致，线上业务受到影响的问题 禁用注册配置 注册到指定的服务器 使用多个协议和使用多个注册中心Dubbo 允许配置多协议，在不同服务上支持不同协议或者同一服务上同时支持多种协议。 Dubbo 支持同一服务向多注册中心同时注册，或者不同服务分别注册到不同的注册中心上去，甚至可以同时引用注册在不同注册中心上的同名服务。 同时链接到多个不同的注册中心 service 省略 registry属性将自动的使用默认配置 分组聚合按组合并返回结果 ，比如菜单服务，接口一样，但有多种实现，用group区分，现在消费方需从每种group中调用一次返回结果，合并结果返回，这样就可以实现聚合菜单项。 注意：dubbo只有默认的四种组合，list set map array ，所以返回的结果值必须要是以上几种类型 结果缓存结果缓存 ，用于加速热门数据的访问速度，Dubbo 提供声明式缓存，以减少用户加缓存的工作量 2。 回声测试回声测试用于检测服务是否可用，回声测试按照正常请求流程执行，能够测试整个调用是否通畅，可用于监控。 所有服务自动实现 EchoService 接口，只需将任意服务引用强制转型为 EchoService，即可使用。 上下文信息获取和从客户端向服务端传递参数上下文中存放的是当前调用过程中所需的环境信息。所有配置信息都将转换为 URL 的参数 RpcContext 是一个 ThreadLocal 的临时状态记录器，当接收到 RPC 请求，或发起 RPC 请求时，RpcContext 的状态都会变化。比如：A 调 B，B 再调 C，则 B 机器上，在 B 调 C 之前，RpcContext 记录的是 A 调 B 的信息，在 B 调 C 之后，RpcContext 记录的是 B 调 C 的信息。 对于服务的消费方，注意只有在使用rpc调用之后才状态会发生改变 服务提供方，注意只有在服务类中进行注册才能坚挺到数据 参数传递流程图 在服务消费方端设置隐式参数 在服务提供方端获取隐式参数 异步调用 注意:要在客户端进行配置 调用代码 你也可以设置是否等待消息发出 sent&#x3D;”true” 等待消息发出，消息发送失败将抛出异常。 sent&#x3D;”false” 不等待消息发出，将消息放入 IO 队列，即刻返回。 如果你只是想异步，完全忽略返回值，可以配置 return&#x3D;”false”，以减少 Future 对象的创建和管理成本 注意:如果服务端长时间进行堵塞，客户端可能会发生超时问题，所以有时可能要在前台设置超时时间 坑点：dubbo的一部调用会延续一次，也就是说a一部调用b，b同步调用c，其实上b还是异步调用c dubbo内置调用拦截在本地拦截方法调用过程中的 before after throww return过程 其实相当于为dubbo远程调用的处理工程提供了统一的模板 （注意dubbo的哲学思想——提供一个公共的api，使用api进行相互的调用） 服务端配置方法 公共api提供，Barservice接口的Stub实现 本地伪装 通常用于服务降级，比如某验权服务，当服务提供方全部挂掉后，客户端不抛出异常，而是通过 Mock 数据返回授权失败。 服务端配置方法 公共api提供接口的Mock实现 这样当发生调用异常的时候，将会调用上面Mock类相应的方法，使用调用 服务懒链接和延迟暴露和对应调用 并发控制和链接控制服务端和客户端并发控制配置 注意：服务端的并发执行参数控制只能在服务端配置，配置参数executes ， 客户端 并发执行参数，可在服务端或者客户端配置 参数active 客户端优先 服务端链接控制 客户端链接控制 一个概念：范化引用泛化接口调用方式主要用于客户端没有 API 接口及模型类元的情况，参数及返回值中的所有 POJO 均用 Map 表示，通常用于框架集成，","date":"2024-12-31","categories":["java"],"tags":["dubbo"]},{"title":"dubbo-深入(4)- 集群架构设计","url":"/2024/12/31/dubbo-深入-4---集群架构设计/","content":"深入dubbo（四）- 集群架构设计这次跟踪源代码，看一看dubbo在进行远程调用的时候进行了那些方法 首先调用我们声明的接口方法 很明显进入了动态代理类中 注意返回值 上一步的return方法中我们需要进一步分析invoke（xxx）方法的过程 来到了第一个关键的类中 AbstractClusterInvoker 通过 下图中的两个方法我们找到了 下一个关键directory 来到了directory类中 list方法是返回一个所需要的invoke结果集合 中间使用directory 返回所有集合和使用router对结果集进行筛选","date":"2024-12-31","categories":["java"],"tags":["dubbo"]},{"title":"dubbo学习和使用(2)-使用初探和配置方法","url":"/2024/12/31/dubbo学习和使用-2--使用初探和配置方法/","content":"快速启动Dubbo 采用全 Spring 配置方式，透明化接入应用，对应用没有任何 API 侵入，只需用 Spring 加载 Dubbo 的配置即可，Dubbo 基于 Spring 的 Schema 扩展进行加载。 如果不想使用 Spring 配置，可以通过 API 的方式 进行调用。 服务提供者 定义服务接口 DemoService.java： 在服务提供方实现接口DemoServiceImpl.java： 用 Spring 配置声明暴露服务provider.xml： 加载 Spring 配置Provider.java： 服务消费者 通过 Spring 配置引用远程服务consumer.xml： 加载Spring配置，并调用远程服务Consumer.java： dubbo 的四中配置方式xml 配置方法配置实例 所有标签都支持自定义参数，用于不同扩展点实现的特殊配置，如： dubbo 所有的标签和作用 标签 用途 解释 dubbo:service/ 服务配置 用于暴露一个服务，定义服务的元信息，一个服务可以用多个协议暴露，一个服务也可以注册到多个注册中心 dubbo:reference/ 引用配置 用于创建一个远程服务代理，一个引用可以指向多个注册中心 dubbo:protocol/ 协议配置 用于配置提供服务的协议信息，协议由提供方指定，消费方被动接受 dubbo:application/ 应用配置 用于配置当前应用信息，不管该应用是提供者还是消费者 dubbo:module/ 模块配置 用于配置当前模块信息，可选 dubbo:registry/ 注册中心配置 用于配置连接注册中心相关信息 dubbo:monitor/ 监控中心配置 用于配置连接监控中心相关信息，可选 dubbo:provider/ 提供方配置 当 ProtocolConfig 和 ServiceConfig 某属性没有配置时，采用此缺省值，可选 dubbo:consumer/ 消费方配置 当 ReferenceConfig 某属性没有配置时，采用此缺省值，可选 dubbo:method/ 方法配置 用于 ServiceConfig 和 ReferenceConfig 指定方法级的配置信息 dubbo:argument/ 参数配置 用于指定方法参数配置 配置覆盖关系 以 timeout 为例，显示了配置的查找顺序，其它 retries, loadbalance, actives 等类似： 方法级优先，接口级次之，全局配置再次之。 如果级别一样，则消费方优先，提供方次之。 其中，服务提供方配置，通过 URL 经由注册中心传递给消费方。 从上到下优先级递减 属性配置如果公共配置很简单，没有多注册中心，多协议等情况，或者想多个 Spring 容器想共享配置，可以使用 dubbo.properties 作为缺省配置。 映射规则将 XML 配置的标签名，加属性名，用点分隔，多个属性拆成多行 比如：dubbo.application.name&#x3D;foo等价于<dubbo:application name&#x3D;”foo” &#x2F;> 比如：dubbo.registry.address&#x3D;10.20.153.10:9090等价于<dubbo:registry address&#x3D;”10.20.153.10:9090” &#x2F;>如果 XML 有多行同名标签配置，可用 id 号区分，如果没有 id 号将对所有同名标签生效 比如：dubbo.protocol.rmi.port&#x3D;1234等价于<dubbo:protocol id&#x3D;”rmi” name&#x3D;”rmi” port&#x3D;”1099” &#x2F;> [2] 比如：dubbo.registry.china.address&#x3D;10.20.153.10:9090等价于<dubbo:registry id&#x3D;”china” address&#x3D;”10.20.153.10:9090” &#x2F;>下面是 dubbo.properties 的一个典型配置： 覆盖策略（从上到下优先级递减） 如果 classpath 根目录下存在多个 dubbo.properties，比如多个 jar 包中有 dubbo.properties，Dubbo 会任意加载，并打印 Error 日志，后续可能改为抛异常。 协议的 id 没配时，缺省使用协议名作为 id api 配置API 属性xml与配置项一对一 服务提供者 服务消费者 特殊场景下面只列出不同的地方，其它参见上面的写法 方法级设置 点对点直连 API使用范围说明：API 仅用于 OpenAPI, ESB, Test, Mock 等系统集成，普通服务提供方或消费方，请采用XML 配置方式使用 Dubbo 注解配置 服务提供方 Service注解暴露服务 javaconfig形式配置公共模块 指定dubbo扫描路径 服务消费方 Reference注解引用服务 javaconfig形式配置公共模块 指定dubbo扫描路径 注意 如果你曾使用旧版annotation配置，请删除所有相关配置，我们将在下个版本删除所有旧版配置项。","date":"2024-12-31","categories":["java"],"tags":["dubbo"]},{"title":"dubbo学习和使用(3)-dubbo功能(1)","url":"/2024/12/31/dubbo学习和使用-3--dubbo功能-1-/","content":"启动时检查Dubbo 缺省会在启动时检查依赖的服务是否可用，不可用时会抛出异常，阻止 Spring 初始化完成，以便上线时，能及早发现问题，默认 check&#x3D;”true”。 注意使用懒加载的时候可以指定这个方法为false，当这个为false的时候，如果服务不可用将会为null 而不是抛出异常，当服务恢复时，能自动连上。 配置方法通过 spring 配置文件关闭某个服务的启动时检查 (没有提供者时报错)： 关闭所有服务的启动时检查 (没有提供者时报错)： 关闭注册中心启动时检查 (注册订阅失败时报错)： 通过 dubbo.properties 配置的含义 dubbo.reference.check&#x3D;false，强制改变所有 reference 的 check 值，就算配置中有声明，也会被覆盖。 dubbo.consumer.check&#x3D;false，是设置 check 的缺省值，如果配置中有显式的声明，如：<dubbo:reference check&#x3D;”true”&#x2F;>，不会受影响。 dubbo.registry.check&#x3D;false，前面两个都是指订阅成功，但提供者列表是否为空是否报错，如果注册订阅失败时，也允许启动，需使用此选项，将在后台定时重试。 集群容错 各节点关系： 这里的 Invoker 是 Provider 的一个可调用 Service 的抽象，Invoker 封装了 Provider 地址及 Service 接口信息 Directory 代表多个 Invoker，可以把它看成 List ，但与 List 不同的是，它的值可能是动态变化的，比如注册中心推送变更 Cluster 将 Directory 中的多个 Invoker 伪装成一个 Invoker，对上层透明，伪装过程包含了容错逻辑，调用失败后，重试另一个 Router 负责从多个 Invoker 中按路由规则选出子集，比如读写分离，应用隔离等 LoadBalance 负责从多个 Invoker 中选出具体的一个用于本次调用，选的过程包含了负载均衡算法，调用失败后，需要重选 集群容错模式 Failover Cluster 失败自动切换，当出现失败，重试其它服务器 [1]。通常用于读操作，但重试会带来更长延迟。可通过 retries&#x3D;”2” 来设置重试次数(不含第一次)。 重试次数配置如下： Failfast Cluster快速失败，只发起一次调用，失败立即报错。通常用于非幂等性的写操作，比如新增记录。 Failsafe Cluster失败安全，出现异常时，直接忽略。通常用于写入审计日志等操作。 Failback Cluster失败自动恢复，后台记录失败请求，定时重发。通常用于消息通知操作。 Forking Cluster并行调用多个服务器，只要一个成功即返回。通常用于实时性要求较高的读操作，但需要浪费更多服务资源。可通过 forks&#x3D;”2” 来设置最大并行数。 Broadcast Cluster广播调用所有提供者，逐个调用，任意一台报错则报错 [2]。通常用于通知所有提供者更新缓存或日志等本地资源信息。 集群模式配置按照以下示例在服务提供方和消费方配置集群模式 负载均衡在集群负载均衡时，Dubbo 提供了多种均衡策略，缺省为 random 随机调用。 负载均衡策略 Random LoadBalance 随机，按权重设置随机概率。在一个截面上碰撞的概率高，但调用量越大分布越均匀，而且按概率使用权重后也比较均匀，有利于动态调整提供者权重。 RoundRobin LoadBalance 轮循，按公约后的权重设置轮循比率。存在慢的提供者累积请求的问题，比如：第二台机器很慢，但没挂，当请求调到第二台时就卡在那，久而久之，所有请求都卡在调到第二台上。 LeastActive LoadBalance 最少活跃调用数，相同活跃数的随机，活跃数指调用前后计数差。使慢的提供者收到更少请求，因为越慢的提供者的调用前后计数差会越大。 ConsistentHash LoadBalance一致性 Hash，相同参数的请求总是发到同一提供者。当某一台提供者挂时，原本发往该提供者的请求，基于虚拟节点，平摊到其它提供者，不会引起剧烈变动。算法参见：http://en.wikipedia.org/wiki/Consistent_hashing缺省只对第一个参数 Hash，如果要修改，请配置 缺省用 160 份虚拟节点，如果要修改，请配置 配置服务端服务级别 客户端服务级别 服务端方法级别 客户端方法级别 线程模型如果事件处理的逻辑能迅速完成，并且不会发起新的 IO 请求，比如只是在内存中记个标识，则直接在 IO 线程上处理更快，因为减少了线程池调度。 但如果事件处理逻辑较慢，或者需要发起新的 IO 请求，比如需要查询数据库，则必须派发到线程池，否则 IO 线程阻塞，将导致不能接收其它请求。 如果用 IO 线程处理事件，又在事件处理过程中发起新的 IO 请求，比如在连接事件中发起登录请求，会报“可能引发死锁”异常，但不会真死锁。 因此，需要通过不同的派发策略和不同的线程池配置的组合来应对不同的场景: Dispatcher all 所有消息都派发到线程池，包括请求，响应，连接事件，断开事件，心跳等。 direct 所有消息都不派发到线程池，全部在 IO 线程上直接执行。 message 只有请求响应消息派发到线程池，其它连接断开事件，心跳等消息，直接在 IO 线程上执行。 execution 只请求消息派发到线程池，不含响应，响应和其它连接断开事件，心跳等消息，直接在 IO 线程上执行。 connection 在 IO 线程上，将连接断开事件放入队列，有序逐个执行，其它消息派发到线程池。 ThreadPool fixed 固定大小线程池，启动时建立线程，不关闭，一直持有。(缺省) cached 缓存线程池，空闲一分钟自动删除，需要时重建。 limited 可伸缩线程池，但池中的线程数只会增长不会收缩。只增长不收缩的目的是为了避免收缩时突然来了大流量引起的性能问题。 eager 优先创建Worker线程池。在任务数量大于corePoolSize但是小于maximumPoolSize时，优先创建Worker来处理任务。当任务数量大于maximumPoolSize时，将任务放入阻塞队列中。阻塞队列充满时抛出RejectedExecutionException。(相比于cached:cached在任务数量超过maximumPoolSize时直接抛出异常而不是将任务放入阻塞队列) 直连提供者（只有测试环境使用）和只订阅，只注册直连在开发及测试环境下，经常需要绕过注册中心，只测试指定服务提供者，这时候可能需要点对点直连，点对点直联方式，将以服务接口为单位，忽略注册中心的提供者列表，A 接口配置点对点，不影响 B 接口从注册中心获取列表。 通过 XML 配置 如果是线上需求需要点对点，可在 dubbo:reference 中配置 url 指向提供者，将绕过注册中心，多个地址用分号隔开，配置如下 [1]： 通过文件映射 如果服务比较多，也可以用文件映射，用 -Ddubbo.resolve.file 指定映射文件路径，此配置优先级高于 dubbo:reference 中的配置 [3]，如： 然后在映射文件 xxx.properties 中加入配置，其中 key 为服务名，value 为服务提供者 URL： 注意:为了避免复杂化线上环境，不要在线上使用这个功能，只应在测试阶段使用。 禁用注册就是指服务方可以拉取配置中心的信息当时并不在配置中心进行注册 禁用注册配置 或者 禁用拉取制定一个配置中心只将自己注册进去但是不进行订阅功能 禁用订阅配置 <dubbo:registry id&#x3D;”hzRegistry” address&#x3D;”10.20.153.10:9090” &#x2F;><dubbo:registry id&#x3D;”qdRegistry” address&#x3D;”10.20.141.150:9090” subscribe&#x3D;”false” &#x2F;>或者 <dubbo:registry id&#x3D;”hzRegistry” address&#x3D;”10.20.153.10:9090” &#x2F;><dubbo:registry id&#x3D;”qdRegistry” address&#x3D;”10.20.141.150:9090?subscribe&#x3D;false” &#x2F;> 多协议 不同服务不同协议 不同服务在性能上适用不同协议进行传输，比如大数据用短连接协议，小数据大并发用长连接协议 多协议暴露服务 需要与 http 客户端互操作 服务分组当一个接口有多种实现时，可以用 group 区分。 服务 引用 多版本当一个接口实现，出现不兼容升级时，可以用版本号过渡，版本号不同的服务相互间不引用。 可以按照以下的步骤进行版本迁移： 在低压力时间段，先升级一半提供者为新版本 再将所有消费者升级为新版本 然后将剩下的一半提供者升级为新版本 老版本服务提供者配置： 新版本服务提供者配置： 老版本服务消费者配置： 新版本服务消费者配置： 如果不需要区分版本，可以按照以下的方式配置 [1]： 分组聚合按组合并返回结果 [1]，比如菜单服务，接口一样，但有多种实现，用group区分，现在消费方需从每种group中调用一次返回结果，合并结果返回，这样就可以实现聚合菜单项。 配置 搜索所有分组 合并指定分组 指定方法合并结果，其它未指定的方法，将只调用一个 Group 某个方法不合并结果，其它都合并结果 指定合并策略，缺省根据返回值类型自动匹配，如果同一类型有两个合并器时，需指定合并器的名称 指定合并方法，将调用返回结果的指定方法进行合并，合并方法的参数类型必须是返回结果类型本身 结果缓存结果缓存 [1]，用于加速热门数据的访问速度，Dubbo 提供声明式缓存，以减少用户加缓存的工作量 [2]。 缓存类型 lru 基于最近最少使用原则删除多余缓存，保持最热的数据被缓存。 threadlocal 当前线程缓存，比如一个页面渲染，用到很多 portal，每个 portal 都要去查用户信息，通过线程缓存，可以减少这种多余访问。 jcache 与 JSR107 集成，可以桥接各种缓存实现。 缓存类型可扩展，参见：缓存扩展 配置 或： 使用泛化调用泛化接口调用方式主要用于客户端没有 API 接口及模型类元的情况，参数及返回值中的所有 POJO 均用 Map 表示，通常用于框架集成，比如：实现一个通用的服务测试框架，可通过 GenericService 调用所有服务实现。 通过 Spring 使用泛化调用在 Spring 配置申明 generic&#x3D;”true”： 在 Java 代码获取 barService 并开始泛化调用： 通过 API 方式使用泛化调用 有关泛化类型的进一步解释 假设存在 POJO 如： 则 POJO 数据： 可用下面 Map 表示：","date":"2024-12-31","categories":["java"],"tags":["dubbo"]},{"title":"dubbo学习和使用(4)-dubbo功能(2)","url":"/2024/12/31/dubbo学习和使用-4--dubbo功能-2-/","content":"泛化调用客户端泛化接口调用方式主要用于客户端没有 API 接口及模型类元的情况，参数及返回值中的所有 POJO 均用 Map 表示，通常用于框架集成，比如：实现一个通用的服务测试框架，可通过 GenericService 调用所有服务实现。 通过 Spring 使用泛化调用在 Spring 配置申明 generic&#x3D;”true”： 在 Java 代码获取 barService 并开始泛化调用： 通过 API 方式使用泛化调用 有关泛化类型的进一步解释 假设存在 POJO 如： 则 POJO 数据： 可用下面 Map 表示： 泛化调用服务端泛接口实现方式主要用于服务器端没有API接口及模型类元的情况，参数及返回值中的所有POJO均用Map表示，通常用于框架集成，比如：实现一个通用的远程服务Mock框架，可通过实现GenericService接口处理所有服务请求。 在 Java 代码中实现 GenericService 接口： 通过 Spring 暴露泛化实现在 Spring 配置申明服务的实现： 通过 API 方式暴露泛化实现 回声测试回声测试用于检测服务是否可用，回声测试按照正常请求流程执行，能够测试整个调用是否通畅，可用于监控。 所有服务自动实现 EchoService 接口，只需将任意服务引用强制转型为 EchoService，即可使用。 Spring 配置： 代码： 上下文信息上下文中存放的是当前调用过程中所需的环境信息。所有配置信息都将转换为 URL 的参数，参见 schema 配置参考手册 中的对应URL参数一列。 RpcContext 是一个 ThreadLocal 的临时状态记录器，当接收到 RPC 请求，或发起 RPC 请求时，RpcContext 的状态都会变化。比如：A 调 B，B 再调 C，则 B 机器上，在 B 调 C 之前，RpcContext 记录的是 A 调 B 的信息，在 B 调 C 之后，RpcContext 记录的是 B 调 C 的信息。 服务消费方 服务提供方 隐式参数可以通过 RpcContext 上的 setAttachment 和 getAttachment 在服务消费方和提供方之间进行参数的隐式传递。 [1] 在服务消费方端设置隐式参数setAttachment 设置的 KV 对，在完成下面一次远程调用会被清空，即多次远程调用要多次设置。 在服务提供方端获取隐式参数public class XxxServiceImpl implements XxxService { } 异步调用基于 NIO 的非阻塞实现并行调用，客户端不需要启动多线程即可完成并行调用多个远程服务，相对多线程开销较小。 [1] 在 consumer.xml 中配置： 调用代码: 你也可以设置是否等待消息发出： [2] sent&#x3D;”true” 等待消息发出，消息发送失败将抛出异常。sent&#x3D;”false” 不等待消息发出，将消息放入 IO 队列，即刻返回。 如果你只是想异步，完全忽略返回值，可以配置 return&#x3D;”false”，以减少 Future 对象的创建和管理成本： 本地调用本地调用使用了 injvm 协议，是一个伪协议，它不开启端口，不发起远程调用，只在 JVM 内直接关联，但执行 Dubbo 的 Filter 链。 配置 定义 injvm 协议 设置默认协议 设置服务协议 优先使用 injvm 或 注意：服务暴露与服务引用都需要声明 injvm&#x3D;”true” 自动暴露、引用本地服务从 2.2.0 开始，每个服务默认都会在本地暴露。在引用服务的时候，默认优先引用本地服务。如果希望引用远程服务可以使用一下配置强制引用远程服务。","date":"2024-12-31","categories":["java"],"tags":["dubbo"]},{"title":"dubbo学习和使用(5)dubbo功能(3)","url":"/2024/12/31/dubbo学习和使用-5-dubbo功能-3-/","content":"参数回调 参数回调方式与调用本地 callback 或 listener 相同，只需要在 Spring 的配置文件中声明哪个参数是 callback 类型即可。Dubbo 将基于长连接生成反向代理，这样就可以从服务器端调用客户端逻辑。 服务接口示例 服务提供者配置示例 服务消费者配置示例 服务消费者调用示例 事件通知在调用之前、调用之后、出现异常时，会触发 oninvoke、onreturn、onthrow 三个事件，可以配置当事件发生时，通知哪个类的哪个方法。 服务提供者与消费者共享服务接口 服务提供者实现 服务提供者配置 服务消费者 Callback 接口 服务消费者 Callback 实现 服务消费者 Callback 配置 callback 与 async 功能正交分解，async&#x3D;true 表示结果是否马上返回，onreturn 表示是否需要回调。 两者叠加存在以下几种组合情况 [2]： 异步回调模式：async&#x3D;true onreturn&#x3D;”xxx” 同步回调模式：async&#x3D;false onreturn&#x3D;”xxx” 异步无回调 ：async&#x3D;true 同步无回调 ：async&#x3D;false 测试代码 本地存根 远程服务后，客户端通常只剩下接口，而实现全在服务器端，但提供方有些时候想在客户端也执行部分逻辑，比如：做 ThreadLocal 缓存，提前验证参数，调用失败后伪造容错数据等等，此时就需要在 API 中带上 Stub，客户端生成 Proxy 实例，会把 Proxy 通过构造函数传给 Stub，然后把 Stub 暴露给用户，Stub 可以决定要不要去调 Proxy。 在 spring 配置文件中按以下方式配置： 提供 Stub 的实现： Stub 必须有可传入 Proxy 的构造函数。 在 interface 旁边放一个 Stub 实现，它实现 BarService 接口，并有一个传入远程 BarService 实例的构造函数 本地伪装本地伪装通常用于服务降级，比如某验权服务，当服务提供方全部挂掉后，客户端不抛出异常，而是通过 Mock 数据返回授权失败。 在 spring 配置文件中按以下方式配置： 在工程中提供 Mock 实现： 如果服务的消费方经常需要 try-catch 捕获异常，如 请考虑改为 Mock 实现，并在 Mock 实现中 return null。如果只是想简单的忽略异常，在 2.0.11 以上版本可用 Mock 是 Stub 的一个子集，便于服务提供方在客户端执行容错逻辑，因经常需要在出现 RpcException (比如网络失败，超时等)时进行容错，而在出现业务异常(比如登录用户名密码错误)时不需要容错，如果用 Stub，可能就需要捕获并依赖 RpcException 类，而用 Mock 就可以不依赖 RpcException，因为它的约定就是只有出现 RpcException 时才执行。 在 interface 旁放一个 Mock 实现，它实现 BarService 接口，并有一个无参构造函数 延迟暴露如果你的服务需要预热时间，比如初始化缓存，等待相关资源就位等，可以使用 delay 进行延迟暴露。 延迟 5 秒暴露服务 延迟到 Spring 初始化完成后，再暴露服务 [1] 注意：Spring 2.x 初始化死锁问题 原因描述在 Spring 解析到 <dubbo:service &#x2F;> 时，就已经向外暴露了服务，而 Spring 还在接着初始化其它 Bean。如果这时有请求进来，并且服务的实现类里有调用 applicationContext.getBean() 的用法。 请求线程的 applicationContext.getBean() 调用，先同步 singletonObjects 判断 Bean 是否存在，不存在就同步 beanDefinitionMap 进行初始化，并再次同步 singletonObjects 写入 Bean 实例缓存。 而 Spring 初始化线程，因不需要判断 Bean 的存在，直接同步 beanDefinitionMap 进行初始化，并同步 singletonObjects 写入 Bean 实例缓存。 这样就导致 getBean 线程，先锁 singletonObjects，再锁 beanDefinitionMap，再次锁 singletonObjects。而 Spring 初始化线程，先锁 beanDefinitionMap，再锁 singletonObjects。反向锁导致线程死锁，不能提供服务，启动不了。 并发控制配置样例 样例 1 限制 com.foo.BarService 的每个方法，服务器端并发执行（或占用线程池线程数）不能超过 10 个： 样例 2 限制 com.foo.BarService 的 sayHello 方法，服务器端并发执行（或占用线程池线程数）不能超过 10 个： 样例 3 限制 com.foo.BarService 的每个方法，每客户端并发执行（或占用连接的请求数）不能超过 10 个： 样例 4 限制 com.foo.BarService 的 sayHello 方法，每客户端并发执行（或占用连接的请求数）不能超过 10 个： 注意：如果 <dubbo:service> 和 <dubbo:reference> 都配了actives，<dubbo:reference> 优先，参见：配置的覆盖策略。 Load Balance 均衡 配置服务的客户端的 loadbalance 属性为 leastactive，此 Loadbalance 会调用并发数最小的 Provider（Consumer端并发数）。 连接控制 服务端连接控制 限制服务器端接受的连接不能超过 10 个： 客户端连接控制 限制客户端服务使用连接不能超过 10 个： 如果 <dubbo:service> 和 <dubbo:reference> 都配了 connections，<dubbo:reference> 优先 延迟连接延迟连接用于减少长连接数。当有调用发起时，再创建长连接. 粘滞连接粘滞连接用于有状态服务，尽可能让客户端总是向同一提供者发起调用，除非该提供者挂了，再连另一台。 粘滞连接将自动开启延迟连接，以减少长连接数。 令牌验证通过令牌验证在注册中心控制权限，以决定要不要下发令牌给消费者，可以防止消费者绕过注册中心访问提供者，另外通过注册中心可灵活改变授权方式，而不需修改或升级提供者 可以全局设置开启令牌验证： 也可在服务级别设置： 还可在协议级别设置：","date":"2024-12-31","categories":["java"],"tags":["dubbo"]},{"title":"dubbo学习和使用(6)-dubbo功能(4)-路由","url":"/2024/12/31/dubbo学习和使用-6--dubbo功能-4--路由/","content":"路由规则 路由规则决定一次 dubbo 服务调用的目标服务器，分为条件路由规则和脚本路由规则，并且支持可扩展。 写入路由规则向注册中心写入路由规则的操作通常由监控中心或治理中心的页面完成 属性解释 condition:&#x2F;&#x2F; 表示路由规则的类型，支持条件路由规则和脚本路由规则，可扩展，必填。 0.0.0.0 表示对所有 IP 地址生效，如果只想对某个 IP 的生效，请填入具体 IP，必填。 com.foo.BarService 表示只对指定服务生效，必填。 group&#x3D;foo 对指定服务的指定group生效，不填表示对未配置group的指定服务生效 version&#x3D;1.0对指定服务的指定version生效，不填表示对未配置version的指定服务生效 category&#x3D;routers 表示该数据为动态配置类型，必填。 dynamic&#x3D;false 表示该数据为持久数据，当注册方退出时，数据依然保存在注册中心，必填。 enabled&#x3D;true 覆盖规则是否生效，可不填，缺省生效。 force&#x3D;false 当路由结果为空时，是否强制执行，如果不强制执行，路由结果为空的路由规则将自动失效，可不填，缺省为 false。 runtime&#x3D;false 是否在每次调用时执行路由规则，否则只在提供者地址列表变更时预先执行并缓存结果，调用时直接从缓存中获取路由结果。如果用了参数路由，必须设为 true，需要注意设置会影响调用的性能，可不填，缺省为 false。 priority&#x3D;1 路由规则的优先级，用于排序，优先级越大越靠前执行，可不填，缺省为 0。 rule&#x3D;URL.encode(“host &#x3D; 10.20.153.10 &#x3D;> host &#x3D; 10.20.153.11”) 表示路由规则的内容，必填。 条件路由规则基于条件表达式的路由规则，如：host &#x3D; 10.20.153.10 &#x3D;> host &#x3D; 10.20.153.11 规则解释： &#x3D;> 之前的为消费者匹配条件，所有参数和消费者的 URL 进行对比，当消费者满足匹配条件时，对该消费者执行后面的过滤规则。 &#x3D;> 之后为提供者地址列表的过滤条件，所有参数和提供者的 URL 进行对比，消费者最终只拿到过滤后的地址列表。 如果匹配条件为空，表示对所有消费方应用，如：&#x3D;> host !&#x3D; 10.20.153.11 如果过滤条件为空，表示禁止访问，如：host &#x3D; 10.20.153.10 &#x3D;> 表达式规则： 参数支持： 服务调用信息，如：method, argument 等，暂不支持参数路由 URL 本身的字段，如：protocol, host, port 等 以及 URL 上的所有参数，如：application, organization 等 条件支持： 等号 &#x3D; 表示”匹配”，如：host &#x3D; 10.20.153.10 不等号 !&#x3D; 表示”不匹配”，如：host !&#x3D; 10.20.153.10 值支持： 以逗号 , 分隔多个值，如：host !&#x3D; 10.20.153.10,10.20.153.11 以星号 * 结尾，表示通配，如：host !&#x3D; 10.20.* 以美元符 $ 开头，表示引用消费者参数，如：host &#x3D; $host 示例： 排除预发布机： &#x3D;> host !&#x3D; 172.22.3.91 白名单： host !&#x3D; 10.20.153.10,10.20.153.11 &#x3D;> 黑名单： host &#x3D; 10.20.153.10,10.20.153.11 &#x3D;> 服务寄宿在应用上，只暴露一部分的机器，防止整个集群挂掉： &#x3D;> host &#x3D; 172.22.3.1*,172.22.3.2* 为重要应用提供额外的机器： application !&#x3D; kylin &#x3D;> host !&#x3D; 172.22.3.95,172.22.3.96 读写分离： method &#x3D; find*,list*,get*,is* &#x3D;> host &#x3D; 172.22.3.94,172.22.3.95,172.22.3.96 method !&#x3D; find*,list*,get*,is* &#x3D;> host &#x3D; 172.22.3.97,172.22.3.98 前后台分离： application &#x3D; bops &#x3D;> host &#x3D; 172.22.3.91,172.22.3.92,172.22.3.93 application !&#x3D; bops &#x3D;> host &#x3D; 172.22.3.94,172.22.3.95,172.22.3.96 隔离不同机房网段： host !&#x3D; 172.22.3.* &#x3D;> host !&#x3D; 172.22.3.* 提供者与消费者部署在同集群内，本机只访问本机的服务： &#x3D;> host &#x3D; $host 脚本路由规则脚本路由规则支持 JDK 脚本引擎的所有脚本，比如：javascript, jruby, groovy 等，通过 type&#x3D;javascript 参数设置脚本类型，缺省为 javascript。 “script:&#x2F;&#x2F;0.0.0.0&#x2F;com.foo.BarService?category&#x3D;routers&dynamic&#x3D;false&rule&#x3D;” + URL.encode(“（function route(invokers) { … } (invokers)）”) 基于脚本引擎的路由规则，如： 路由规则扩展点：路由扩展 注意：一个服务只能有一条白名单规则，否则两条规则交叉，就都被筛选掉了 注意：脚本没有沙箱约束，可执行任意代码，存在后门风险","date":"2024-12-31","categories":["java"],"tags":["dubbo"]},{"title":"dubbo学习和使用(7)-dubbo功能(5)","url":"/2024/12/31/dubbo学习和使用-7--dubbo功能-5-/","content":"##配置规则 向注册中心写入动态配置覆盖规则。该功能通常由监控中心或治理中心的页面完成。 其中： override:&#x2F;&#x2F; 表示数据采用覆盖方式，支持 override 和 absent，可扩展，必填。 0.0.0.0 表示对所有 IP 地址生效，如果只想覆盖某个 IP 的数据，请填入具体 IP，必填。 com.foo.BarService 表示只对指定服务生效，必填。 category&#x3D;configurators 表示该数据为动态配置类型，必填。 dynamic&#x3D;false 表示该数据为持久数据，当注册方退出时，数据依然保存在注册中心，必填。 enabled&#x3D;true 覆盖规则是否生效，可不填，缺省生效。 application&#x3D;foo 表示只对指定应用生效，可不填，表示对所有应用生效。 timeout&#x3D;1000 表示将满足以上条件的 timeout 参数的值覆盖为 1000。如果想覆盖其它参数，直接加在 override 的 URL 参数上。 示例： 禁用提供者：(通常用于临时踢除某台提供者机器，相似的，禁止消费者访问请使用路由规则) 调整权重：(通常用于容量评估，缺省权重为 100) 调整负载均衡策略：(缺省负载均衡策略为 random) 服务降级：(通常用于临时屏蔽某个出错的非关键服务) 服务降级可以通过服务降级功能临时屏蔽某个出错的非关键服务，并定义降级后的返回策略。 向注册中心写入动态配置覆盖规则： 其中： mock&#x3D;force:return+null 表示消费方对该服务的方法调用都直接返回 null 值，不发起远程调用。用来屏蔽不重要服务不可用时对调用方的影响。 还可以改为 mock&#x3D;fail:return+null 表示消费方对该服务的方法调用在失败后，再返回 null 值，不抛异常。用来容忍不重要服务不稳定时对调用方的影响。 优雅停机Dubbo 是通过 JDK 的 ShutdownHook 来完成优雅停机的，所以如果用户使用 kill -9 PID 等强制关闭指令，是不会执行优雅停机的，只有通过 kill PID 时，才会执行。 原理 服务提供方 停止时，先标记为不接收新请求，新请求过来时直接报错，让客户端重试其它机器。然后，检测线程池中的线程是否正在运行，如果有，等待所有线程执行完成，除非超时，则强制关闭。 服务消费方 停止时，不再发起新的调用请求，所有新的调用在客户端即报错。然后，检测有没有请求的响应还没有返回，等待响应返回，除非超时，则强制关闭。 设置方式 设置优雅停机超时时间，缺省超时时间是 10 秒，如果超时则强制关闭。 dubbo.properties 如果 ShutdownHook 不能生效，可以自行调用，使用tomcat等容器部署的場景，建议通过扩展 ContextListener等自行调用以下代码实现优雅停机： 日志适配自 2.2.1 开始，dubbo 开始内置 log4j、slf4j、jcl、jdk 这些日志框架的适配，也可以通过以下方式显示配置日志输出策略： 命令行 在 dubbo.properties 中指定 在 dubbo.xml 中配置 开启访问日志如果你想记录每一次请求信息，可开启访问日志，类似于apache的访问日志。注意：此日志量比较大，请注意磁盘容量。 将访问日志输出到当前应用的log4j日志： 将访问日志输出到指定文件： 服务容器服务容器是一个 standalone 的启动程序，因为后台服务不需要 Tomcat 或 JBoss 等 Web 容器的功能，如果硬要用 Web 容器去加载服务提供方，增加复杂性，也浪费资源。 服务容器只是一个简单的 Main 方法，并加载一个简单的 Spring 容器，用于暴露服务。 服务容器的加载内容可以扩展，内置了 spring, jetty, log4j 等加载，可通过容器扩展点进行扩展。配置配在 java 命令的 -D 参数或者 dubbo.properties 中。 容器类型 Spring Container 自动加载 META-INF&#x2F;spring 目录下的所有 Spring 配置。 配置 spring 配置加载位置： dubbo.spring.config&#x3D;classpath*:META-INF&#x2F;spring&#x2F;*.xml Jetty Container 启动一个内嵌 Jetty，用于汇报状态。 配置： dubbo.jetty.port=8080：配置 jetty 启动端口 dubbo.jetty.directory=/foo/bar：配置可通过 jetty 直接访问的目录，用于存放静态文件 dubbo.jetty.page=log,status,system：配置显示的页面，缺省加载所有页面 Log4j Container 自动配置 log4j 的配置，在多进程启动时，自动给日志文件按进程分目录。 配置： dubbo.log4j.file=/foo/bar.log：配置日志文件路径 dubbo.log4j.level=WARN：配置日志级别 dubbo.log4j.subdirectory=20880：配置日志子目录，用于多进程启动，避免冲突 容器启动缺省只加载 spring ReferenceConfig 缓存ReferenceConfig 实例很重，封装了与注册中心的连接以及与提供者的连接，需要缓存。否则重复生成 ReferenceConfig 可能造成性能问题并且会有内存和连接泄漏。在 API 方式编程时，容易忽略此问题。 因此，自 2.4.0 版本开始， dubbo 提供了简单的工具类 ReferenceConfigCache用于缓存 ReferenceConfig 实例。 使用方式如下： 消除 Cache 中的 ReferenceConfig，将销毁 ReferenceConfig 并释放对应的资源。 缺省 ReferenceConfigCache 把相同服务 Group、接口、版本的 ReferenceConfig 认为是相同，缓存一份。即以服务 Group、接口、版本为缓存的 Key。 可以修改这个策略，在 ReferenceConfigCache.getCache 时，传一个 KeyGenerator。详见 ReferenceConfigCache 类的方法。 现场dump当业务线程池满时，我们需要知道线程都在等待哪些资源、条件，以找到系统的瓶颈点或异常点。dubbo通过Jstack自动导出线程堆栈来保留现场，方便排查问题 默认策略: 导出路径，user.home标识的用户主目录导出间隔，最短间隔允许每隔10分钟导出一次指定导出路径：","date":"2024-12-31","categories":["java"],"tags":["dubbo"]},{"title":"gradle-学习笔记(1)-初步使用","url":"/2024/12/31/gradle-学习笔记-1--初步使用/","content":"最近想深入的学习一下工程化方面相关的东西，在maven和gradle直接纠结不已，因为maven的扩展性太差劲了，学习成本颇高，所以最后投入了gradle的怀抱中，以后有时间再重新学习一下maven吧 最近的学习笔记是基于gradle 5 系列，其中各种教程和例子大都是来源于官方文档或者网络上的博客。内容涵盖我在学习gradle过程中的各种心得和gradle的一些使用方法 注意: 这里使用的配偶语言世kotlin 而不是使用groovy gradle创建项目一个命令 用户可以通过这个命令创建一个基本的gradle项目包括gradle项目的目录结构 (1) gradle 的构建脚本用来构建当前的gradle项目,最核心的配置文件 (2) (3) 一个gradle副本和配置文件，用来当如果系统中的gradle版本和项目使用的gradle版本不同，将会在这里下载一个项目中的版本 (4) (5) 配套使用的命令行工具 没有.bat后缀的是unix系统命令有的是windowns系统，可以用来执行gradle中定义的各种task 任务 (6) 用于配置Gradle构建的Gradle设置脚本 gradle的taskgradle 方便用户进行配置的特性是源于gradle提供了方便使用task参数这里编写一个很基本的copy文件的权限，在路径中添加一个src文件夹和dest文件夹，在src文件中添加一个文件markfile.txt 并且里面有一个内容hello world！ 然后在build.gradle 中编写一个任务 其中的type 字段将会调用系统中的Copy函数，而group和description 只是描述这个过程的描述符，只会影响系统的log输出，并不会影响实际的效果 运行对应的命令就能运行对应的copy方法 使用一个gradle内部的插件系统在项目中使用插件标签plugins添加指定的base插件到系统中 然后在指定的位置我们添加一个插件(和任务的使用方法相同，我怀疑其实gradle的插件就是打包好的任务) 然后运行对应的命令,就可以在对应的目录 build&#x2F;distributions 中找到 src目录下的压缩文件了 查看当前拥有的taskgradle 有一个内置的命令 这条命令将会将所有的当前的gradle项目中拥有的构建命令全部列出来 gradle 提供的在线查看构建详情的方法 –scan命令我们在使用构建命令的时候可以在命令的后面添加一个 –scan命令,通过这个命令可以链接到gradle官方的view显示仓库或者连接到自定义的链接仓库中,然后轻松的查看项目使用的构建任务,构建时间等等信息 然后可以登入命令行打出的一个网址,在其中的form表单中填写邮箱地址,然后就会将构建的信息发送到邮箱中了 注意:这个功能是收费的 gradle 展示当前系统的可用参数信息 properties命令","date":"2024-12-31","categories":["java"],"tags":["gradle"]},{"title":"gradle-学习笔记(2)-多项目构建","url":"/2024/12/31/gradle-学习笔记-2--多项目构建/","content":"记得在maven中支持多个子项目的构建方法,同样的在gradle 中也会支持多项目的构建方法 还记得在maven中如何配置多项目工程吗, 这里回忆一下 首先我们需要一个父元素pom文件比如这样 而在gradle中,我们并不需要指定父元素的标签,我们只需要编写好对应的文件夹名称，并且将文件夹名称和对应的目录结构对应清，gradle 就能自动的识别这个子项目 比如我创建这样一个子项目名称是greeting-library 子项目中的配置未见build.gradle 父项目中的setting.gradle中添加这样一条配置 这样就能使用greeting-library目录下的gradle子项目了 一个简单的项目这使用一个简单的项目介绍一下这个如何使用gradle 实现整合打包 项目结构和模块划分 这个项目中划分为根项目gradle-demo，包项目greeting-library，core可运行项目greeter。 注意： 通过上面的例子我们可以得出，在gradle 中不同的子项目的命名规则是使用文件夹的注意：在java 项目中，gradle 要求 必须指定项目的main函数具体方法见下方 这里针对gradle的多语言的编程的目录结构做一下补充，gradle中源代码同意放置在这样的位置中src->main&#x2F;test->编程语言名称文件夹 下 总结在构建gradle 多模块项目的时候,一定要注意多模块的之间的引用，模块中main函数的编写，父模块的include","date":"2024-12-31","categories":["java"],"tags":["gradle"]},{"title":"gradle-学习笔记(3)-构建jar包","url":"/2024/12/31/gradle-学习笔记-3--构建jar包/","content":"作为一个最基本的入门，这里只是打一个最简单的jar包 打jar包三个重要的gradle的参数 name，build.gradle的version和jar 通过jar 命令构建jar包中MANIFSET.MF文件中的参数信息jar 配置的方法 然后使用jar 命令的时候，将会在build文件夹中的libs子文件夹里生成对应的jar包，这里摘出他的MANIFSET.MF文件中的内容 version 制定生成的jar包版本号我们配置一个参数 然后当执行gradle jar 的task的时候将会在lib文件夹中生成后缀为1.0-SNAPSHOT的jar包","date":"2024-12-31","categories":["java"],"tags":["gradle"]},{"title":"gradle-学习笔记(4)-执行测试样例","url":"/2024/12/31/gradle-学习笔记-4--执行测试样例/","content":"gradle可以自动的执行测试样例，前提是要求测试的包名和要测试的类是一一对应的 当项目中编写了Test目录中的内容的时候，然后指定的测试样例写法正确，当运行test task的时候，gradle将会自动的执行对应的测试，并且在build的build 文件夹中的reports 以html的形式，输出对应的结果","date":"2024-12-31","categories":["java"],"tags":["gradle"]},{"title":"gradle-学习笔记(5)-生成可执行的appliaction","url":"/2024/12/31/gradle-学习笔记-5--生成可执行的appliaction/","content":"gradle 初步的学习已经进行到这里了,越来越觉得gradle设计的非常巧妙,并且idea对gradle的支持也是非常到位的 在我们深入理解gradle体系的时候,一定要贯彻一个gradle的设计哲学,一切皆task gradle生成可执行的application - gradle 的 application 插件gradle 想要生成一个可执行的任务是非常容易的,只要在build.gradle 文件中引入application插件,并且使用mainapplication 字段指明了main函数的对象,就可以使用 gradle run 命令来执行这个main函数 (或者使用gradlew 这两个命令本质上是等价的) 这个是在idea中查看的,可以看出已经出现了run的task 通过gradle的task –all命令同样可以看见 一个demo实例项目的目录结构如下 首先在main方法中实现了一个简单的函数返回一个字符串 并且编写了一个test样例 TestApp 整个项目的build.gradle如下 这里我们引入了application插件,通过这个插件,我们就能在项目中使用gradle 对应的命令来执行我们自己的main方法了 执行 gradle run 可以看见这里其实运行了对应的函数输出了hello world 引申 一下把,看一下这里的测试,当运行了gradle 的build命令的时候,同样会执行测试样例,并且在build的reports文件夹中生成对应的测试报告html","date":"2024-12-31","categories":["java"],"tags":["gradle"]},{"title":"gradle-学习笔记(6)-javaWeb项目开发","url":"/2024/12/31/gradle-学习笔记-6--javaWeb项目开发/","content":"gradle 通过自带的war插件可以编程生成web包项目,并且社区提供了gretty这个插件,通过这个插件,我们可以不必人工的采用手动打war包部署到web容器中来运行web项目,而是直接编译成class文件在项目中运行容器和web项目 gradle web 项目目录解析其实本质上gradle 的web 项目相比较war 项目的一个本质上的区别就是在main 目录下多了一个webapp 文件夹,这个是默认存放javaweb 相关信息的文件夹 一个简单的web项目 web项目的build.gradle 注意这里,gradle的war 包体添加了一个新的依赖选项,proovidedCompile和providedRuntime 分别和java 构建的compile和runtime相同,只不过打包的时候不会将这个应用打进对应的包中 这里引用了java的mockito自动测试框架,相比较junit更具有流程化的特点 servlet 测试servlet jsp和index.html 文件 项目的整体结构 项目的运行方法gradle的gretty插件提供了大量的task供我们选择,这里我们运行只是使用appRun Task就行了 注意:gretty 的仓库必须使用jcenter()而不能使用mavenCentral(),因为其中的一些依赖包在maven中不存在,我在使用的时候是两个都连 这里我们运行一下gradle appRun –console&#x3D;plain 引申:–console&#x3D;plain gradle5 将一些控制台输出合并了,这里使用这个参数可以将合并的信息展开显示 我们发现gretty启动了一个jetty服务监听8080打开对应的地址就能显示对应的信息了","date":"2024-12-31","categories":["java"],"tags":["gradle"]},{"title":"gradle-学习笔记(7)-编写自定义Tasks(custom tasks)","url":"/2024/12/31/gradle-学习笔记-7--编写自定义Tasks-custom-tasks-/","content":"好了终于到了重头戏了,gradel最为强大的地方就是可以非常方便的定义task并集成到开发环境中 简单的临时任务gradle 提供在build中直接编写临时task的方法,像下面这样 这段代码注册了一个hello task,分配到了task group组里 描述为a simple demo task,作用是打印一个hello,world 到控制台 使用gradle task 命令可以看到我们自定义的task group组下的 hello task 运行gradle hello 命令可以打印出 hello,world 使用gradle 编程实现任务其实本质上和简单任务相同,只过使用了gradle声明了一个对象罢了 具体的一看代码就明白了,说两点1.@TaskAction 表明这个是默认的任务方法,如果没有指明就默认使用这个方法 2. DefaultTask 是gradle的默认扩展,gradle还提供了其他的方法,这里不做展开了","date":"2024-12-31","categories":["java"],"tags":["gradle"]},{"title":"gradle-进阶(1)-命令行 操作完全总结","url":"/2024/12/31/gradle-进阶-1--命令行-操作完全总结/","content":"这里记录一下gradle的详细使用信息 gradle 命令的组成 gradle的命令总体上只有两个点 taskname 和 option 如果不带– 就是task 带的话就是option option可能带有参数,这里使用&#x3D;号来为他赋值 可以使用 –no- 来命名反转,表示不进行什么操作 option具有等价形式比如 gradle 执行task 在子项目环境下可以使用[:子项目名称]…[:子项目名称][:taskName] 来运行指定的task(只是在root项目中可以运行) 如果没有指定子项目名称将会从当前目录的项目中向下寻找task 多task运行 应用上面的规则,使用空格可开就可以指定多个子task顺序执行了 移除一个task 使用 –exclude-task 移除gradle运行过程中的一个task 重新运行 不会保留之前运行的结果而是重新执行 非中断执行 –continue 这个参数将会覆盖之前gradle运行是如果有一个task失败接下来的都不会执行,而变为跳过失败的task在其他非依赖本失败task的task执行完后打印错误 gradle中一些常用的task 名称 作用 build 组装所有输出并运行所有检查 run 执行某些脚本或二进制文件 check 使用该任务执行所有验证任务（包括测试和linting） clean 删除构建目录的内容 projects 列出所有的项目报告,子项目树 tasks 主要task列表,可以使用–all 打印出更多的信息 gradle help –task someTas 显示指定任务的详细信息 gradle myTask –scan 构建扫描提供了关于哪些配置，传递依赖关系和依赖关系版本选择存在哪些依赖关系的完整可视报告 gradle dependencies 提供所选项目的依赖关系列表，按配置细分。对于每个配置，该配置的直接和传递依赖关系都显示在树中 gradle buildEnvironment 通上面的依赖配置,这个展示的是父子项目或者其他形式的配置依赖 gradle properties 为您提供所选项目的属性列表 gradle model gradle中一些常用的option DEBUG相关的参数 名称 作用 -?, -h, –help 帮助文档 -v, –version 输出整个环境的版本信息groovy java gradle等等 -S, –full-stacktrace 打印出任何异常的完整（非常详细）堆栈跟踪。 另请参阅日志记录选项 -s, –stacktrace 打印堆栈跟踪也用于用户异常（例如编译错误）。另请参阅日志记录选项 –scan 创建一个构建扫描，其中包含有关Gradle构建的所有方面的细粒度信息 -Dorg.gradle.debug&#x3D;true Debug Gradle client (non-Daemon) process. Gradle will wait for you to attach a debugger at localhost:5005 by default. -Dorg.gradle.daemon.debug&#x3D;true Debug Gradle Daemon进程 性能选择参数(Performance options) 这里的很多性能优化参数其实可以在gradle.properties中指定的 名称 作用 properties文件中的参数 –build-cache, –no-build-cache 切换Gradle构建缓存.Gradle将尝试重用以前版本的输出.默认为关闭 –configure-on-demand, –no-configure-on-demand 切换按需配置。在此构建运行中仅配置相关项目。默认为关闭。 –max-workers 设置Gradle可以使用的最大工作数。默认值是处理器数量 –parallel, –no-parallel 并行构建项目。有关此选项的限制，请参阅并行项目执行。默认为关闭。 –profile 在$ buildDir &#x2F; reports &#x2F; profile目录中生成高级性能报告。 –scan是首选。 –scan 使用详细的性能诊断生成构建扫描。 Gradle守护程序选项 名称 作用 –daemon, –no-daemon 使用Gradle Daemon运行构建。如果未运行守护程序或现有守护程序繁忙，则启动守护程序。默认打开。 –foreground 在前台进程中启动Gradle守护程序 –status (Standalone command) 运行gradle –status列出正在运行且最近停止的Gradle守护进程。仅显示相同Gradle版本的守护程序。 –stop (Standalone command) 运行gradle –stop以停止相同版本的所有Gradle守护进程。 -Dorg.gradle.daemon.idletimeout&#x3D;(number of milliseconds) Gradle守护程序将在此毫秒的空闲时间后停止运行。默认值为10800000（3小时） gradle 日志参数-Dorg.gradle.logging.level&#x3D;(quiet,warn,lifecycle,info,debug)|通过Gradle属性设置日志记录级别。-q, –quiet|仅记录错误。-w, –warn|将日志级别设置为警告-i, –info|Set log level to info.-d, –debug|登录调试模式（包括正常的堆栈跟踪）。 gradle自定义日志样式您可以通过以下方式指定“控制台”模式来控制丰富输出（颜色和字体变体）的使用 -Dorg.gradle.console&#x3D;(auto,plain,rich,verbose),–console&#x3D;(auto,plain,rich,verbose)|通过Gradle属性指定控制台模式。下面描述了不同的模式 plain : 设置为plain以仅生成纯文本。此选项禁用控制台输出中的所有颜色和其他丰富输出。当Gradle未连接到终端时，这是默认设置 auto : 设置为auto（默认值）以在构建过程附加到控制台时在控制台输出中启用颜色和其他丰富输出，或仅在未连接到控制台时生成纯文本。 这是Gradle连接到终端时的默认设置。 rich : 设置为rich以在控制台输出中启用颜色和其他丰富输出，无论构建过程是否未附加到控制台。 如果未连接到控制台，则构建输出将使用ANSI控制字符来生成丰富的输出。 verbose : 设置为verbose以启用颜色和其他丰富输出，如富，但在生命周期日志级别输出任务名称和结果，如Gradle 3.5及更早版本中默认执行 gradle 警告提示级别默认情况下，Gradle不会显示所有警告（例如，弃用警告）。相反，Gradle将收集它们并在构建结束时呈现摘要 -Dorg.gradle.warning.mode&#x3D;(all,none,summary)通过Gradle属性指定警告模式。 下面描述了不同的模式。 –warning-mode&#x3D;(all,none,summary)指定如何记录警告。 默认为摘要。 设置为all以记录所有警告。 设置为summary以禁止所有警告并在构建结束时记录摘要。 设置为none以禁止所有警告，包括构建结束时的摘要。 执行时设置option ps 感觉没少啥用,具体可以看文档这里不记录了 gradle 配置参数(这些其实可以在配置文件中指定的对应的方法) 名称 作用 -b,-build-file 指定构建文件。例如：gradle –build-file &#x3D; foo.gradle。默认值为build.gradle，然后是build.gradle.kts，然后是myProjectName.gradle。 -c,-setting-fil 指定设置文件。例如：gradle –settings-file &#x3D; somewhere&#x2F;else&#x2F;settings.gradle -g,-gradle-user-home 指定Gradle用户主目录。默认值是用户主目录中的.gradle目录。 -p,-project-dir 指定Gradle的起始目录。默认为当前目录。 –project-缓存目录 指定项目特定的缓存目录。根项目目录中的默认值为.gradle。 -u,-no-search-upward（不建议使用） 不要在父目录中搜索settings.gradle文件。 -D,-system-prop 设置JVM的系统属性，例如-Dmyprop &#x3D; myvalue。请参阅系统属性。 -I,-init-script 指定初始化脚本。请参阅Init Scripts。 -P,-project-prop 设置根项目的项目属性，例如-Pmyprop &#x3D; myvalue。请参阅系统属性。 -Dorg.gradle.jvmargs 设置JVM参数。 gradle 初始化构建主要就是一个命令 这个命令可以附带一些参数,比如 –type java-library : 指定构建的项目类型–gradle-version : 指定gradle的版本 等等 gradle 的持续构建目前是一项测试功能 应用参数是–continuousjava9 的更新太大了,现在很多框架都是应用的java8构建的,java9 扯到蛋了…….java11 真的很强大,学了gradle准备开始造轮子了,那些框架没有更新是因为用的人少,社区也没有去推动………………..","date":"2024-12-31","categories":["java"],"tags":["gradle"]},{"title":"gradle-进阶(2)-gradlePlugins开发","url":"/2024/12/31/gradle-进阶-2--gradlePlugins开发/","content":"gradle插件,我的理解其实本质上就是一堆task的集合","date":"2024-12-31","categories":["java"],"tags":["gradle"]},{"title":"gradle-进阶(2)-gradle环境信息,属性配置详解","url":"/2024/12/31/gradle-进阶-2--gradle环境信息-属性配置详解/","content":"这一篇文章是之前的一篇文章gradle命令行相关的记录的文章的延续,主要记录一下gradle的各种环境或者说属性的配置方法 gradle 各种属性变量的优先级gradle有五种属性构建优先级,这里将优先级从高到底排序 命令行标志如–build-cache.它们优先于属性和环境变量. 系统属性，例如systemProp.http.proxyHost&#x3D;somehost.org存储在gradle.properties文件中.指的试一些有systemProp前缀的参数 Gradle属性(例如org.gradle.caching&#x3D;true通常存储在gradle.properties项目根目录或GRADLE_USER_HOME(linux 系统的$USER_HOME&#x2F;.gradle目录)环境变量中的文件中). 环境变量，例如GRADLE_OPTS由执行Gradle的环境提供的,在环境变量中设置对应的参数,包括的环境变量有 GRADLE_OPTS GRADLE_USER_HOME JAVA_HOME (ps 这个一般不使用) 项目构建项目属性，如-PreleaseType&#x3D;final,就是在gradle.properties中除了gradle.properties之外的参数 推荐用法使用gradle.propertiesgradle.properties 承接了gradle 系统的很多属性,通过这种方法,我们可以统一gradle的环境信息配置 gradle.properties 中的参数具有一定的优先级,相同的参数如果配置在不同的位置上将会有覆盖效果,覆盖规则如下后者覆盖前者: gradle.properties 在项目根目录中. gradle.properties在GRADLE_USER_HOME目录中. 系统属性 下面的属性将会应用在gradle中 名称 参数 org.gradle.caching&#x3D;(true,false) 设置为true时，Gradle将尽可能重用任何先前构建的任务输出，从而使构建速度更快.了解有关使用构建缓存的更多信息. org.gradle.caching.debug&#x3D;(true,false) 设置为true时，将在控制台上记录各个输入属性哈希值以及每个任务的构建缓存键.了解有关任务输出缓存的更多信息. org.gradle.configureondemand&#x3D;(true,false) 允许按需孵化配置，Gradle将尝试仅配置必要的项目. org.gradle.console&#x3D;(auto,plain,rich,verbose) 自定义控制台输出着色或详细程度.默认值取决于Gradle的调用方式.有关其他详细信息，请参阅命令行日志记 org.gradle.daemon&#x3D;(true,false) 当设置true的摇篮守护进程来运行构建.默认是true. org.gradle.daemon.idletimeout&#x3D;(# of idle millis) Gradle守护程序将在指定的空闲毫秒数后自行终止.默认为10800000(3小时) org.gradle.debug&#x3D;(true,false) 设置true为时，Gradle将在启用远程调试的情况下运行构建，侦听端口5005.请注意，这相当于添加-agentlib:jdwp&#x3D;transport&#x3D;dt_socket,server&#x3D;y,suspend&#x3D;y,address&#x3D;5005到JVM命令行，并将挂起虚拟机，直到连接调试器.默认是false. org.gradle.java.home&#x3D;(path to JDK home) 指定Gradle构建过程的Java主目录.可以将值设置为a jdk或jrelocation，但是，根据构建的功能，使用JDK会更安全.如果未指定设置，则使用合理的默认值. org.gradle.jvmargs&#x3D;(JVM arguments) 指定用于Gradle守护程序的JVM参数.该设置对于为构建性能配置JVM内存设置特别有用. org.gradle.logging.level&#x3D;(quiet,warn,lifecycle,info,debug) 当设置为quiet，warn，lifecycle，info或debug时，Gradle将使用此日志级别.值不区分大小写.该lifecycle级别是缺省值.请参阅选择日志级别. org.gradle.parallel&#x3D;(true,false) 配置完成后，Gradle将分叉到org.gradle.workers.maxJVM以并行执行项目.要了解有关并行任务执行的更多信息，请参阅Gradle性能指南. org.gradle.warning.mode&#x3D;(all,none,summary) 设置为all，summary或者none，Gradle将使用不同的警告类型显示.有关详细信息，请参阅命令行日志记录选 org.gradle.workers.max&#x3D;(max # of worker processes) 配置后，Gradle将使用给定数量的工作者的最大值.默认值是CPU处理器数.另请参见性能命令行选项. gradle jvm 配置gradle 使用org.gradle.jvmargs 可以配置jvm的各种运行参数 也可以针对某一个task单独进行配置 gradle 编程设置参数信息这我们先是声明了一个自定义的task,然后在内部使用if else 和 project.hasProperty 方法判断gradle 的运行环境中时候有isCI这个参数 gradle通过http代理访问web配置HTTP或HTTPS代理（例如，用于下载依赖项）是通过标准JVM系统属性完成的。这些属性可以直接在构建脚本中设置; 例如，设置HTTP代理主机将完成System.setProperty(‘http.proxyHost’, ‘www.somehost.org')。或者，可以在gradle.properties中指定属性。 使用配置HTTP代理 gradle.properties HTTPS有单独的设置。 使用配置HTTPS代理 gradle.properties","date":"2024-12-31","categories":["java"],"tags":["gradle"]},{"title":"gradle-进阶(3)-性能优化专题","url":"/2024/12/31/gradle-进阶-3--性能优化专题/","content":"gradle内部进行了大量的优化以提高gradle的性能,这里整理一下gradle性能相关的配置方法 ps : 基于gradle5+ 系列 gradle的守护进程原理 : gradle使用守护进程的原理非常简单,第一条,减少了jvm每次的启动成本,第二条,在内存中进行优化,重用之前的构建结果 注意问题 : gradle守护程序是依赖进程重用的,在容器中需要禁用掉 相关命令 : –profile : 了解守护进程可能造成的影响,并分析构建 –status : 查看守护进程的列表和状态 –stop : 终止守护进程 –deamon,–no-deamon : 启用或者停用守护进程禁用守护进程方法: 在gradle运行环境中添加参数(见gradle环境信息,属性配置详解) 多个gradle守护进程产生的原因 : 本质上是因为gradle检测了环境(java版本,java运行参数等)不同所以生成了新的进程 可能存在的问题 : 构建脚本内存泄漏,或者全局状态损坏导致的不一致问题,(windows问题更加严重,因为它对读取或写入后无法关闭文件的程序的宽容度较低)","date":"2024-12-31","categories":["java"],"tags":["gradle"]},{"title":"gradle-进阶(5)-gradle创建和脚本","url":"/2024/12/31/gradle-进阶-5--gradle创建和脚本/","content":"通过之前的入门学习，一个gradle构建可以理解成有一个或者多个task组成，这里简单回顾一下gradle脚本如何构建一个简单的task 使用gradle脚本创建一个简单的task在build.gradle 文件夹中写入一个任务 这个task的作用很简单就是输出一个hello world 字符串 引申:如果使用过ant的话那么可以理解gradle task就是ant中的目标 gradle构建脚本高级玩法1. 可以使用局部变量其实就是在gradle的task脚本中可以声明变量供下面引用 gradle 依赖性脚本使用如果使用依赖字段dependsOn 关键子的时候,系统将会先运行此字段对应的task gralde使用编程方法批量添加task，和动态添加依赖 引申一下这个动态依赖添加 ： 在上面的例子中，如果运行了task0 将会先与性task2和task3 然后再运行task0，在更强大的方法见下面的例子 gradle 使用api访问任务 运行的结果如下 动态获取tash属性的方法这个方法的使用需要配合gradle api使用的 在这个方法中，使用$hello.name获取到了这个任务的名称,使用ext.xxx 定义了一个变量的名称，兵使用hello.xxx 调用了这个变量的名称 gradle build文件中使用方法 我们这里使用了groovy与语法声明了一个函数，并且在我们自己定义的任务中使用这些函数 指定默认的task默认的task 的使用方法就是调用 gradle 命令将会默认运行的方法，使用defaultTasks 定义 gradle控制执行阶段简单实例 输出信息 在上面的例子中我们使用了 taskGraph方法来在gradle的配置过程中（还有一个是运行过程）动态改变 变量（version）的方法 添加外部依赖的方法这个点应该大家都很熟悉了 这里groovy 提供了非常强大的功能，我们可以在代码中直接使用gradle 引用的代码（例子中的task定义） 我们这里里一定要注意import这个包，idea框架并不会自动提示 我们使用命令 gradle encode 将会有如下的输出","date":"2024-12-31","categories":["java"],"tags":["gradle"]},{"title":"gradle-进阶(6)-增强版task构建","url":"/2024/12/31/gradle-进阶-6--增强版task构建/","content":"增强版task构建之前的一篇文章整理gradle task的一个简单创建，这里整理一下task的高级应用方法 任务的定义方法如果是简单的任务我们可以这样进行定义 或者指定任务的类型（继承原有的任务） 这种方法，后面的type 字段其实是可以省略的 除此之外我们还能使用tasks组的方法创建对应的任务 定位到指定的任务 其实本质上，gradle将一个个任务抽象成一个一个的变量，这样我们就能在使用的时候直接使用变量名成引用就能实现 或者使用按路径和名称的方法，获取一个task 这里要注意两个地方 首先 project 中指定的路径必须是一个已经定义的gradle子路径，如果没有的话，gradle将会报错。其次是输出的方法就是按照定义的方法进行执行 使用组的方法获取一个变量 gradle 任务的变量化操作之前说过在gradle 中 所有的任务都可以抽象成变量，这里写一个例子来表明这个观点 我们声明了一个类型是Copy，名称时myCopy的task，接下来使用方法动态的获取这个task并对他进行一定的修改操作 定义一个自定义任务之前整理过，在gradle中我们其实可以生命一个task类型，方法如下 通过继承DefaultTask类，我们可以动态的声明一个task 在使用的时候可以使用如下两种发法定义任务 我们指定了名称和类型，最后的两个参数将会传入构造函数中，但是在gradle中，如果要使用构造函数传递参数，那么必须使用@Inject注解 这个方法就很明确了,直接使用type和constructorArgs字段定义要使用的参数类型是啥 gradle task的依赖之前我们整理了task依赖，使用dependsOn字段，指明依赖见下面的例子 这里扩展一个gradle 依赖处理的高级用法，动态添加依赖 这个方法通过groovy的扩展方法，指定了所有名称有lib前缀的task都是他的依赖 gradle 任务运行时排序两个关键字 shouldAfterRun mustAfterRun 一个例子： shouldRunAfter 指定的是这些任务应该按照指定的顺序执行mustRunAfter 表示必须按照指定的顺序执行 上面的例子中会按照 taskz -> taskY->taskx 的顺序执行，如果使用了mustRunAfter将会报错 gradle 最终任务使用finalizedBy字段描述执行器的最终任务，这个任务将不论定义放是否执行成功，最终都将会执行 gradle task的描述，替换，和跳过执行，启用，禁用，超时时间操作gradle 针对任务提供了大量的操作性功能 使用description 同名覆盖操作 如果自己命名的task与系统中或者自己定义的其他task重名，那么可以使用这种方法进行覆盖 跳过执行操作 在这里gradle 提供了两个方法跳过执行操作，第一种，使用哦onlyIf 关键字 指定只在什么情况下才会执行对应的任务 第二种，在程序执行的过程中抛出 stopExecutionException异常，终止这个调用链操作 启用禁用设置超时时间这些gradle操作，我用一个例子来解释吧 启用禁用操作 超时时间 gradle 任务组级别全局规则注入 gradle的project 和 其他信息生命周期任务","date":"2024-12-31","categories":["java"],"tags":["gradle"]},{"title":"gradle-进阶(7)-gradle文件操作","url":"/2024/12/31/gradle-进阶-7--gradle文件操作/","content":"其实对于任何一种构建工具来说，针对文件操作的功能是必不可少的，这里整理一下gradle针对文件提供的功能 gradle 中的文件路径 gradle 的 Project.file方法 作用： 指定单个文件或者目录的位置，解析规则（相对路径相对于项目目录进行解析，而绝对路径保持不变）。 例子： 注意：gradle不可使用new File(relative path) 这个方法，），因为new File（relative）会创建相对与运行时的（当时的）路径，gradle不能保证运行时路径相同（内部的守护任务可能会切换路径） Project.getRootDir 作用： 获取项目的根路径 例子： gradle 文件操作文件集合gradle这么定义文件集合的-就是一组文件，不考虑目录什么 获取文件集合 gradle推荐使用 ProjectLayout.files（java.lang.Object …）方法获取文件集合，本方法返回一个FileCollection实例，此方法非常灵活，允许您传递多个字符串，File实例，字符串集合，Files 集合等 这里写一个例子在指定的目录下寻找文件 文件集合的一些操作 改变集合类型 集合间进行集合操作 groovy 使用+ 表示union操作 使用 - 表示 defference操作 注意如果使用这些集合方法无法满足要求，需要使用filter方法来对集合进行过滤操作 文件树和文件的区别，其实本质上文件树是文件集合的一种延伸，不过文件树保留了树状结构 创建文件树 简单方法 从zip或者tar中创建文件树 引申：文件集合（包括文件树，作为参数进行传递的时候的处理方法） 在gradle 中，很多时候source——资源被解释称一个文件目录或者一个文件集合，如果传入的参数是一个文件集合如JavaCompile任务有一个source属性 在gradle中souce属性可以有很多不同的表现形式，比如下面的例子 使用文件树 文件复制gradle 提供copy task 实现文件复制的相关功能 copy需要的参数信息 from into方法表示文件的复制来源和输出来源，注意这些属性的参数都可以是集合或者文件目录 include exclude 选择性过滤方法 重命名文件 rename方法 gradle 提供了两种方法，进行重命名文件 使用过滤器 使用正则表达式 copy文件使用模板方法 Gradle提供的一个解决方案是Project.copySpec（org.gradle.api.Action）方法。这允许您在任务之外创建复制规范，然后可以使用CopySpec.with（org.gradle.api.file.CopySpec …）方法将其附加到适当的任务。 任务中添加复制功能 sync方法将复制目录和被copy目录同步 简单点说就是如果复制的时候少了几个文件，那么复制结束后，被复制的文集夹将会删除这些文件，其他的用法和gradle相同 其实这个方法是继承自copy中的，和copy的使用方法相同 常见用法例子 单文件复制 多文件复制 注意一点，使用**&#x2F;*.java的时候会复制目录结构，包括不包含指定文件的目录结构 include 指令，如果写在外面就是针对整个copy过程都适用，如果是在from中编写，就只针对这个from语句适用 一个复杂的复制构建例子 这个例子是将 src&#x2F;dist中的所有html png jpg文件 添加到 $buildDir&#x2F;explodeWar文件夹中，而output和 classpath中的所有文件将会考到指定的文件夹中 zip 压缩文件 注意一点，使用**&#x2F;*.java的时候会复制目录结构，包括不包含指定文件的目录结构include 指令，不同于copy，只能在from中编写，针对这个from语句适用 解压缩 普通副本一样，您可以控制哪些文件通过过滤器解压缩，甚至在解压缩时重命名文件。注意zipTree 方法同样适用 Jar 和 war ear 能力提升，将环境中所有的jar class 文件都统一到编译之后的jar包中方法很简单（jar任务使用zipTree打包） ps暂时没有成功 文件移动 Gradle没有用于移动文件和目录的API，但您可以使用Apache Ant集成轻松地执行此操作，如以下示例所示： 重新命名方法 文件删除","date":"2024-12-31","categories":["java"],"tags":["gradle"]},{"title":"groovy-区别特性梳理(1)-基本信息","url":"/2024/12/31/groovy-区别特性梳理-1--基本信息/","content":"groovy相比较java的特殊的字符串操作 $ 引用当前作用域中的变量名称,这种方法叫插值 groovy中可以使用${xxx sxx}或者 $xxx 来直接在字符串中拼接信息 groovy 插值方法还有两种特殊用法 表达式使用方法 注意: 如果<<后面是字符串的时候,将表示插入方法,如果是字符串就是位移逻辑 ‘x’ ,”xx”, ‘’’xxxx’’’,&#x2F;xxx&#x2F;,四种字符串的区别 在很多编程语言中都有这个特性 ‘’ : 简单的一个字符串,在groovy中这个字符串不支持插值($)操作 “” : ‘’ 特性加上支持插值-‘’’ ‘’’ : 保留格式,有些地方需要使用\\ 来消除 ‘\\n’ 换行问题-&#x2F;xx&#x2F; : 特殊字符不需要转义(只有&#x2F;需要使用\\进行转义),正则表达式专用 使用~ 或者&#x3D;~快速创建正则表达匹配式 通过~ 这个方法可以快速的创建java中的java.utilregex.pattern对象,注意这种方法&#x3D;和~ 之间一定要有空格,否则会解释成~&#x3D;操作运算符 引申: 匹配运算符（&#x3D;&#x3D;~）是find运算符的一个小变体，它不返回Matcher一个布尔值，并且需要输入字符串的严格匹配 groovy中相比较java特殊的变量名称操作 groovy中可以使用字符串变量来作为变量名称 groovy相比较java特殊的对象操作 空指针的安全保证 groovy中如果对象为空,可以使用?.的方法来消除异常 对象的getter和setter的不同之处 在groovy中如果设置了getter使用对象.xx的时候将会自动的调用getter方法,如果没有设置,将会直接使用这个变量groovy还是支持使用对象.@xx方法来直接的调用对应的变量 注意其实本质上是groovy默认就为每一个参数声明了getter和setter方法罢了,我们可以不使用getxx而是直接使用对象.xxx来调用getter或者setter方法 方法指针 这个算是一个高级特性了,通过这种方法,可以快速的拉取一个对象的方法引用,类似js,function也是变量 注意:groovy是引用了对应对象的方法 对象属性聚合操作 *. 在groovy中可以使用*.操作将一个对象数组中的对象的某个属性聚合起来,表达式cars*.make相当于cars.collect{ it.make },区别是groovy的*,表达式可以使用null 对象中的call用法 在groovy中,对象也可以抽象成一个函数,使用对象名()方法的时候,将会隐式的调用对象中的call方法 groovy 相对比java 特殊的函数用法 函数参数数组化映射 函数参数map化映射 在groovy中使用map可以动态的指定相关的参数 注意groovy支持map和其他类型参数混用的逻辑,只有一点需要注意,定义的函数,他的传参的第一个一定是map类型,否则必须显示的指定参数 def 返回值参数 groovy是弱类型编程语言,所以返回值支持def弱类型的返回值 groovy 相对比java的特殊数组用法 groovy的快速添加数据 groovy提供了引用的方法,赖在第二个数组中应用另一个数组中的内容 快速创建数组信息 通过这种方法可以快速的创建一个指定迭代规律的字符串 数组批量赋值 groovy和java 在比较方法中的区别 &#x3D;&#x3D; 和 equal的区别 在groovy中&#x3D;&#x3D; 将会调用java中的equal 方法,如果要比较值是否相等,应该使用is方法 compareTo快速创建方法 在groovy中可以使用<&#x3D;>来替代compareTo groovy 与java不用的运算符用法 运算符重载 在groovy中只要事先指定函数的对象就能将函数对应的运算法运算方法集成到这个对象中 运算符号对应的方法见下表 Operator Method Operator Method + a.plus(b) a[b] a.getAt(b) - a.minus(b) a[b] &#x3D; c a.putAt(b, c) * a.multiply(b) a in b b.isCase(a) &#x2F; a.div(b) << a.leftShift(b) % a.mod(b) >> a.rightShift(b) ** a.power(b) >>> a.rightShiftUnsigned(b) | a.or(b) ++ a.next() & a.and(b) – a.previous() ^ a.xor(b) +a a.positive() as a.asType(b) -a a.negative() a() a.call() ~a a.bitwiseNegate()","date":"2024-12-31","categories":["java"],"tags":["groovy"]},{"title":"groovy-区别特性梳理(2)-包管理引用","url":"/2024/12/31/groovy-区别特性梳理-2--包管理引用/","content":"groovy 和java 在包管理特性上的不同其实本质上groovy和java 在包管理上其实是类似的,但是groovy提供了更加强大的静态功能 许您定义与导入方法同名的方法，只要您有不同的类型 导入别名 静态函数名称导入别名 对象别名 groovy 和 java 运行上的不同groovy继承了java类的定义,如果使用了class来表示groovy对象,本质上和java并没有区别 但是如果使用groovy脚本模式变成并运行,groovy内部会将脚本进行编译 比如这样一个groovy脚本 groovy将会将这个脚本编译成","date":"2024-12-31","categories":["java"],"tags":["groovy"]},{"title":"groovy-区别特性梳理(3)-面向对象的特殊点","url":"/2024/12/31/groovy-区别特性梳理-3--面向对象的特殊点/","content":"groovy 变量可见性和javabean相比较java的不同点groovy如果没有定义可见性(public private),默认使用public groovy 不提供getter和setter 在groovy中如果设置了getter使用对象.xx的时候将会自动的调用getter方法,如果没有设置,将会直接使用这个变量groovy还是支持使用对象.@xx方法来直接的调用对应的变量 groovy集成登groovy的继承和java的非常类似,都拥有private protected public 关键字,都有interface abstract 关键字,并且语义相同 Groovy接口不支持Java 8接口等默认实现。但是groovy提供了一个特殊的东西叫做Traits,特征 groovy对象new groovy既然是脚本语言所以在构建一个新的对象的时候也相当灵活,groovy提供了如下的几种方法来创建对象 传统的java创建方法 使用as指定创建的对象类型 直接定义创建的类型,进行强制转换 如果没有提供构造参数,groovy还提供了参数映射来动态的初始化对应的字段信息 注意: 这种方案传递的参数是一个map类型,名称和对象中的变量一一对应","date":"2024-12-31","categories":["java"],"tags":["groovy"]},{"title":"groovy-学习使用(一)","url":"/2024/12/31/groovy-学习使用-一-/","content":"","date":"2024-12-31","categories":["java"],"tags":["groovy"]},{"title":"jackson使用代码进行配置","url":"/2024/12/31/jackson使用代码进行配置/","content":"jackson 使用代码进行配置的一个例子 Jackson配置属性如果上面的工具类实例，在Jackson中存在一些属性配置，这些配置决定了最后在解析或者编码后数据视图。因此，在分析Jackson之前，先了解下，Jackson具有的一些配置含义。 JsonParser解析相关配置属性 Note: 在枚举最后有一个公共静态方法collectDefaults()，这个方法返回一个整形，整形包含的是所有枚举项对应位bit为初始默认值（true：1；false：0），如果默认属性为true，则通过对1 << ordinal()的值和flags进行亦或来置位。 DeserializationConfig反序列化相关配置属性将Java 对象序列化为Json字符串。Jackson在序列化Java对象的时候，对于有些不存在的属性处理，以及一些类型转换等，都可以通过配置来设置。","date":"2024-12-31","categories":["java"],"tags":["jackson"]},{"title":"jackson使用指南","url":"/2024/12/31/jackson使用指南/","content":"jaskson注解详解序列化相关注解@JsonAnyGetter这个注解可以将javabean中的mapper在序列化的时候展开成变量 比如这个json 就可以由这个javabean序列化而来 @JsonGetter指定变量名称和getter方法的绑定关系,比如下面的例子将name属性和getThemName方法绑定在一起 @JsonPropertyOrder指定序列化的顺序 注意：我们还可以使用@JsonPropertyOrder（alphabetic &#x3D; true）按字母顺序对属性进行排序 @JsonRawValue强制jack序列化 被格式化的json字符串 比如有一个javabean 其中string存了一个json，可以使用这个注解将这个json格式化出来 @JsonValue@JsonValue表示库将用于序列化整个实例的单个方法 简单的说就是指定了一个方法用来在序列化的时候调用 @JsonRootName将序列化的javabean外层包裹一个包名称 @JsonSerialize自定义序列化过程 Jackson反序列化注释@JsonCreator和@JsonProperty连用可以在创建bean的时候强制绑定json中的属性 比如这个json中的theName绑定到javabean中的name @JacksonInject属性注入，这个场景貌似有点晕 @JsonAnySetter这个@JsonAnyGetter相反 @JsonAnySetter使我们可以灵活地使用Map作为标准属性。反序列化时，JSON中尚未绑定的属性，将被简单地添加到map中 @JsonSetter@JsonSetter是一种替代@JsonProperty -这标志着方法作为setter方法。 当我们需要读取一些JSON数据但目标实体类与该数据不完全匹配时，这非常有用，因此我们需要调整过程以使其适合。 和@JsonGetter逻辑相同 @JsonDeserialize自定义反序列化逻辑 @JsonAlias反序列化过程为属性的一个或多个的替代名称 Jackson属性展示型注解@JsonIgnoreProperties@JsonIgnoreProperties是一个类级别的注释，用于标记Jackson将忽略的一个属性或一系列属性。 @JsonIgnore该@JsonIgnore注释用来标记在外地被忽略的属性。 @JsonIgnoreType@JsonIgnoreType将带注释类型的所有属性标记为忽略。 @JsonInclude @JsonAutoDetect@JsonAutoDetect可以覆盖默认语义，即哪些属性可见，哪些属性不可见。 让我们来看一个简单的示例，该批注如何非常有用–让我们序列化私有属性： Jackson多态类型处理注释 有点麻烦掠过 其他通用注解@JsonProperty在处理非标准的getter和setter时，让我们使用@JsonProperty对属性名称进行序列化&#x2F;反序列化： @JsonFormat该@JsonFormat序列化的日期&#x2F;时间值时，注解指定的格式。 @JsonUnwrapped@JsonUnwrapped定义在序列化&#x2F;反序列化时应该解包&#x2F;展平的值。 @JsonView@JsonView指示将在其中包含属性以进行序列化&#x2F;反序列化的View。 @ JsonManagedReference，@ JsonBackReference该@JsonManagedReference和@JsonBackReference注释可以处理父&#x2F;子关系和解决循环。 @JsonIdentityInfo@JsonIdentityInfo指示在对值进行序列化&#x2F;反序列化时应使用对象标识，例如，以处理无限递归类型的问题。 @JsonFilter该@JsonFilter注释指定一个过滤器序列化过程中使用。 自定义杰克逊注释创建自定义Jackson注释。我们可以使用@JacksonAnnotationsInside注释 我们可以看到它如何将现有的注释合并为一个更简单的自定义注释，可以用作速记：","date":"2024-12-31","categories":["java"],"tags":["jackson"]},{"title":"java-EE-servlet3.1规范","url":"/2024/12/31/java-EE-servlet3-1规范/","content":"说道spring的web支持首先就可以想到了spring MVC 的技术(其他的技术还有spring的webflux 以后讨论),从这片博客开始要进行相关知识点的整理. spring MVC 整体的架构设计 spring 自己的webapplication支持嵌套作用域,通过这个方法可以实现spring applicationcongtext的继承特性(继承特性,子作用域可以访问夫作用域的中的属性,但是父作用域中的属性无法访问子作用域中的属性,具体的使用看HierarchicalBeanFactory) web容器的初始化设置springMVC 支持使用xml进行配置 从spring5.0 开始 spring官方文档提倡使用接口配置,容器在初始化的时候将会自动的加载这个接口的实现类从而进行配置spring mvc 的自动化配置是通过 WebApplicationInitializer 接口实现的 但是系统提供了更加高级的接口 AbstractAnnotationConfigDispatcherServletInitializer 如果使用基于xml 的 spring配置则需要使用这个方法进行相关的调用 如果要添加filter 配置 重构如下的方法 这个方法将会为每一个filter 添加一个默认的过滤器,并且自动的添加到对应的display中 这个方法还有一个isisAsyncSupported() 默认情况下返回true 表示spring mvc 框架中的filter 将会异步的处理请求 当使用idea 进行操作的时候注意要保证spring—web包要在class path目录下，不然tomcat等web 容器将不会使用spi技术将相关的各种需要的东西夹在到class中 引申： tomcat 此处实现spi技术的解析 spring的web包的META-INF的文件夹中有一个名为，javax.servlet.ServletContainerInitializer的文件，其中的内容org.springframework.web.SpringServletContainerInitializer，表示定义的ServletContainerInitializer和spring的实现接口SpringServletContainerInitializer，其中@HandlesTypes注解表示CustomServletContainerInitializer 可以处理的类，在onStartup 方法中，可以通过Set<Class<?>> c 获取得到。 见下面代码： 最核心类 DispatcherServlet如果要说这个类就需要看一下springmvc的流程图 ; 在这里之前 DispathcerServlet 将会webapplicationcontext的字符串引用放入java中 一个一个看 HandlerMapping 这个类解决了 url地址映射到对应的处理类中，主要有两个实现RequestMappingHandlerMapping-为@RequestMapping 注解提供支持 ，SimpleUrlHandlerMapping，实现简单的url地址映射 HandlerExceptionResolver 这个是试图返回的异常处理包括相关的错误处理方法 HandlerIntercepter 处理相关的接口进行拦截 HandlerAdapter 使用适配器模式，将试图的映射由指定的接口处理 各种resolver 提供视图解析展示的功能 LocaleResolver, LocaleContextResolver，ThemeResolver，MultipartResolver FlashMapManager 处理flash的时候使用的 估计用不到了 Interception 拦截器在spring MVC 中声明springmvc 的方法有如下几种： 实现HandlerInterceptor接口或者实现HandlerInterceptorAdapter 抽象类 实现WebRequestInterceptor接口，或者实现了WebRequestInterceptor的类 HandlerInterceptor 接口方法(1)boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handle)方法，这个将会在请求传入之前进行相关的处理，如果返回值是true，将会自动的进行链式调用其他按照顺序执行的定时器，否则将会终止调用controll和其他拦截器。 (2) postHandle (HttpServletRequest request, HttpServletResponse response, Object handle, ModelAndView modelAndView) 方法,这个方法将会在处理器执行完后进行处理，和preHandle的执行方法，注意这个方法将不会自动拦截@requsetBody注解ResponseEntity 注意这里：（1）和（2）的第三个参数 handle 官方的解释是@controller标记的本身或者使用HandleMethod这个类，其实这里是springmvc 自己增强的方法， ResponseEntity 详解： 这个类其实是对 http请求的一个封装，封装了http的报头，状态码，http code 等数据，本质上是一种通信协议 如果使用的不是string类型，那么将会是一种类似rpc协议 当使用string类型的时候就和@ResponceBody+@ResponseStatus 相同 (3)afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handle, Exception ex) 方法，该方法也是需要当前对应的Interceptor 的preHandle 方法的返回值为true 时才会执行。该方法将在整个请求结束之后，也就是在DispatcherServlet 渲染了对应的视图之后执行。这个方法的主要作用是用于进行资源清理工作的。 除了实现HandlerInterceptor可以实现拦截器之外，使用WebRequestInterceptor 同样可以实现拦截器 (1)preHandle(WebRequest request) 方法。注意这个方法没有返回直，一般主要用它来进行资源的准备工作，其中的webrequest参数是HttpServletRequest的加强版，可以使用setAttribute(name, value, scope)添加参数到指定的作用域中，scope参数有如下的几个值： 1. SCOPE_REQUEST ：它的值是0 ，代表只有在request 中可以访问。 2. SCOPE_SESSION ：它的值是1 ，如果环境允许的话它代表的是一个局部的隔离的session，否则就代表普通的session，并且在该session范围内可以访问。 3. SCOPE_GLOBAL_SESSION ：它的值是2 ，如果环境允许的话，它代表的是一个全局共享的session，否则就代表普通的session，并且在该session 范围内可以访问。(2)postHandle(WebRequest request, ModelMap model) 方法。该方法将在请求处理之后，也就是在Controller 方法调用之后被调用，但是会在视图返回被渲染之前被调用，request 就是传递的请求参数，model就是返回的视图(3)afterCompletion(WebRequest request, Exception ex) 方法。该方法会在整个请求处理完成，也就是在视图返回并被渲染之后执行。所以在该方法中可以进行资源的释放操作。而WebRequest 参数就可以把我们在preHandle 中准备的资源传递到这里进行释放。Exception 参数表示的是当前请求的异常对象，如果在Controller 中抛出的异常已经被Spring 的异常处理器给处理了的话，那么这个异常对象就是是null 。 ####HandlerExceptionResolver springmvc 异常处理 异常处理用于处理@controll 这种接口抛出的各种异常，主要有如下的几种 SimpleMappingExceptionResolver 异常类名称和错误视图名称之间的映射。用于在浏览器应用程序中呈现错误页面。 DefaultHandlerExceptionResolver 解决Spring MVC引发的异常并将它们映射到HTTP状态代码。另请参阅备用ResponseEntityExceptionHandler和REST API例外。 ResponseStatusExceptionResolver 根据@ResponseStatus注释中的值解决注释中的异常并将其映射到HTTP状态代码。 ExceptionHandlerExceptionResolver 通过调用@ExceptionHandler一个@Controller或一个 @ControllerAdvice类中的方法来解决异常。请参阅@ExceptionHandler方法。 对于异常处理来说通常的解决结果有如下的几种方法 使用ModelAndView 指向错误视图。 返回空的modelandview 如果异常被处理 如果异常没有被解决，将会使用异常调用连进行处理，如果调用到最后将会抛出到servlet中 在spring mvc 中集中进行异常处理的有三种方法 使用@ResponseStatus 注释一个异常类，当spring中抛出这个异常的时候将会自动的交由这个类处理,并且可以制定http code值，比如下面的方法将会跑出403错误 注意如果这个注解使用在 一个方法上的时候 ， 不论结果如何都将会放回制定的httpcode 异常 2.@ControllerAdvice 和 @ExceptionHandler 这两个注解同样用于异常处理 （1）@ExceptionHandler 当一个Controller中有方法加了@ExceptionHandler之后，这个Controller其他方法中没有捕获的异常就会以参数的形式传入加了@ExceptionHandler注解的那个方法中。注意这个方法要有一个参数，这个参数就是指定要处理的异常 （2）@ControllerAdvice 实现这个注解的类可以让这个类中 @ExceptionHandler标记的方法实现全觉异常监听 最后对于springmvc 如果异常没有被处理，spring提供了默认的页面展示异常，使用如下方法配置xml ： 制定默认错误页面 java ： 处理url ViewResolver 提供了view 名称到view实例之间的绑定，并且完成真正视图展示之前将相关数据进行整理的功能spring mvc 中将相关的视图变成一个网页经历的过程 将SpringMVC控制器中的返回结果封装成一个ModelAndView对象。 通过SpringMVC中的视图解析器，使用ViewResolver对控制器返回的ModelAndView对象进行解析，将逻辑视图转换成物理视图。 调用View中的render()方法对物理视图进行渲染。 几个主要的视图的介绍 AbstractCachingViewResolver: 最抽象的类提供了视图缓存的功能 UrlBasedViewResolver: 提供了更加细粒度的url控制，支持前缀后缀等功能，但是使用这个方法必须制定相关的view解析工具，默认使用的是jsp解析工具InternalResourceView（和可以使用其他的是解析工具比如FreeMarkerView等）， 并且提供了重定向和转发的功能 InternalResourceViewResolver：这个方法是 UrlBasedViewResolver 的子类，支持父类的所有功能，InternalResourceViewResolver会把返回的视图名称都解析为InternalResourceView对象，内部使用重定向的方法，将controller返回的view 包装成InternalResourceView， 并且鞋带上前缀和后缀，同时再转发出去 视图解析链：在SpringMVC中可以同时定义多个ViewResolver视图解析器，然后它们会组成一个ViewResolver链。当Controller处理器方法返回一个逻辑视图名称后，ViewResolver链将根据其中ViewResolver的优先级来进行处理。所有的ViewResolver都实现了Ordered接口，在Spring中实现了这个接口的类都是可以排序的。在ViewResolver中是通过order属性来指定顺序的，默认都是最大值。所以我们可以通过指定ViewResolver的order属性来实现ViewResolver的优先级，order属性是Integer类型，order越小，对应的ViewResolver将有越高的解析视图的权利，所以第一个进行解析的将是ViewResolver链中order值最小的那个。当一个ViewResolver在进行视图解析后返回的View对象是null的话就表示该ViewResolver不能解析该视图，这个时候如果还存在其他order值比它大的ViewResolver就会调用剩余的ViewResolver中的order值最小的那个来解析该视图，依此类推。当ViewResolver在进行视图解析后返回的是一个非空的View对象的时候，就表示该ViewResolver能够解析该视图，那么视图解析这一步就完成了，后续的ViewResolver将不会再用来解析该视图。当定义的所有ViewResolver都不能解析该视图的时候，Spring就会抛出一个异常。 spring mvc 常用注解spring 提供了一整套注解来简化spring相关的配置 @Controll 和 @RestControllerSpring MVC提供了一种基于注释的编程模型，其中@Controller和@RestController组件使用注释来表示请求映射，请求输入，异常处理等。 其中的@RestController 是@ResponseBody和@Controller注解的一种集合。 注意：如果使用aop方法对controller 进行增强的话请使用class-based proxying代理，但是如果使用了非spring context回调接口的方法的时候，需要明确的制定相关的配置信息：tx:annotation-driven/, 改变为 <tx:annotation-driven proxy-target-class&#x3D;”true”&#x2F;>. @RequestMapping requestMapping:有简化版的各种注解@GetMapping，@PostMapping，@PutMapping，@DeleteMapping，@PatchMapping，指定了相关的method方法对应的各种请求 request 请求可以接受的请求可以通过通配符或者glob参数的方法进行匹配 spring mvc 的地址匹配方法满足一定的相关原则，可以使用通配符进行匹配 Wildcard Description ？ 匹配任何单字符 * 匹配0或者任意数量的字符 ** 匹配0或者更多的目录 注意：spring将会按照匹配的字符最长的那个进行匹配，比如&#x2F;**&#x2F;.jsp 和&#x2F;app&#x2F;dir&#x2F;.jsp，在这个过程中，将会匹配后者。 注意：spring mvc 的匹配原则是按照后缀匹配的原则，比如一个url地址 &#x2F;name这个地址，表示的就是&#x2F;name.*，一定程度上实现了文件扩展名引用 如果使用了文件扩展名称这种东西如果想要配置请查看如下两个接口： 在requestMethod中可以接收的数据接受的函数体中可以使用的注解和参数 WebRequest, NativeWebRequest WebRequest是Spring Web MVC提供的统一请求访问接口，不仅仅可以访问请求相关数据（如参数区数据、请求头数据，但访问不到Cookie区数据），还可以访问会话和上下文中的数据；NativeWebRequest继承了WebRequest，并提供访问本地Servlet API的方法。 javax.servlet.ServletRequest, javax.servlet.ServletResponse，javax.servlet.http.HttpSession javax 提供的具体接口注意其中的httpsession ，会话访问不是线程安全的。如果允许多个请求同时访问会话，请考虑将RequestMappingHandlerAdapter的“synchronizeOnSession”标志设置为“true”。 HttpMethod 这个值中有传入的方式，比如get还是post java.io.InputStream, java.io.Reader，java.io.OutputStream, java.io.Writer 请求的原始请求数据流，和原始返回数据 @PathVariable和@MatrixVariable 使用{}表示的url请求对应的相关参数, @PathVariable 没什么好说的关键是@MatrixVariable ， 这个注解将会自动的匹配url 地址中 ;uuu&#x3D;123;iii&#x3D;333 这种参数，并且一定的程度下并不需要{jj}中指定的名称来匹配，当发生参数冲突的时候可以使用pathVar参数指定名称，目前有bug不记录了 @PathVariable,@MatrixVariable,@RequestParam，@RequestBody，@RequestHeader，@CookieValue，@RequestPart,@ModelAttribute,@SessionAttribute,@RequestAttribute @PathVariable ,spring mvc提供了一套支持reastfulapi的方法，这套方法可以使用{}+通配符的方式，将url中的数据传递到method 对应的参数中 注意：{}里面的值值也可以使用正则表达式进行相关的配置，格式：{varName:regex} @MatrixVariable，这个注解将会自动的匹配url地址中;uuu&#x3D;123;iii&#x3D;333这种参数，并且一定的程度下并不需要{jj}中指定的名称来匹配，当发生参数冲突的时候可以使用pathVar参数指定名称，目前有bug不记录了 @RequestParam接口将会自动的将传入到指定的地址中 如果使用这个注解标识Map<String, String> or MultiValueMap<String, String>，将会将所有的属性注入进来A） 常用来处理简单类型的绑定，通过Request.getParameter() 获取的String可直接转换为简单类型的情况（ String–> 简单类型的转换操作由ConversionService配置的转换器来完成）；因为使用request.getParameter()方式获取参数，所以可以处理get 方式中queryString的值，也可以处理post方式中 body data的值；B）用来处理Content-Type: 为 application&#x2F;x-www-form-urlencoded编码的内容，提交方式GET、POST；C) 该注解有两个属性： value、required； value用来指定要传入值的id名称，required用来指示参数是否必须绑定； @RequestBody 该注解常用来处理Content-Type: 不是application&#x2F;x-www-form-urlencoded编码的内容，例如application&#x2F;json, application&#x2F;xml等；它是通过使用HandlerAdapter 配置的HttpMessageConverters来解析post data body，然后绑定到相应的bean上的。 使用：HttpMessageConverter接口，需要开启<mvc:annotation-driven &#x2F;>。 AnnotationMethodHandlerAdapter将会初始化7个转换器，可以通过调用AnnotationMethodHandlerAdapter的getMessageConverts()方法来获取转换器的一个集合 List，这7个转化器如下： PS:Spring默认的json协议解析由Jackson完成。 这种方式的时候默认使用的就是json解析。 问题 如何扩展 ps：如果使用xml 方式，需要使用注解 注意 这里如果要使用注意spring mvc 环境的配置 使用xml 进行配置 使用java 进行配置 @RequestPart 和文件上传相关，难度有点大，和http协议相关的暂时不考虑 @RequestHeader 将http协议中相关的头注入到指定的数据中 @ModelAttribute 在使用model view 场景下，有如下的一应用 全局model配置，在获得请求&#x2F;helloWorld 后，populateModel方法在helloWorld方法之前先被调用，它把请求参数（&#x2F;helloWorld?abc&#x3D;text）加入到一个名为attributeName的model属性中，在它执行后 helloWorld被调用，返回视图名helloWorld和model已由@ModelAttribute方法生产好了。 指派model配置 返回 helloworld.do视图，有一个model 参数是attributeName和值 hi 绑定application&#x2F;x-www-form-urlencoded 提交的请求中 的值到对象中 支持user.xxx,user2.ddd 嵌套对应法 @CookieValue 和之前的相同，就是将cookie中相关的数据拿出来 HttpEntity HttpEntity或多或少与使用@RequestBody相同，但基于公开请求标头和主体的容器对象。 注意这个方法多是用于post请求用来针对ajax序列化的json对象解析，其中有一个getbody方法 @InitBinder spring自带的数据处理模块 由@InitBinder表示的方法，可以对WebDataBinder对象进行初始化。WebDataBinder是DataBinder的子类，用于完成由表单到JavaBean属性的绑定。@InitBinder方法不能有返回值，它必须盛名为void。@InitBinder方法的参数通常是WebDataBinder，@InitBinder可以对WebDataBinder进行初始化。 注意这个注解只是针对这个controller中的方法起作用，无法针对所有的controller 注意WebDataBinder这个对象，这个对象拥有一个方法registerCustomEditor,这个方法将会自动的配置属性映射器，将相关的属性映射到指定的位置，属性映射器可以使用如下的方法进行自定义实现PropertyEditor或者重写PropertyEditorSupport对象中的方法，注意这种方法只能实现string到对象的转换 setValue中就是转化后的对象，setAsText传入的就是传入的url字符串 同时WebDataBinder这个对象，这个对象拥有一个addCustomFormatter 可以直接使用formatter进行参数转化本质上是相同的 addCustomFormatter本质上还是和registerCustomEditor是一样的见源代码 @RequestMapping 方法返回值中的参数 @ResponseBody HttpEntity, ResponseEntity HttpEntity HttpEntity或多或少与使用@RequestBody相同，但基于公开请求标头和主体的容器对象。 ResponseEntity 这个类其实是对 http请求的一个封装，封装了http的报头，状态码，http code 等数据，本质上是一种通信协议 如果使用的不是string类型，那么将会是一种类似rpc协议 当使用string类型的时候就和@ResponceBody+@ResponseStatus 相同 HttpHeaders 返回一个封装的httpheaders的头，这个类有一个set方法，制定方法的头和内容，如果想深入的使用，需要精通http协议 string 最简单的一个方法，spring将会使用这个字符串找到对应的view java.util.Map, org.springframework.ui.Model，@ModelAttribute spring mvc modelandview体系的东西 ResponseBodyEmitter, SseEmitter， StreamingResponseBody Reactive types — Reactor, RxJava, or others via ReactiveAdapterRegistry @ResponseStatus(HttpStatus.CREATED) 制定返回值的头部信息, 比如制定401 402 这种http code","date":"2024-12-31","categories":["java"]},{"title":"java-JAAS(Java 认证和授权服务)","url":"/2024/12/31/java-JAAS-Java-认证和授权服务-/","content":"之前写过一篇有关java内部安全机制和沙箱的一片文章,那一篇中jvm是基于代码的维度进行权限管理的,但是java在java1.4内置一个JAAS基于用户的权限管理(虽然个人觉得这个东西非常鸡肋,适合单体应用并不适合现在的微服务和分布式框架,因为有个非常坑爹的配置文件-我权限管理都是使用数据库实现的:disappointed_relieved:) JAAS的大体组成 客户端通过一个LoginContext对象与JAAS相互作用，这个LoginContext对象提供了一种开发应用程序的方式，它不依赖于底层的认证技术。 LoginContext：是javax.security.auth.login包里的一个类，它描述了用于验证对象(subjects)的方法。 Subject：就是在某个你想去认证和分配访问权限的系统里的一个标识。一个主体(subject)可能是一个用户、一个进程或者是一台机器，它用javax.security.auth.Subject类表示。 java.security.Principal： 由于一个Subject可能涉及多个授权（一个网上银行密码和另一个电子邮件系统），java.security.Principal就被用作在那些关联里的标识。也就是说，该Principal接口是一个能够被用作代表某个实体、公司或者登陆ID的抽象概念。一个Subject可能包含多个Principles. 稍后将有一个示例类实现了这个Principal接口。 LoginContext对象调用负责实现和执行认证的LoginModules。LoginModule接口(在javax.security.auth.spi 包里)必须让认证技术的提供者去实现，并且能够被应用程序指定提供一个特定认证类型。 LoginContext： 用来读取Configuaration和实例化特定的LoginModules. Configuaration： 被某个特定的应用程序用作指定认证技术或者LoginModule。因此，不同的LoginModules能够被应用到某个应用程序而不用对这个应用程序做任何的代码修改。 CallbackHandler 安全机制的回调器处理器,用来处理和显示回调的信息 一个例子 主类:关键的一句第八行-在配置文件example.conf里，实体名”WeatherLogin”就是被MyClient.java用作关联这个实体的名字。这里同时还将一个回调处理程序传给了底层的LoginModule，因此他们能够通过提示用户名&#x2F;密码同用户进行交流和作用，例如：通过文本或者图形用户接口。一旦LoginContext已经被实例化了，login方法就被调用去登陆. 回调类:底层安全服务可能要求通过传递单个的callbacks到回调处理程序。基于传递的callbacks，回调处理程序决定怎样去获取和显示信息。 Module类:真实的控制操作类 Principal : Principal接口的一个实现。 配置文件设置 LoginContext通过读取Configuration去决定那个LoginModule将被使用。 使用下面的命令（指定了login配置文件）运行客户端. 引申:如果开启了安全模式java -Djava.security.manager,需要指定安全策略的 运行如下的命令运行 结束语: 看看就好了,知道是个啥就行了,基本上用不上","date":"2024-12-31","categories":["java"]},{"title":"java-System.getProperty(xxxx)方法解惑","url":"/2024/12/31/java-System-getProperty-xxxx-方法解惑/","content":"System.getProperty(“java.version”) 方法是输出java的运行环境变量(系统属性)的方法 主要可以输出的数据有: java.version Java 运行时环境版本java.vendor Java 运行时环境供应商java.vendor.url Java 供应商的 URLjava.home Java 安装目录java.vm.specification.version Java 虚拟机规范版本java.vm.specification.vendor Java 虚拟机规范供应java.vm.specification.name Java 虚拟机规范名称java.vm.version Java 虚拟机实现版本java.vm.vendor Java 虚拟机实现供应商java.vm.name Java 虚拟机实现名称java.specification.version Java 运行时环境规范版本java.specification.vendor Java 运行时环境规范供应商java.specification.name Java 运行时环境规范名称java.class.version Java 类格式版本号java.class.path Java 类路径java.library.path 加载库时搜索的路径列表java.io.tmpdir 默认的临时文件路径java.compiler 要使用的 JIT 编译器的名称java.ext.dirs 一个或多个扩展目录的路径os.name 操作系统的名称os.arch 操作系统的架构os.version 操作系统的版本file.separator 文件分隔符（在 UNIX 系统中是“&#x2F;”）path.separator 路径分隔符（在 UNIX 系统中是“:”）line.separator 行分隔符（在 UNIX 系统中是“&#x2F;n”）user.name 用户的账户名称user.home 用户的主目录user.dir 用户的当前工作目录 java运行环境的设置方法其实本质上就是使用jvm虚拟机提供的-D参数 比如设置 java -Done.xxx.yyy&#x3D;123 功能解析-D&#x3D;value官网解释： Set a system property value. If value is a string that contains spaces, you must enclose the string in double quotes:在虚拟机的系统属性中设置属性名&#x2F;值对，运行在此虚拟机上的应用程序可用： 得到value的值。如果value中有空格，则需要用双引号将该值括起来，如：-Dname&#x3D;”kazaf f” 该参数通常用于设置系统级全局变量值，如配置文件路径，保证该属性在程序中任何地方都可访问。 注意事项 需要设置的是JVM参数而不是program参数； 使用此参数的参数优先级最高，会覆盖项目中配置的此项；","date":"2024-12-31","categories":["java"]},{"title":"java-classpath的理解","url":"/2024/12/31/java-classpath的理解/","content":"这次写这一片篇文章的起因是对老大搭建的底层框架存在怀疑结果然后自己照着老大的方法运行竟然可以成功原因的费解。因此在这里记录一下自己的疑问解惑过程。 classpath到底是啥我的理解classpath 其实是 java 的一种虚拟的文件目录结构，java通过包名（相当于路径名称），针对每一个classpath地址（java支持多个classpath地址）进行资源管理的一种抽象路径 1. 针对于资源加载最直观是我在mybatis中配置map xml文件路径的时候会加上 这么做将会让mybati所有所有的classpat路径下的对应的map&#x2F;目录中的文件 也就是说如果jar包1 中有map&#x2F;目录，并且jar2 中也有这个目录 jar1 引用了jar2，当jar1运行的时候并且jar2又在classpath当中的时候将会自动的搜索两个jar包classpath相对路径下的，map&#x2F;目录下的信息 注意如果两个文件名称是相同的（包括classpath对应的相对路径），将会以先加载的文件优先 2. 对与和class对象加载jvm的类加载分三中方式： 系统级别：rt.jar 扩展级别：java_home&#x2F;jre&#x2F;lib&#x2F;ext&#x2F;目录下的jar文件 应用级别：环境变量中的classpath或javac java中的参数指定java -classpath … 或者自己写ClassLoader加载。 前面2中是JVM自动处理。其中第二种是为了处理Java的classpath灾难而提供的解决方案。 实际上java虚拟机是由java luncher初始化的，也就是java（或java.exe）这个程序来做的。 虚拟机按以下顺序搜索并装载所有需要的类： 引导类:组成java平台的类，包含rt.jar和i18n.jar中的类。 扩展类：使用java扩展机制的类，都是位于扩展目录（$JAVA_HOME&#x2F;jre&#x2F;lib&#x2F;ext）中的。jar档案包。 用户类:开发者定义的类或者没有使用java扩展机制的第三方产品。你必须在命令行中使用-classpath 选项或者使用CLASSPATH环境变量来确定这些类的位置。我们在上面所说的用户自己的类就是特指这些类。 这样，一般来说,用户只需指定用户类的位置，引导类和扩展类是 “自动 “寻找的。当你的程序需要第三方的类库支持，而且比较常用，就可以采用此种方法。比如常用的数据库驱动程序，写servlet需要的servlet包等等。设置方法就是在环境变量中加入CLASSPATH.然后就可以直接编译运行了。而你的程序只用了些基础类，寻找时，就用不着必须设定它。 在执行Java程序的时候，会自动加载程序用中需要的在rt.jar和其他java_home\\jre\\lib中包含的。jar文件中包含的Java基础类库和一些扩展类库。这些都是JVM自动处理的，对用户来说是透明的。 如果Java程序中使用到了一些应用级别的类（如第三方的类），可以在javac和java中的-classpath选项中指定它们的搜索路径，或者是自己写ClassLoader加载，另外也可以设置ClassPath环境变量，在里面指定那些蝶阀应用级别的类的搜索路径。 设置ClassPath环境变量不是必须的，只是为了方便使用，设置了ClassPath，JDK就会按ClassPath制定的路径去搜索需要的应用级别的类，而不必每一次都使用-classpath选项或自己写ClassLoader。 还有需要注意的就是：如果相关的类就在当前工作目录下的话，上面3种方法都可以不要，因为JDK系统会首先搜索会在当前的工作目录中搜索程序相关的类。 注意如果在class系统中两个类称是相同的（包名+类名），将会以先加载的类优先","date":"2024-12-31","categories":["java"]},{"title":"java-io-javaio操作的函数整理","url":"/2024/12/31/java-io-javaio操作的函数整理/","content":"java io 操作是一个非常重要的工具包,同样也是网络编程和各种框架编写的基本函数库,这里做一下整理 java io的层次结构 这里就先整理一下我最长用的吧 BufferReader和BufferWriter 这两个提供了writer 和 readLine 的功能InputStreamReader和OutputStreamWriter 这两个提供了字节流转字符流的方法PrintWriter 可以 象System.out.println() 那样的输入流","date":"2024-12-31","categories":["java"]},{"title":"java-jar包文件读取方法和资源使用","url":"/2024/12/31/java-jar包文件读取方法和资源使用/","content":"","date":"2024-12-31","categories":["java"]},{"title":"java-java怎样一行一行的读取字符串","url":"/2024/12/31/java-java怎样一行一行的读取字符串/","content":"最近开发中遇到了一个问题,需要一行一行的读取字符串,自己想了想总结了两种方法\\ 方法一 使用Scanner类 这种方法就是使用了Scanner特性,思路是将带换行符的字符串想象成聪明行进行输入的 方法二 使用Reader进行处理","date":"2024-12-31","categories":["java"]},{"title":"java-java的SPI和API详解","url":"/2024/12/31/java-java的SPI和API详解/","content":"api 机制 好说其实就是定义一个接口然后使用他的一个实现类去进行上转型 但是使用spi则是另一种特殊的方法 SPI 全称为 (Service Provider Interface) ,是JDK内置的一种服务提供发现机制(使用接口发现他的实现类-ServiceLoader s &#x3D; ServiceLoader.load(Search.class);)。 目前有不少框架用它来做服务的扩展发现， 简单来说，它就是一种动态替换发现的机制， 举个例子来说， 有个接口，想运行时动态的给它添加实现，你只需要添加一个实现.而后，把新加的实现，描述给JDK知道就行啦（通过改一个文本文件即可） 公司内部，目前Dubbo框架和早期的common.log使用这种方法。 使用的结构图如下: !()[blogimg&#x2F;1.jpg] 其实使用spi的核心就是这个配置文件,这个配置文件在jar包的路径如下 我们都知道vm使用双亲委派机制的类加载器加载各种类(ClassLoader A -> System class loader -> Extension class loader -> Bootstrap class loader)在spi 中,SPI 接口的代码中使用线程上下文类加载器(使用 ServiceLoader loaders &#x3D; ServiceLoader.load(接口名称.class);)，就可以成功的加载到 SPI 实现的类. 具体的实现见下面的代码 看一个实例 接口: 三个实现类 META-INF&#x2F;services&#x2F;com.liu.spi.IA这个文件的内容(如果使用maven工程构建在 src&#x2F;main&#x2F;resources&#x2F;META-INF&#x2F;services&#x2F;com.liu.spi.IA这个目录) 运行的主函数","date":"2024-12-31","categories":["java"]},{"title":"java-jvm常用参数整理","url":"/2024/12/31/java-jvm常用参数整理/","content":"堆 －Xms和—Xmx 堆的最小值 & 堆的最大值默认值是物理内存的1&#x2F;4(<1GB) & 默认值是物理内存的1&#x2F;64(<1GB)空余堆内存小于40%时，JVM就会增大堆直到-Xmx的最大限制;空余堆内存大于70%时，JVM会减少堆直到 -Xms的最小限制;通常会将 -Xms 与 -Xmx两个参数配置相同的值，目的是为了能够在java垃圾回收机制清理完堆区后不需要重新分隔计算堆区的大小而浪费资源。 新生代 -Xmn 设置新生代大小。值＝eden+ 2 survivor sun公司推荐大小整个java堆的3&#x2F;8 -XX:NewRatio 新生代（eden+2*Survivor）和老年代（不包含永久区）的比值 -XX:SurvivorRatio（幸存代） 设置Eden:1个Survivor内存空间大小的比值 -XX:NewSize 设置年轻代大小 -XX:MaxNewSize 设置年轻代最大值 栈 －Xss 栈容量（ps：－Xoss用于设定本地方法栈大小，但是对于HotSpot来说不起作用，因为其不区分本地方法栈和虚拟机栈）JDK5.0以后每个线程堆栈大小为1M 方法区 －XX：PermSize 方法区大小 －XX：MaxPermSize 最大方法区大小 ps：方法区大小和最大方法区大小限制了常量池的容量 其它 －XX：＋PrintGCDetails 打印GC日志 －XX：＋&#x2F;-UseTLAB 是否使用本地线程分配缓存 了解参数 -XXThreadStackSize 设置线程栈的大小(0 means use default stack size) -XXThreadStackSize 设置内存页的大小，不可设置过大，会影响Perm的大小 -XX:+UseFastAccessorMethods 设置原始类型的快速优化 -XX:+DisableExplicitGC设置关闭System.gc()(这个参数需要严格的测试)-XX:MaxTenuringThreshold设置垃圾最大年龄。如果设置为0的话,则年轻代对象不经过Survivor区,直接进入年老代. 对于年老代比较多的应用,可以提高效率。如果将此值设置为一个较大值,则年轻代对象会在Survivor区进行多次复制,这样可以增加对象再年轻代的存活时间,增加在年轻代即被回收的概率。该参数只有在串行GC时才有效.-XX:+AggressiveOpts加快编译-XX:+UseBiasedLocking锁机制的性能改善-Xnoclassgc禁用垃圾回收-XX:SoftRefLRUPolicyMSPerMB设置每兆堆空闲空间中SoftReference的存活时间，默认值是1s 。（softly reachable objects will remain alive for some amount of time after the last time they were referenced. The default value is one second of lifetime per free megabyte in the heap）-XX:PretenureSizeThreshold设置对象超过多大时直接在旧生代分配，默认值是0。-XX:TLABWasteTargetPercent设置TLAB占eden区的百分比，默认值是1% 。-XX:+CollectGen0First设置FullGC时是否先YGC，默认值是false。 ———————————————— 版权声明：本文为CSDN博主「iCoding91」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。 原文链接：https://blog.csdn.net/caoxiaohong1005/article/details/82931474","date":"2024-12-31","categories":["java"]},{"title":"java-web乱码问题解决","url":"/2024/12/31/java-web乱码问题解决/","content":"request.setCharacterEncoding(“UTF-8”)&#x2F;&#x2F;作用是设置对客户端请求进行重新编码的编码 该方法用来指定对浏览器发送来的数据进行重新编码（或者称为解码）时，使用的编码。 response.setCharacterEncoding(“UTF-8”)&#x2F;&#x2F;作用是指定对服务器响应进行重新编码的编码 服务器在将数据发送到浏览器前，对数据进行重新编码时，使用的就是该编码。","date":"2024-12-31","categories":["java"]},{"title":"java-一些基本知识点整理","url":"/2024/12/31/java-一些基本知识点整理/","content":"学习java一定的时间了，这里一些知识点的整理 变量必须是字母,下划线或者$符开头不能是 java 关键字变量命名规范 驼峰写法,类首字母大写驼峰写法 基本数据类型 java 运算符赋值运算符 : &#x3D;关系运算符 : > , < , >&#x3D; , <&#x3D; , &#x3D;&#x3D; , !&#x3D; , instanceof**逻辑运算符** : && , || , !**位运算符** : & , | , ^异或 , ~非 , >> , << , >>>条件运算符: ? :扩展赋值运算符 : +&#x3D; , -&#x3D; , *&#x3D; , &#x2F;&#x3D; 流程控制语句 基础方法 scanner 枚举类型 简单枚举类型 Java包注释","date":"2024-12-31","categories":["java"]},{"title":"java-从AccessController.doPrivileged看java的授权和沙箱","url":"/2024/12/31/java-从AccessController-doPrivileged看java的授权和沙箱/","content":"这里主要是介绍java以代码为核心的授权方法(还有基于用户的jaas授权方法不做讨论),这个还是比较重要的(其实是看很多源代码框架使用了这个方法,为了保证代码在沙盒环境中可以运行). 权限在Java平台安全体系结构中，所有访问权限都是类型化的并且有层次结构，其根是抽象类 java.security.Permission 。标准的权限类大约有如下的这些(使用类来标记权限) 类型 权限名 操作 例子 文件权限 java.io.FilePermission 文件名（平台依赖） 读、写、删除、执行 允许所有问价的读写删除执行：permission java.io.FilePermission “<< ALL FILES>>”, “read,write,delete,execute”;。允许对用户主目录的读：permission java.io.FilePermission “${user.home}&#x2F;-“, “read”;。 套接字权限 java.net.SocketPermission 主机名:端口 接收、监听、连接、解析 允许实现所有套接字操作：permission java.net.SocketPermission “:1-“, “accept,listen,connect,resolve”;。允许建立到特定网站的连接：permission java.net.SocketPermission “.abc.com:1-“, “connect,resolve”;。 属性权限 java.util.PropertyPermission 需要访问的jvm属性名 读、写 读标准Java属性：permission java.util.PropertyPermission “java.“, “read”;。在sdo包中创建属性：permission java.util.PropertyPermission “sdo.“, “read,write”;。 运行时权限 java.lang.RuntimePermission 多种权限名[见附录A] 无 允许代码初始化打印任务：permission java.lang.RuntimePermission “queuePrintJob” AWT权限 java.awt.AWTPermission 6种权限名[见附录B] 无 允许代码充分使用robot类：permission java.awt.AWTPermission “createRobot”; permission java.awt.AWTPermission “readDisplayPixels”;。 网络权限 java.net.NetPermission 3种权限名[见附录C] 无 允许安装流处理器：permission java.net.NetPermission “specifyStreamHandler”;。 安全权限 java.security.SecurityPermission 多种权限名[见附录D] 无 序列化权限 java.io.SerializablePermission 2种权限名[见附录E] 无 反射权限 java.lang.reflect.ReflectPermission suppressAccessChecks（允许利用反射检查任意类的私有变量） 无 完全权限 java.security.AllPermission 无（拥有执行任何操作的权限） 无 在这种模式下 jvm的权限是给代码的(可以说类反正就代码的),一个类(代码)可以有多个权限(在$JREHOME&#x2F;lib&#x2F;security&#x2F;java.policy文件中进行配置下面说),使用PermissionCollection这样的类进行封装(jvm在运行的时候通过文件自动生成好了,然后通过反射进行校验这些东西都被隐藏起来了,研究下去意义不大,就没有接着研究) 保护域和代码源 java2 是针对代码(对象类)添加保护区,有一个内部类ProtectionDomain,通过这个类授权访问权限,如果多个类使用同一个ProtectionDomain将会认为这个权限是相同的 显然，一定要能惟一地标识一段运行代码以保证它的访问权限没有冲突。运行代码的惟一标识属性共有两项：代码的来源（代码装载到内存所用的 URL）和代码的 signer 实体（由对应于运行代码的数字签名的一组公共密钥指定）。这两种特性的组合在 Java 2 平台安全体系结构中编写为给定运行代码的 CodeSource (在ProtectionDomain内部可以看见这个对象)。 权限生成的具体过程 Java 运行时通过名为 java.security.Policy 的类（的具体扩展）设置 ProtectionDomain 与授予它的权限之间的映射。 这个类的默认扩展是 sun.security.provider.PolicyFile 。正如其名字所表明的， sun.security.provider.PolicyFile 从一个文件中获得 CodeSource （由位置 URL 和 signer 标识别名）与授予它的权限之间的映射。 可以通过环境变量 java.security.policy 将这个文件的位置作为输入提供给 JVM。 Policy 类提供了一个名为 getPermissions() 的方法，可以调用它以获得授予特定 CodeSource 的一组权限。 权限加载的过程 一个类与 其 ProtectionDomain 之间的映射是在类第一次装载时设置的，并在类被垃圾收集之前不会改变。 一个类通常是由一个名为 SecureClassLoader 的特殊类装载的。 SecureClassLoader 首先从相应 URL 处装载字节，如果需要还会验证包围文档文件的数字签名。然后它调用上述 getPermissions() 方法获得授予类的 CodeSource 的一个填充了静态绑定权限的异类 PermissionCollection 。 然后 SecureClassLoader 创建新的 ProtectionDomain ，传递 CodeSource 及其相关的权限作为其构造函数的参数（当然，这假定对于给定 CodeSource 还不存在 ProtectionDomain 。如果用一个现有的 CodeSource 装载类，那么就会重复使用它已经建立的 ProtectionDomain ） 。 最后，用装载的类字节向 JVM 定义一个类，并在关联的 ProtectionDomain 中维护一个引用指针。 执行过程一个名为 SecurityManager 的类负责实施系统安全策略。在默认情况下不安装安全管理器，必须通过一个在启动时传递给 JVM 的、名为 java.security.manager 的环境变量显式地指定。任何应用程序都可找到安装的 SecurityManager 并调用它相应的 check 方法。如果所要求的权限在给定运行时上下文中是授予的，那么调用将无声地返回。如果权限没有授予，那么将抛出一个 java.security.AccessControlException 。 Java 2 平台安全体系结构通过引入一个名为 AccessController 的新类使这一切变得简单了，并更具有可扩展性。这个类的目的与 SecurityManager 是一样的，即它负责做出访问决定。当然， 为了向后兼容性保留了 SecurityManager 类，但是其更新的实现委派给了底层的 AccessController 。对 SecurityManager 类进行的所有 check 方法调用都解释为相应的 Permission 对象，并将它作为输入参数传递给 AccessController 类的 checkPermission() 方法。 权限的继承和优化问题 如果说类A调用了类B,B调用了类C,那么C的权限就是ABC权限的交集 最终节-> 看java的AccessController.doPrivileged 通过上面的引子,终于了解的这个方法是干啥的了,这个方法就是解决这样一个问题,如果A调用了B,但是A的权限太小,B的权限太大导致,A影响了B的功能,这个使用使用上面的方法,这个方法将会让jvm进行权限计算的时候将会使用B最为最顶层的权限,这个样A或者A之前的权限就对B和B之后的不起作用了 一个简单的小例子策略文件$JREHOME&#x2F;lib&#x2F;security&#x2F;java.policy 参数文件$JREHOME&#x2F;lib&#x2F;security&#x2F;java.security-用来指定运行安全模式下的各种参数(比如在哪里加载策略文件)\\ 代码 启动参数加入-Djava.security.manager使用安全沙箱模式运行,将会报错 这样,在权限文件中添加一条这样的规则 然后代码就通过了 结语 这个特性感觉使用的应该不多,框架里用的挺多,比较冷门记录一下","date":"2024-12-31","categories":["java"]},{"title":"java-从java线程池来看java的阻塞队列","url":"/2024/12/31/java-从java线程池来看java的阻塞队列/","content":"一说到java的阻塞队列，我们就会想到在java的jdk中的那么多的类 1.ArrayDeque, （数组双端队列）2.PriorityQueue, （优先级队列）3.ConcurrentLinkedQueue, （基于链表的并发队列）4.DelayQueue, （延期阻塞队列）（阻塞队列实现了BlockingQueue接口）5.ArrayBlockingQueue, （基于数组的并发阻塞队列）6.LinkedBlockingQueue, （基于链表的FIFO阻塞队列）7.LinkedBlockingDeque, （基于链表的FIFO双端阻塞队列）8.PriorityBlockingQueue, （带优先级的无界阻塞队列）9.SynchronousQueue （并发同步阻塞队列） 这里不去细说的这些东西，而是从线程池的一个异常来聊聊这个事情 构造一个线程池异常 - 线程池过载异常 这段代码如果直接运行将会抛出异常 总结一下： 其实这个问题是加入的线程数量已经超过了整个线程池能负载的最大数量了（新建线程池的时候使用了有界队列），所以抛出了了异常 避免线程池溢出异常 - 使用无界队列和有界队列 BlockingQueue 这个是为了解决java并发同步问题的，本质上是解决线程间消息同步而设计的 有一下几个类： 1.DelayQueue, （延期阻塞队列）（阻塞队列实现了BlockingQueue接口） 这个队列是无界的，并且没有指定长度的构造方法2.ArrayBlockingQueue, （基于数组的并发阻塞队列） 必须设置长度3.LinkedBlockingQueue, （基于链表的FIFO阻塞队列） 没有指定长度就是有界的反之是有界的4.LinkedBlockingDeque, （基于链表的FIFO双端阻塞队列） 没有指定长度就是有界的反之是有界的5.PriorityBlockingQueue, （带优先级的无界阻塞队列） 这个只能传入Comperable接口的类新，不是有界的6.SynchronousQueue （并发同步阻塞队列）不能指定长度，只能传入一个值，有界 回过来看看上面的源码，其实线程池在加入线程时候的逻辑是这样的 构建常驻线程coreNum指定，如果超过，建立临时线程maxNum , 还不够，增加到队列中 线程池判断能不能增加仅对列是使用的队列的offset方法 如果是有界的队列党对列满了自然返回false，因为添加不进去了，然后就会抛出异常 总结一下我们在使用java线程池的时候需要做好容量规划，如果无法确定是否超出了指定的线程数量，可以使用无界队列，但是要注意到防止内存泄漏","date":"2024-12-31","categories":["java"]},{"title":"java-关于system.gc()与finalize()方法","url":"/2024/12/31/java-关于system-gc--与finalize--方法/","content":"1. finalize的作用 finalize()是Object的protected方法，子类可以覆盖该方法以实现资源清理工作，GC在回收对象之前调用该方法。 finalize()与C++中的析构函数不是对应的。C++中的析构函数调用的时机是确定的（对象离开作用域或delete掉），但Java中的finalize的调用具有不确定性 不建议用finalize方法完成“非内存资源”的清理工作，但建议用于：① 清理本地对象(通过JNI创建的对象)；② 作为确保某些非内存资源(如Socket、文件等)释放的一个补充：在finalize方法中显式调用其他资源释放方法。其原因可见下文[finalize的问题] 2. finalize的问题 一些与finalize相关的方法，由于一些致命的缺陷，已经被废弃了，如System.runFinalizersOnExit()方法、Runtime.runFinalizersOnExit()方法System.gc()与System.runFinalization()方法增加了finalize方法执行的机会，但不可盲目依赖它们 Java语言规范并不保证finalize方法会被及时地执行、而且根本不会保证它们会被执行finalize方法可能会带来性能问题。因为JVM通常在单独的低优先级线程中完成finalize的执行对象再生问题：finalize方法中，可将待回收对象赋值给GC Roots可达的对象引用，从而达到对象再生的目的 finalize方法至多由GC执行一次且只执行一次(用户当然可以手动调用对象的finalize方法，但并不影响GC对finalize的行为) 3. finalize的执行过程(生命周期)描述一下finalize流程： 当对象变成(GC Roots)不可达时，GC会判断该对象是否覆盖了finalize方法，若未覆盖，则直接将其回收。否则，若对象未执行过finalize方法，将其放入F-Queue队列，由一低优先级线程执行该队列中对象的finalize方法。 执行finalize方法完毕后，GC会再次判断该对象是否可达，若不可达，则进行回收，否则，对象“复活”。 具体的finalize流程： 对象可由两种状态控制，涉及到两类状态空间，一是终结状态空间 F &#x3D; {unfinalized, finalizable, finalized}；二是可达状态空间 R &#x3D; {reachable, finalizer-reachable, unreachable}。 各状态含义如下： unfinalized: 新建对象会先进入此状态，GC并未准备执行其finalize方法，因为该对象是可达的 finalizable: 表示GC可对该对象执行finalize方法，GC已检测到该对象不可达。正如前面所述，GC通过F-Queue队列和一专用线程完成finalize的执行 finalized: 表示GC已经对该对象执行过finalize方法 reachable: 表示GC Roots引用可达 finalizer-reachable(f-reachable)：表示不是reachable，但可通过某个finalizable对象可达 unreachable：对象不可通过上面两种途径可达 状态变迁图： 变迁说明： 新建对象首先处于[reachable, unfinalized]状态(A) 随着程序的运行，一些引用关系会消失，导致状态变迁，从reachable状态变迁到f-reachable(B, C, D)或unreachable(E, F)状态 若JVM检测到处于unfinalized状态的对象变成f-reachable或unreachable，JVM会将其标记为finalizable状态(G,H)。若对象原处于[unreachable, unfinalized]状态，则同时将其标记为f-reachable(H)。 在某个时刻，JVM取出某个finalizable对象，将其标记为finalized并在某个线程中执行其finalize方法。由于是在活动线程中引用了该对象，该对象将变迁到(reachable, finalized)状态(K或J)。该动作将影响某些其他对象从f-reachable状态重新回到reachable状态(L, M, N) 处于finalizable状态的对象不能同时是unreachable的，由第4点可知，将对象finalizable对象标记为finalized时会由某个线程执行该对象的finalize方法，致使其变成reachable。这也是图中只有八个状态点的原因 程序员手动调用finalize方法并不会影响到上述内部标记的变化，因此JVM只会至多调用finalize一次，即使该对象“复活”也是如此。程序员手动调用多少次不影响JVM的行为 若JVM检测到finalized状态的对象变成unreachable，回收其内存(I) 若对象并未覆盖finalize方法，JVM会进行优化，直接回收对象（O） 注：System.runFinalizersOnExit()等方法可以使对象即使处于reachable状态，JVM仍对其执行finalize方法 意思的示例代码 对象复活 注意：之所以只能进行一次的原因是对象的两个状态已经改变了不能改变回来了 jdk内部一些包就是使用这中方法进行资源的释放的 覆盖finalize方法以确保资源释放 作为一个补充操作，以防用户忘记“关闭“资源，JDK中FileInputStream、FileOutputStream、Connection类均用了此”技术“，下面代码摘自FileInputStream类","date":"2024-12-31","categories":["java"]},{"title":"java-四种引用的差别和使用场景","url":"/2024/12/31/java-四种引用的差别和使用场景/","content":"一.了解 强引用、软引用、弱引用、虚引用的概念在Java中，虽然不需要程序员手动去管理对象的生命周期，但是如果希望某些对象具备一定的生命周期的话（比如内存不足时JVM就会自动回收某些对象从而避免OutOfMemory的错误）就需要用到软引用和弱引用了。 从Java SE2开始，就提供了四种类型的引用：强引用、软引用、弱引用和虚引用。Java中提供这四种引用类型主要有两个目的：第一是可以让程序员通过代码的方式决定某些对象的生命周期；第二是有利于JVM进行垃圾回收。下面来阐述一下这四种类型引用的概念： 1.强引用（StrongReference）强引用就是指在程序代码之中普遍存在的，比如下面这段代码中的object和str都是强引用： 只要某个对象有强引用与之关联，JVM必定不会回收这个对象，即使在内存不足的情况下，JVM宁愿抛出OutOfMemory错误也不会回收这种对象。比如下面这段代码： 当运行至Object[] objArr &#x3D; new Object[1000];这句时，如果内存不足，JVM会抛出OOM错误也不会回收object指向的对象。不过要注意的是，当fun1运行完之后，object和objArr都已经不存在了，所以它们指向的对象都会被JVM回收。 如果想中断强引用和某个对象之间的关联，可以显示地将引用赋值为null，这样一来的话，JVM在合适的时间就会回收该对象。 比如Vector类的clear方法中就是通过将引用赋值为null来实现清理工作的： 2.软引用（SoftReference）软引用是用来描述一些有用但并不是必需的对象，在Java中用java.lang.ref.SoftReference类来表示。对于软引用关联着的对象，只有在内存不足的时候JVM才会回收该对象。因此，这一点可以很好地用来解决OOM的问题，并且这个特性很适合用来实现缓存：比如网页缓存、图片缓存等。 软引用可以和一个引用队列（ReferenceQueue）联合使用，如果软引用所引用的对象被JVM回收，这个软引用就会被加入到与之关联的引用队列中。下面是一个使用示例： 3.弱引用（WeakReference）弱引用也是用来描述非必需对象的，当JVM进行垃圾回收时，无论内存是否充足，都会回收被弱引用关联的对象。在java中，用java.lang.ref.WeakReference类来表示。下面是使用示例： 输出结果为： 第二个输出结果是null，这说明只要JVM进行垃圾回收，被弱引用关联的对象必定会被回收掉。不过要注意的是，这里所说的被弱引用关联的对象是指只有弱引用与之关联，如果存在强引用同时与之关联，则进行垃圾回收时也不会回收该对象（软引用也是如此）。 弱引用可以和一个引用队列（ReferenceQueue）联合使用，如果弱引用所引用的对象被JVM回收，这个软引用就会被加入到与之关联的引用队列中。 4.虚引用（PhantomReference）虚引用和前面的软引用、弱引用不同，它并不影响对象的生命周期。在java中用java.lang.ref.PhantomReference类表示。如果一个对象与虚引用关联，则跟没有引用与之关联一样，在任何时候都可能被垃圾回收器回收。 要注意的是，虚引用必须和引用队列关联使用，当垃圾回收器准备回收一个对象时，如果发现它还有虚引用，就会把这个虚引用加入到与之 关联的引用队列中。程序可以通过判断引用队列中是否已经加入了虚引用，来了解被引用的对象是否将要被垃圾回收。如果程序发现某个虚引用已经被加入到引用队列，那么就可以在所引用的对象的内存被回收之前采取必要的行动。 二.进一步理解软引用和弱引用对于强引用，我们平时在编写代码时经常会用到。而对于其他三种类型的引用，使用得最多的就是软引用和弱引用，这2种既有相似之处又有区别。它们都是用来描述非必需对象的，但是被软引用关联的对象只有在内存不足时才会被回收，而被弱引用关联的对象在JVM进行垃圾回收时总会被回收。 在SoftReference类中，有三个方法，两个构造方法和一个get方法（WekReference类似）： 两个构造方法： get方法用来获取与软引用关联的对象的引用，如果该对象被回收了，则返回null。 在使用软引用和弱引用的时候，我们可以显示地通过System.gc()来通知JVM进行垃圾回收，但是要注意的是，虽然发出了通知，JVM不一定会立刻执行，也就是说这句是无法确保此时JVM一定会进行垃圾回收的。 三.如何利用软引用和弱引用解决OOM问题前面讲了关于软引用和弱引用相关的基础知识，那么到底如何利用它们来优化程序性能，从而避免OOM的问题呢？ 下面举个例子，假如有一个应用需要读取大量的本地图片，如果每次读取图片都从硬盘读取，则会严重影响性能，但是如果全部加载到内存当中，又有可能造成内存溢出，此时使用软引用可以解决这个问题。 设计思路是：用一个HashMap来保存图片的路径 和 相应图片对象关联的软引用之间的映射关系，在内存不足时，JVM会自动回收这些缓存图片对象所占用的空间，从而有效地避免了OOM的问题。在Android开发中对于大量图片下载会经常用到。","date":"2024-12-31","categories":["java"]},{"title":"java-在文件中追加数据的方法","url":"/2024/12/31/java-在文件中追加数据的方法/","content":"","date":"2024-12-31","categories":["java"]},{"title":"java-异常处理-更优雅的关闭资源","url":"/2024/12/31/java-异常处理-更优雅的关闭资源/","content":"传统的资源关闭方式为了确保外部资源一定要被关闭，通常关闭代码被写入finally代码块中，当然我们还必须注意到关闭资源时可能抛出的异常，于是变有了下面的经典代码： 这种方法提供统一的入口将相关的资源关闭功能如c++的析构函数等，不符合面向对象编程的方法 JDK7及其之后的资源关闭方式 try-with-resource语法 确实，在JDK7以前，Java没有自动关闭外部资源的语法特性，直到JDK7中新增了try-with-resource语法，才实现了这一功能,就是当一个外部资源的句柄对象（比如FileInputStream对象）实现了AutoCloseable接口，那么就可以将上面的板式代码简化为如下形式： 将外部资源的句柄对象的创建放在try关键字后面的括号中，当这个try-catch代码块执行完毕后，Java会确保外部资源的close方法被调用。代码是不是瞬间简洁许多！ 实现原理try-with-resource并不是JVM虚拟机的新增功能，只是JDK实现了一个语法糖，当你将上面代码反编译后会发现，其实对JVM虚拟机而言，它看到的依然是之前的写法： 引申: 异常抑制 通过反编译的代码，大家可能注意到代码中有一处对异常的特殊处理： 这是try-with-resource语法涉及的另外一个知识点，叫做异常抑制。当对外部资源进行处理（例如读或写）时，如果遭遇了异常，且在随后的关闭外部资源过程中，又遭遇了异常，那么你catch到的将会是对外部资源进行处理时遭遇的异常，关闭资源时遭遇的异常将被“抑制”但不是丢弃，通过异常的getSuppressed方法，可以提取出被抑制的异常。 总结 当一个外部资源的句柄对象实现了AutoCloseable接口，JDK7中便可以利用try-with-resource语法更优雅的关闭资源，消除板式代码。 try-with-resource时，如果对外部资源的处理和对外部资源的关闭均遭遇了异常，“关闭异常”将被抑制，“处理异常”将被抛出，但“关闭异常”并没有丢失，而是存放在“处理异常”的被抑制的异常列表中。","date":"2024-12-31","categories":["java"]},{"title":"java-异常处理-自定义异常","url":"/2024/12/31/java-异常处理-自定义异常/","content":"学习java感觉，java的异常处理机制异常的强大，为程序的编写和调试提供的方便快捷的错误提示功能，可以节省大量的时间。系统内置的异常就不记录了，这里记录一下怎样自定义异常 一.创建异常类 继承Exception类并且是想相关的有参构造函数 二.使用自定义的异常类 使用throw 抛出自定义的异常 三.在其他类中进行监听抛出的异常 异常类中的.printStackTrace();将会打印出异常堆栈 .getMessage();将会打印出异常描述也就是自己定义的类中传入的字符串 四.总结1.自定义异常 2.标识可能抛出的异常（用在方法上面） 3.捕获异常 4.异常的解释 调用异常类的相关方法 五.引申 java7 之后try-catch 语句将会自动的关闭相关的资源,前提是相关类必须实现AutoCloseable或者Closeable接口和实现其中的close()方法本质上是一种观察者模式","date":"2024-12-31","categories":["java"]},{"title":"java-异常处理-详解","url":"/2024/12/31/java-异常处理-详解/","content":"异常的体系结构Java把异常当作对象来处理，并定义一个基类java.lang.Throwable作为所有异常的超类。 在Java API中已经定义了许多异常类，这些异常类分为两大类，错误Error和异常Exception。 Java异常层次结构图如下图所示： 从图中可以看出所有异常类型都是内置类Throwable的子类，因而Throwable在异常类的层次结构的顶层。 接下来Throwable分成了两个不同的分支，一个分支是Error，它表示不希望被程序捕获或者是程序无法处理的错误。另一个分支是Exception，它表示用户程序可能捕捉的异常情况或者说是程序可以处理的异常。其中异常类Exception又分为运行时异常(RuntimeException)和非运行时异常。 Java异常又可以分为不受检查异常（Unchecked Exception）和检查异常（Checked Exception）。 下面将详细讲述这些异常之间的区别与联系： Error：Error类对象由 Java 虚拟机生成并抛出，大多数错误与代码编写者所执行的操作无关。如Java虚拟机运行错误（Virtual MachineError），内存不足错误OutOfMemoryError。这些异常发生时，Java虚拟机（JVM）一般会选择线程终止；还有发生在虚拟机试图执行应用时，如类定义错误（NoClassDefFoundError）、链接错误（LinkageError）。这些错误是不可查的，因为它们在应用程序的控制和处理能力之外，而且绝大多数是程序运行时不允许出现的状况。对于设计合理的应用程序来说，即使确实发生了错误，本质上也不应该试图去处理它所引起的异常状况。在Java中，错误通常是使用Error的子类描述。 Exception：在Exception分支中有一个重要的子类RuntimeException（运行时异常），该类型的异常自动为你所编写的程序定义ArrayIndexOutOfBoundsException（数组下标越界）、NullPointerException（空指针异常）、ArithmeticException（算术异常）、MissingResourceException（丢失资源）、ClassNotFoundException（找不到类）等异常，这些异常是不检查异常，程序中可以选择捕获处理，也可以不处理。这些异常一般是由程序逻辑错误引起的，程序应该从逻辑角度尽可能避免这类异常的发生；而RuntimeException之外的异常我们统称为非运行时异常，类型上属于Exception类及其子类，从程序语法角度讲是必须进行处理的异常，如果不处理，程序就不能编译通过。如IOException、SQLException等以及用户自定义的Exception异常，一般情况下不自定义检查异常。 注意：Error和Exception的区别：Error通常是灾难性的致命的错误，是程序无法控制和处理的，当出现这些异常时，Java虚拟机（JVM）一般会选择终止线程；Exception通常情况下是可以被程序处理的，并且在程序中应该尽可能的去处理这些异常。 除了RuntimeException及其子类以外，其他的Exception类及其子类都属于检查异常，当程序中可能出现这类异常，要么使用try-catch语句进行捕获，要么用throws子句抛出，否则编译无法通过。 不受检查异常：包括RuntimeException及其子类和Error。 异常抑制(看下面这段代码) 大家可能注意到代码中有一处对异常的特殊处理： 这是try-with-resource语法涉及的另外一个知识点，叫做异常抑制。当对外部资源进行处理（例如读或写）时，如果遭遇了异常，且在随后的关闭外部资源过程中，又遭遇了异常，那么你catch到的将会是对外部资源进行处理时遭遇的异常，关闭资源时遭遇的异常将被“抑制”但不是丢弃，通过异常的getSuppressed方法，可以提取出被抑制的异常。 java try catch finally 基本异常模型 finally 中如果有return 将会发生什么程序将返回finally中的返回值，如果存在finally代码块，try中的return语句不会立马返回调用者，而是记录下返回值待finally代码块执行完毕之后再向调用者返回其值，然后如果在finally中修改了返回值，就会返回修改后的值。","date":"2024-12-31","categories":["java"]},{"title":"java-时间处理夏令时冬令时跨时区问题处理","url":"/2024/12/31/java-时间处理夏令时冬令时跨时区问题处理/","content":"作为一个成熟的编程语言，java自然有一堆方法来解决的时间的问题。有的时候我们会因为对java一些内置的api不是太熟悉，对某个场景不熟悉，导致遇到了棘手的问题。比如这个场景夏令时和冬令时 夏令时和冬令时在做全球性的功能时绝对少不了遇到时区转化，一般情况下使用时间戳+java内置的api就能解决99%的问题，但是如果遇到夏令时或者冬令时的时候这个问题就可能变得不是这么容易。 首先记录一下什么是夏令时和冬令时：简单的说在这个世界上的某些国家会规定在某个日期将本国所在的时区发生改变，然后在某个时间将他改回来，进行改变的日期就是夏令时或者冬令时 注意：这个概念深层次的东西可以自行用搜索引擎查找相关内容，我这里没有用其他人的那种解释比如夏令时就是把表调快一小时，而是使用修正时区这个概念，这么做是为了方便做下面的解释 夏令时和冬令时产生的业务场景举个例子： 一个实行了夏令时和冬令时的国家在夏令有一个活动，每天11点到1一点参加，为期七天，而这7天正好过了令时变化的这一天， 这样会导致什么问题呢？ 因为跨过令时所以跨令时之前一天的12点20分和后一天的12点20分之间相隔的并不是24小时，因为令时的变化携带的时区的变化，因为时区变化了，所以相同的12点20分对应的毫秒数是不同的（毫秒没有时区问题）所以为期七天这个过程不能简单使用+24小时来处理了，因为这样就可能导致跨令时前是11点到1点，跨令时之后就是12点到2点了 怎么解决java提供了一个非常牛逼的api TimeZone ，专门用来处理时区问题 有两个api 这样就很简单了,使用这两个api如果返回的值不相等,就说明当前时间处于某一个令时中 进阶一下,解决一下上面的需求,跨令时的时候保证日期是+1的解释一下这个需求, 直接用一个最基本的例子来说 我用西班牙国家的时区来做一个+24小时的天数迭代,理论上应该让2019-10-26 4点和6点,变成2019-10-27 4点和6点但是应为夏令时的问题,其实变成的是成2019-10-27 3点和5点 跑一下下面的代码 输出: 所以不能这么做,直接上代码,其实就是一个很简单的算法题了 上面的算法简单的说就是补偿时差,当前的时间和之后的进行比较,如果有差别就+上差距就行了 , 这样就能保证在跨夏令时的时候保证时间统一","date":"2024-12-31","categories":["java"]},{"title":"java-枚举类型","url":"/2024/12/31/java-枚举类型/","content":"枚举类型在java世界使用enum类型的场景是非常多的，但是相比较java中其他的类型这个类型使用的时候还是比较少的这里记录一下 枚举类型创建 枚举类型的创建和传统的类的实现是相同的，不过要注意枚举类型拥有一个高级的用法用法就是使用values对枚举类型每部的属性进行迭代，和使用valueof 获得指定的枚举对象 使用switch 判断Enum对象 注意这里使用的getPositionValue 方法 因为POSTOPN相当一个声明和String类似，所以不能直接的使用sqlPostion来作为switch的标记，需要使用实现类来标记，所以使用switch的写法要这样写","date":"2024-12-31","categories":["java"]},{"title":"java-网络-bio网络编程","url":"/2024/12/31/java-网络-bio网络编程/","content":"java的网络编程是一个硬茬 这里记录一下java 阻塞式网络编程的正确姿势 编写一个服务端程序 client端程序 注意这种底层网络编程的坑点 java 的 tcp sockt 实现中一个socket和一个socket 一一对应 要注意tcp读取的字节数量的问题,比如使用buffer的时候readLine 就是读取一个换行回车的数据,如果没有找到对应的标识符将不会返回的","date":"2024-12-31","categories":["java"]},{"title":"java-网络-nio-nio解析","url":"/2024/12/31/java-网络-nio-nio解析/","content":"java nio支持的背后unix系统的五种IO模型(1) 阻塞I&#x2F;O模型 非常好理解最简单的IO模型 (2) 非阻塞I&#x2F;O模型 其实本质上就是轮训来查找是否有可以进行读取的资源 (3) I&#x2F;O复用模型 这个是select&#x2F;poll,linux 的epoll是进化版,基于事件驱动模型性能更好 (4) 信号驱动I&#x2F;O模型 (5) 异步I&#x2F;O模型 这个就是AIO 全信号驱动模型 java 几种io的对比 java 集中io模型样例BIO 模型java 的同步阻塞模型是一对一的线程模型最大的缺点就是缺少弹性,单用户量增加的时候,系统的线程数出于一种线性的增加状态中 服务端 服务端使用线程池 客户端 这种阻塞式网络io模型的缺点和优点 缺点: 线程占用的比较多 如果处理多个请求一个请求阻塞了,会导致所有的请求都阻塞掉 java NIOps java nio比较难 , 里面有很多的点需要处理","date":"2024-12-31","categories":["java"]},{"title":"java-网路-nio-从源代码分析javaNIO的运行过程(一)","url":"/2024/12/31/java-网路-nio-从源代码分析javaNIO的运行过程-一-/","content":"这一节解析一下 java NIO中 ServerSocketChannel创建的过程 channel 在java中表示通道，是用来进行数据的有效传输 广义上来说通道可以被分为两类：File I&#x2F;O和Stream I&#x2F;O，也就是文件通道和套接字通道 如果分的更细致一点则是： FileChannel 从文件读写数据 SocketChannel 通过TCP读写网络数据 ServerSocketChannel 可以监听新进来的TCP连接，并对每个链接创建对应的SocketChannel DatagramChannel 通过UDP读写网络中的数据 Pipe 通道既可以是单向的也可以是双向的。只实现ReadableByteChannel接口中的read()方法或者只实现WriteableByteChannel接口中的write()方法的通道皆为单向通道，同时实现ReadableByteChannel和WriteableByteChannel为双向通道，比如ByteChannel。对于socket通道来说，它们一直是双向的，而对于FileChannel来说，它同样实现了ByteChannel，但是我们知道通过FileInputStream的getChannel（）获取的FileChannel只具有文件的只读权限，那此时的在该通道调用write（）会出现什么情况？不出意外的抛出了NonWriteChannelException异常。通过以上，我们得出结论：通道都与特定的I&#x2F;O服务挂钩，并且通道的性能受限于所连接的I&#x2F;O服务的性质。 通道的工作模式有两种：阻塞或非阻塞。在非阻塞模式下，调用的线程不会休眠，请求的操作会立刻返回结果；在阻塞模式下，调用的线程会产生休眠。另外除FileChannel不能运行在非阻塞模式下，其余的通道都可阻塞运行也可以以非阻塞的方式运行。 另外从SelectableChannel引申出的类可以和支持有条件选择的Selector结合使用，进而充分利用多路复用的I&#x2F;O（multiplexed I&#x2F;O）来提高性能.对于Socket通道类来说，通常与Selector共同使用以提高性能。需要注意的是通道不能被重复使用，一个打开的通道代表着与一个特定I&#x2F;O服务进行连接并封装了该连接的状态，通道一旦关闭，该连接便会断开。通道的close（）比较特殊，无论在通道时在阻塞模式下还是非阻塞模式下，由于close（）方法的调用而导致底层I&#x2F;O的关闭都可能会造成线程的暂时阻塞。在一个已关闭的通道上调用close（）并没有任何意义，只会立即返回。 现在进入正题，讨论一下ServerSocketChannelJava NIO中的 ServerSocketChannel 是一个可以监听新进来的TCP连接的通道, 类似ServerSocket一样。要注意的是和DatagramChannel和SocketChannel不同，ServerSocketChannel本身不具备传输数据的能力，而只是负责监听传入的连接和创建新的SocketChannel。 创建一个ServerSocket很简单，代码如下 深入研究一下， 进入ServerSocketChannel的open方法 ServerSocketChannel类 open方法 这里首先将会调用SelectorProvider的provider方法,这一局将会返回一个SelectorProvider实例,主要作用就是使用spi或者-Dxxx参数传入的SelectorProvider的类url地址（类的包名加类名）返回一个imp实例 SelectorProvider类 provider() 方法 然后会调用实例的 openServerSocketChannel()方法 这个方法将会真正的返回一个ServerSocketChannelImpl实例，这个类一般就是由虚拟机的厂商提供的，这里就不进行深入的研究了 最后看一下这个serversocketchannel个人觉得有用的方法 public static ServerSocketChannel open() throws IOException 开启一个serversocketchannel **public final int validOps()**：返回这个通道支持的SelectionKey值，应为这个通道只是开启一个SocketChannel并没有信息交换的功能，只是监听功能，所有只能返回SelectionKey.OP_ACCEPT **public final ServerSocketChannel bind(SocketAddress local)和public final ServerSocketChannel bind(SocketAddress local，int backlog)**：字如其意将一个套接字和本地地址链接，有一个重构方法提供最大链接数支持 **public ServerSocket socket()**：返回一个ServerSocket public SocketChannel accept() throws IOException：和传统的网络编程相同，如果使用的Nio那么将会立即的返回null否则将会阻塞住直到有链接建立 public SocketAddress getLocalAddress() throws IOException：返回这里绑定的地址 public final SelectableChannel configureBlocking(boolean block) throws IOException: 指定这个接口是阻塞的还是非阻塞的","date":"2024-12-31","categories":["java"]},{"title":"java-资源获取-文件系统中-jar包中-jar包中的jar中","url":"/2024/12/31/java-资源获取-文件系统中-jar包中-jar包中的jar中/","content":"一开始其实用这个题目作为文章的标题很难受，但是没有办法java的文件获取就是这么负载:cold_sweat: java获取数据获取文件系统的api这个就不罗嗦了，就是java的文件操作FileInputStream等等 java如果获取jar中的文件怎么做java 有一个特殊的api的ClassLoader.getResource()与getResources() 简单的说这个api是干嘛的,加载类路径资源文件,也就是classPath中的东西（也就是可以拿到classpath中的所有东西包括jar中的东西） 直接上源码 注意代码这一段Enumeration urls &#x3D; getClassLoader().getResources(packageName.replace(“.”, “&#x2F;“)); 返回值石URL类型，注意在java中这个url其实有多种协议构成的，file前缀表示在classpath文件路径中的资源，而jar前缀表示在jar中的资源 如果是file前缀的资源 这种情况下就当成基本的文件数据流来处理就好了其实很简单 如果石jar前缀的资源 这个地方要注意一下，有一个坑，就是如果jvm发现这个地方有我们需要的资源我们需要通过JarURLConnection jarURLConnection &#x3D; (JarURLConnection) url.openConnection();这个api来获取资源的引用，但是呢这个引用会将这个jar中所有的文件都加载进来，所以在使用的时候需要做一个过滤。 总结一下上面的代码已经实现了一个java jar中文件的搜索，网络上有类似的代码，但是实际情况是有一些bug的，这里我做了一些修改（因为我们传入的是指定的包，但是jar加载会把整个jar文件全部加载进来，所以我在里面加了一个过滤操作），现在已经可以稳定运行了。","date":"2024-12-31","categories":["java"]},{"title":"java-重载-重写和桥接方法","url":"/2024/12/31/java-重载-重写和桥接方法/","content":"今天在进行 java 反射库的封装过程中,发现了一个问题,之前对java重载和重写的机制认识并不是很深入.这里决定深入的整理一下 重载 定义 一个类中具有多个重名的方法,如果父子类满足条件也能构成重载方法 特点 方法名称相同 参数列表不许不同 个数不同 个数相同但是类型不同(包不包括父子关系) 返回值,访问修饰符号可以相同也可以不相同 注意:这一点没有考虑到如果是继承关系,两个看似重载的父子关系是否实际上构成重载关系 重写 定义 子类重写父类的一个方法 特点 子类继承父类 返回值名称参数不能改变(子类的参数,返回值可以是父类对应的参数的子类) 限制(不能进行重写的方法) private修饰的方法 构造函数 final,static 方法(static 将会被再次声明) 重写方法不能缩小父方法的权限 重写方法不能抛出任何新的强制性异常或者比父类方法范围广的强制性异常(补充出列RuntimeException之外都是强制异常) 重载和重写的对比 重载中产生的桥接方法重载中可能有这样一种情况:重写方法的返回类型是其父类返回类型的子类型 在这种情况下jvm将会自动的生成桥接方法来作为子类和父类调用之间的桥梁 比如这样的两个类,NaiveMerchant类继承自Merchant,重载了actionPrice方法,并且Double类型是Number类型的子类 这样在jvm编译过程中将会自动生成一个中间方法 注意这种情况只是针对,使用父类调用子类重写的逻辑这种情况 引申一下: 其实在泛型的使用中,同样会出现这种情况","date":"2024-12-31","categories":["java"]},{"title":"java-领域模型(po,vo等)","url":"/2024/12/31/java-领域模型-po-vo等-/","content":"这一块的东西是今天看apollo框架的时候，发现了，其实是一个javabean 的封装，但是在一定程度上又有一些不同，这里详细的介绍一下。 PO(persistant object) 持久对象 在 o&#x2F;r 映射的时候出现的概念，如果没有 o&#x2F;r 映射，没有这个概念存在了。通常对应数据模型 ( 数据库 ), 本身还有部分业务逻辑的处理。可以看成是与数据库中的表相映射的 java 对象。最简单的 PO 就是对应数据库中某个表中的一条记录，多个记录可以用 PO 的集合。 PO 中应该不包含任何对数据库的操作。 DO（Domain Object）领域对象 就是从现实世界中抽象出来的有形或无形的业务实体。一般和数据中的表结构对应。 TO(Transfer Object) ，数据传输对象 在应用程序不同 tie( 关系 ) 之间传输的对象 DTO（Data Transfer Object）数据传输对象 这个概念来源于J2EE的设计模式，原来的目的是为了EJB的分布式应用提供粗粒度的数据实体，以减少分布式调用的次数，从而提高分布式调用的性能和降低网络负载，但在这里，我泛指用于展示层与服务层之间的数据传输对象。 VO(view object) 值对象 视图对象，用于展示层，它的作用是把某个指定页面（或组件）的所有数据封装起来。 BO(business object) 业务对象 从业务模型的角度看 , 见 UML 元件领域模型中的领域对象。封装业务逻辑的 java 对象 , 通过调用 DAO 方法 , 结合 PO,VO 进行业务操作。 business object: 业务对象 主要作用是把业务逻辑封装为一个对象。这个对象可以包括一个或多个其它的对象。 比如一个简历，有教育经历、工作经历、社会关系等等。 我们可以把教育经历对应一个 PO ，工作经历对应一个 PO ，社会关系对应一个 PO 。 建立一个对应简历的 BO 对象处理简历，每个 BO 包含这些 PO 。 这样处理业务逻辑时，我们就可以针对 BO 去处理。 POJO(plain ordinary java object) 简单无规则 java 对象 纯的传统意义的 java 对象。就是说在一些 Object&#x2F;Relation Mapping 工具中，能够做到维护数据库表记录的 persisent object 完全是一个符合 Java Bean 规范的纯 Java 对象，没有增加别的属性和方法。我的理解就是最基本的 Java Bean ，只有属性字段及 setter 和 getter 方法！。 DAO(data access object) 数据访问对象 是一个 sun 的一个标准 j2ee 设计模式， 这个模式中有个接口就是 DAO ，它负持久层的操作。为业务层提供接口。此对象用于访问数据库。通常和 PO 结合使用， DAO 中包含了各种数据库的操作方法。通过它的方法 , 结合 PO 对数据库进行相关的操作。夹在业务逻辑与数据库资源中间。配合 VO, 提供数据库的 CRUD 操作 O&#x2F;R Mapper 对象 &#x2F; 关系 映射 定义好所有的 mapping 之后，这个 O&#x2F;RMapper 可以帮我们做很多的工作。通过这些 mappings, 这个 O&#x2F;RMapper 可以生成所有的关于对象保存，删除，读取的 SQL 语句，我们不再需要写那么多行的 DAL 代码了。本质上就是数据表到领域bean的对应关系","date":"2024-12-31","categories":["java"]},{"title":"java-多线程编程1-基础","url":"/2024/12/31/java-多线程编程1-基础/","content":"java的jvm虚拟机将线程技术发扬光大了 线程的实现方式实现线程主要有3种方式:使用内核线程实现、使用用户线程实现和使用用户线程加轻量级进程混合实现。 内核线程(Kernel-Level Thread,KLT)就是直接由操作系统内核(Kernel,下称内核)支持的线程,这种线程由内核来完成线程切换,内核通过操纵调度器(Scheduler)对线程进行调度,并负责将线程的任务映射到各个处理器上。 程序一般不会直接去使用内核线程,而是去使用内核线程的一种高级接口——轻量级进程(Light Weight Process,LWP),轻量级进程就是我们通常意义上所讲的线程,由于每个轻量级进程都由一个内核线程支持,因此只有先支持内核线程,才能有轻量级进程，对应关系如下：（p:用户，LWP：轻量级线程，KLT：内核线程，Thread Scheduer：线程池） 使用内核线程模式的局限性：基于内核线程实现的,所以各种线程操作,如创建、析构及同步,都需要进行系统调用。而系统调用的代价相对较高,需要在用户态(User Mode)和内核态(KernelMode)中来回切换。其次,每个轻量级进程都需要有一个内核线程的支持,因此轻量级进程要消耗一定的内核资源(如内核线程的栈空间),因此一个系统支持轻量级进程的数量是有限 使用用户线程实现通俗来说就是使用建立在用户空间的线程库，而不是使用系统内部的内核机制 使用用户线程的优势在于不需要系统内核支援,劣势也在于没有系统内核的支援,所有的线程操作都需要用户程序自己处理。线程的创建、切换和调度都是需要考虑的问题,而且由于操作系统只把处理器资源分配到进程,那诸如“阻塞如何处理”、“多处理器系统中如何将线程映射到其他处理器上”这类问题解决起来将会异常困难,甚至不可能完成。 使用用户线程加轻量级进程混合实现在这种混合实现下，用户线程的创建、切换、析构等操作依然廉价,并且可以支持大规模的用户线程并发，操作系统提供支持的轻量级进程则作为用户线程和内核线程之间的桥梁,这样可以使用内核提供的线程调度功能及处理器映射,并且用户线程的系统调用要通过轻量级线程来完成,大大降低了整个进程被完全阻塞的风险 对于Sun JDK来说,它的Windows版与Linux版都是使用一对一的线程模型实现的,一条Java线程就映射到一条轻量级进程之中,因为Windows和Linux系统提供的线程模型就是一对一的 。 java 多线程提供的相关知识java内部线程状态的切换模式 创建线程的方法继承 Thread类或者实现Runnable借口 java线程的一些操作方法 java基础锁机制 使用atomic工具实现数据共享","date":"2024-12-31","categories":["java"],"tags":["java多线程"]},{"title":"java-多线程编程2-并发性问题-底层实现","url":"/2024/12/31/java-多线程编程2-并发性问题-底层实现/","content":"硬件效率和一致性问题由于计算机的存储设备与处理器的运算速度有几个数量级的差距,所以现代计算机系统都不得不加入一层读写速度尽可能接近处理器运算速度的高速缓存(Cache)来作为内存与处理器之间的缓冲:将运算需要使用到的数据复制到缓存中,让运算能快速进行,当运算结束后再从缓存同步回内存之中,这样处理器就无须等待缓慢的内存读写了。 基于高速缓存的存储交互很好地解决了处理器与内存的速度矛盾,但是也为计算机系统带来更高的复杂度,因为它引入了一个新的问题:缓存一致性(Cache Coherence)。在多处理器系统中,每个处理器都有自己的高速缓存,而它们又共享同一主内存(MainMemory) 主内存与工作内存Java内存模型规定了所有的变量都存储在主内存(Main Memory)中(此处的主内存与介绍物理硬件时的主内存名字一样,两者也可以互相类比,但此处仅是虚拟机内存的一部分)。每条线程还有自己的工作内存(Working Memory,可与前面讲的处理器高速缓存类比),线程的工作内存中保存了被该线程使用到的变量的主内存副本拷贝 ,线程对变量的所有操作(读取、赋值等)都必须在工作内存中进行,而不能直接读写主内存中的变量 。 主内存与工作内存之间具体的交互协议,即一个变量如何从主内存拷贝到工作内存、如何从工作内存同步回主内存之类的实现细节,Java内存模型中定义了以下8种操作来完成,虚拟机实现时必须保证下面提及的每一种操作都是原子的、不可再分的 lock(锁定):作用于主内存的变量,它把一个变量标识为一条线程独占的状态。 unlock(解锁):作用于主内存的变量,它把一个处于锁定状态的变量释放出来,释放后的变量才可以被其他线程锁定。 read(读取):作用于主内存的变量,它把一个变量的值从主内存传输到线程的工作内存中,以便随后的load动作使用。 load(载入):作用于工作内存的变量,它把read操作从主内存中得到的变量值放入工作内存的变量副本中。 use(使用):作用于工作内存的变量,它把工作内存中一个变量的值传递给执行引擎,每当虚拟机遇到一个需要使用到变量的值的字节码指令时将会执行这个操作。 assign(赋值):作用于工作内存的变量,它把一个从执行引擎接收到的值赋给工作内存的变量,每当虚拟机遇到一个给变量赋值的字节码指令时执行这个操作。 store(存储):作用于工作内存的变量,它把工作内存中一个变量的值传送到主内存中,以便随后的write操作使用。 write(写入):作用于主内存的变量,它把store操作从工作内存中得到的变量的值放入主内存的变量中。 volatile型变量的特殊规则当一个变量定义为volatile之后,它将具备两种特性,第一是保证此变量对所有线程的可见性,这里的“可见性”是指当一条线程修改了这个变量的值,新值对于其他线程来说是可以立即得知的。而普通变量不能做到这一点,普通变量的值在线程间传递均需要通过主内存来完成,例如,线程A修改一个普通变量的值,然后向主内存进行回写,另外一条线程B在线程A回写完成了之后再从主内存进行读取操作,新变量值才会对线程B可见。 注意这个方法并不能保证原子性，只能保证可见性 预期输出为200000实际并不是，因为java中运算操作并不是原子操作，会有字节码重排序的情况发生 volatile对 long和double型变量的特殊规则 Java内存模型要求lock、unlock、read、load、assign、use、store、write这8个操作都具有原子性,但是对于64位的数据类型(long和double),在模型中特别定义了一条相对宽松的规定:允许虚拟机将没有被volatile修饰的64位数据的读写操作划分为两次32位的操作来进行,即允许虚拟机实现选择可以不保证64位数据类型的load、store、read和write这4个操作的原子性如果有多个线程共享一个并未声明为volatile的long或double类型的变量,并且同时对它们进行读取和修改操作,那么某些线程可能会读取到一个既非原值,也不是其他线程修改值的代表了“半个变量”的数值 注意：目前各种平台下的商用虚拟机几乎都选择把64位数据的读写操作作为原子操作来对待,因此我们在编写代码时一般不需要把用到的long和double变量专门声明为volatile 原子性、可见性与有序性 原子性(Atomicity):由Java内存模型来直接保证的原子性变量操作包括read、load、assign、use、store和write,我们大致可以认为基本数据类型的访问读写是具备原子性的(例外就是long和double的非原子性协定,读者只要知道这件事情就可以了,无须太过在意这些几乎不会发生的例外情况)。 如果应用场景需要一个更大范围的原子性保证(经常会遇到),Java内存模型还提供了lock和unlock操作来满足这种需求,尽管虚拟机未把lock和unlock操作直接开放给用户使用,但是却提供了更高层次的字节码指令monitorenter和monitorexit来隐式地使用这两个操作,这两个字节码指令反映到Java代码中就是同步块——synchronized关键字,因此在synchronized块之间的操作也具备原子性。 可见性(Visibility):可见性是指当一个线程修改了共享变量的值,其他线程能够立即得知这个修改。除了volatile之外,Java还有两个关键字能实现可见性,即synchronized和final。 有序性(Ordering):Java内存模型的有序性在前面讲解volatile时也详细地讨论过了,Java程序中天然的有序性可以总结为一句话:如果在本线程内观察,所有的操作都是有序的;如果在一个线程中观察另一个线程,所有的操作都是无序的。前半句是指“线程内表现为串行的语义”(Within-Thread As-If-Serial Semantics),后半句是指“指令重排序”现象和“工作内存与主内存同步延迟”现象 结论:一个操作“时间上的先发生”不代表这个操作会是“先行发生“ Java与线程对于Sun JDK来说,它的Windows版与Linux版都是使用一对一的线程模型实现的,一条Java线程就映射到一条轻量级进程之中,因为Windows和Linux系统提供的线程模型就是一对一的 Java线程调度线程调度是指系统为线程分配处理器使用权的过程,主要调度方式有两种,分别是协同式线程调度(CooperativeThreads-Scheduling)和抢占式线程调度(PreemptiveThreads-Scheduling) 状态转换 新建(New):创建后尚未启动的线程处于这种状态。 运行(Runable):Runable包括了操作系统线程状态中的Running和Ready,也就是处于此状态的线程有可能正在执行,也有可能正在等待着CPU为它分配执行时间。 无限期等待(Waiting):处于这种状态的线程不会被分配CPU执行时间,它们要等待被其他线程显式地唤醒。以下方法会让线程陷入无限期的等待状态:●没有设置Timeout参数的Object.wait()方法。●没有设置Timeout参数的Thread.join()方法。●LockSupport.park()方法。 限期等待(Timed Waiting):处于这种状态的线程也不会被分配CPU执行时间,不过无须等待被其他线程显式地唤醒,在一定时间之后它们会由系统自动唤醒。以下方法会让线程进入限期等待状态:●Thread.sleep()方法。●设置了Timeout参数的Object.wait()方法。●设置了Timeout参数的Thread.join()方法。●LockSupport.parkNanos()方法。●LockSupport.parkUntil()方法。 阻塞(Blocked):线程被阻塞了,“阻塞状态”与“等待状态”的区别是:“阻塞状态”在等待着获取到一个排他锁,这个事件将在另外一个线程放弃这个锁的时候发生;而“等待状态”则是在等待一段时间,或者唤醒动作的发生。在程序等待进入同步区域的时候,线程将进入这种状态。 结束(Terminated):已终止线程的线程状态,线程已经结束执行。","date":"2024-12-31","categories":["java"],"tags":["java多线程"]},{"title":"java-多线程编程3-线程安全","url":"/2024/12/31/java-多线程编程3-线程安全/","content":"Java语言中的线程安全等级 按照线程安全的“安全程度”由强至弱来排序,我们 可以将Java语言中各种操作共享的数据分为以下5类:不可变、绝对线程安全、相对线程安全、线程兼容和线程对立。 不可变：如final关键字 定义的数据不可修改，可靠性最高 绝对线程安全：绝对的线程安全完全满足Brian Goetz给出的线程安全的定义,这个定义其实是很严格的,一个类要达到“不管运行时环境如何,调用者都不需要任何额外的同步措施”通常需要付出很大的,甚至有时候是不切实际的代价。 相对线程安全：相对的线程安全就是我们通常意义上所讲的线程安全,它需要保证对这个对象单独的操作是线程安全的,我们在调用的时候不需要做额外的保障措施,但是对于一些特定顺序的连续调用,就可能需要在调用端使用额外的同步手段来保证调用的正确性，在Java语言中,大部分的线程安全类都属于这种类型,例如Vector、HashTable、Collections的synchronizedCollection()方法包装的集合等 线程兼容：线程兼容是指对象本身并不是线程安全的,但是可以通过在调用端正确地使用同步手段来保证对象在并发环境中可以安全地使用,我们平常说一个类不是线程安全的,绝大多数时候指的是这一种情况 线程对立：线程对立是指无论调用端是否采取了同步措施,都无法在多线程环境中并发使用的代码。由于Java语言天生就具备多线程特性,线程对立这种排斥多线程的代码是很少出现的,而且通常都是有害的,应当尽量避免 线程同步的实现方法互斥同步：互斥同步(Mutual Exclusion&Synchronization)是常见的一种并发正确性保障手段。同步是指在多个线程并发访问共享数据时,保证共享数据在同一个时刻只被一个(或者是一些,使用信号量的时候)线程使用。而互斥是实现同步的一种手段,临界区(CriticalSection)、互斥量(Mutex)和信号量(Semaphore)都是主要的互斥实现方式。因此,在这4个字里面,互斥是因,同步是果;互斥是方法,同步是目的。 引申：jvm执行monitorenter指令时,首先要尝试获取对象的锁。如果这个对象没被锁定,或者当前线程已经拥有了那个对象的锁,把锁的计数器加1,相应的,在执行monitorexit指令时会将锁计数器减1,当计数器为0时,锁就被释放 java sybcgrinized是重量级锁的原因：Java的线程是映射到操作系统的原生线程之上的,如果要阻塞或唤醒一个线程,都需要操作系统来帮忙完成,这就需要从用户态转换到核心态中,因此状态转换需要耗费很多的处理器时间。对于代码简单的同步块(如被synchronized修饰的getter()或setter()方法),状态转换消耗的时间有可能比用户代码执行的时间还要长。所以synchronized是Java语言中一个重量级(Heavyweight)的操作,有经验的程序员都会在确实必要的情况下才使用这种操作 注意：互斥同步最主要的问题就是进行线程阻塞和唤醒所带来的性能问题,因此这种同步也称为阻塞同步(Blocking Synchronization)：互斥同步属于一种悲观的并发策略,总是认为只要不去做正确的同步措施(例如加锁),那就肯定会出现问题,无论共享数据是否真的会出现竞争,它都要进行加锁(这里讨论的是概念模型,实际上虚拟机会优化掉很大一部分不必要的加锁)、用户态核心态转换、维护锁计数器和检查是否有被阻塞的线程需要唤醒等操作 非阻塞同步:硬件指令集的发展,出现基于冲突检测的乐观并发策略,通俗地说,就是先进行操作,如果没有其他线程争用共享数据,那操作就成功了;如果共享数据有争用,产生了冲突,那就再采取其他的补偿措施(最常见的补偿措施就是不断地重试,直到成功为止),这种乐观的并发策略的许多实现都不需要把线程挂起,因此这种同步操作称为非阻塞同步(Non-Blocking Synchronization) 引申非阻塞的实现CAS：CAS指令需要有3个操作数,分别是内存位置(在Java中可以简单理解为变量的内存地址,用V表示)、旧的预期值(用A表示)和新值(用B表示)。CAS指令执行时,当且仅当V处的值符合旧预期值A时,处理器用新值B更新V处的值,否则它就不执行更新,但是无论是否更新了V处的值,都会返回V的旧值,上述的处理过程是一个原子操作。 JDK1.5之后,Java程序中才可以使用CAS操作：操作由sun.misc.Unsafe类里面的compareAndSwapInt()和compareAndSwapLong()等几个方法包装提供，我们只能通过其他的JavaAPI来间接使用它,如J.U.C包里面的整数原子类,其中的compareAndSet()和getAndIncrement()等方法都使用了Unsafe类的CAS操作 CAS缺点 ABA问题。因为CAS需要在操作值的时候检查下值有没有发生变化，如果没有发生变化则更新，但是如果一个值原来是A，变成了B，又变成了A，那么使用CAS进行检查时会发现它的值没有发生变化，但是实际上却变化了。ABA问题的解决思路就是使用版本号。在变量前面追加上版本号，每次变量更新的时候把版本号加一，那么A－B－A 就会变成1A-2B－3A。JDK的atomic包里提供了一个类AtomicStampedReference来解决ABA问题。这个类的compareAndSet方法作用是首先检查当前引用是否等于预期引用，并且当前标志是否等于预期标志，如果全部相等，则以原子方式将该引用和该标志的值设置为给定的更新值。 循环时间长开销大。自旋CAS如果长时间不成功，会给CPU带来非常大的执行开销。 只能保证一个共享变量的原子操作。当对一个共享变量执行操作时，我们可以使用循环CAS的方式来保证原子操作，但是对多个共享变量操作时，循环CAS就无法保证操作的原子性，这个时候就可以用锁，或者有一个取巧的办法，就是把多个共享变量合并成一个从Java1.5开始JDK提供了AtomicReference类来保证引用对象之间的原子性，你可以把多个变量放在一个对象里来进行CAS操作。共享变量来操作。 无需线程同步方案有些有一些代码天生就是线程安全的,笔者简单地介绍其中的两类：无需同步方案：有些有一些代码天生就是线程安全的,笔者简单地介绍其中的两类 可重入代码(Reentrant Code):不依赖存储在堆上的数据和公用的系统资源、用到的状态量都由参数中传入、不调用非可重入的方法 线程本地存储(Thread Local Storage):线程唯一变量，java中的java.lang.ThreadLocal类来实现线程本地存储的功能 ThreadLocal实现原理 一个线程的Thread对象中都有一个ThreadLocalMap对象,这个对象存储了一组以ThreadLocal.threadLocalHashCode为键,以本地线程变量为值的K-V值对,ThreadLocal对象就是当前线程的ThreadLocalMap的访问入口,每一个ThreadLocal对象都包含了一个独一无二的threadLocalHashCode值,使用这个值就可以在线程K-V值对中找回对应的本地线程变量。","date":"2024-12-31","categories":["java"],"tags":["java多线程"]},{"title":"java-多线程编程4-虚拟机的锁优化技术","url":"/2024/12/31/java-多线程编程4-虚拟机的锁优化技术/","content":"自旋锁与自适应自旋虚拟机的开发团队也注意到在许多应用上,共享数据的锁定状态只会持续很短的一段时间,为了这段时间去挂起和恢复线程并不值得。如果物理机器有一个以上的处理器,能让两个或以上的线程同时并行执行,我们就可以让后面请求锁的那个线程“稍等一下”,但不放弃处理器的执行时间,看看持有锁的线程是否很快就会释放锁。为了让线程等待,我们只需让线程执行一个忙循环(自旋),这项技术就是所谓的自旋锁 注意：如果自旋超过了限定的次数仍然没有成功获得锁,就应当使用传统的方式去挂起线程了。自旋次数的默认值是10次,用户可以使用参数-XX:PreBlockSpin来更改 在JDK 1.6中引入了自适应的自旋锁。自适应意味着自旋的时间不再固定了,而是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定。自旋成功率高就自旋多次，反之就减少 锁消除锁消除是指虚拟机即时编译器在运行时,对一些代码上要求同步,但是被检测到不可能存在共享数据竞争的锁进行消除。锁消除的主要判定依据来源于逃逸分析的数据支持(第11章已经讲解过逃逸分析技术),如果判断在一段代码中,堆上的所有数据都不会逃逸出去从而被其他线程访问到,那就可以把它们当做栈上数据对待,认为它们是线程私有的,同步加锁自然就无须进行 简单总结，就是编辑器分析出这段代码并不需要进行上锁就不进行同步处理 上面代码中每个StringBuffer.append()方法中都有一个同步块,锁就是sb对象。虚拟机观察变量sb,很快就会发现它的动态作用域被限制在concatString()方法内部。也就是说,sb的所有引用永远不会“逃逸”到concatString()方法之外,其他线程无法访问到它,因此,虽然这里有锁,但是可以被安全地消除掉,在即时编译之后,这段代码就会忽略掉所有的同步而直接执行了 锁粗化 如果一系列的连续操作都对同一个对象反复加锁和解锁,甚至加锁操作是出现在循环体中的,那即使没有线程竞争,频繁地进行互斥同步操作也会导致不必要的性能损耗 连续的append()方法就属于这类情况。如果虚拟机探测到有这样一串零碎的操作都对同一个对象加锁,将会把加锁同步的范围扩展(粗化)到整个操作序列的外部,以代码清单13-7为例,就是扩展到第一个append()操作之前直至最后一个append()操作之后,这样只需要加锁一次就可以了 偏向锁和轻量级锁和重量级锁介绍在了解锁的时候首先需要理解java的对象头 java对象头：所有锁的实现都依赖于对象头 锁是存在Java对象头里的。如果对象是数组类型,则虚拟机用3个字宽(Word)存储对象头,如果对象是非数组类型,则用2字宽存储对象头。在32位虚拟机中,1字宽等于4字节,即32bit,如表 Java对象头里的Mark Word里默认存储对象的HashCode、分代年龄和锁标记位。32位JVM的Mark Word的默认存储结构如表 java中锁一共有4种状态,级别从低到高依次是:无锁状态、偏向锁状态、轻量级锁状态和重量级锁状态,这几个状态会随着竞争情况逐渐升级。锁可以升级但不能降级,意味着偏向锁升级成轻量级锁后不能降级成偏向锁。这种锁升级却不能降级的策略,目的是为了提高获得锁和释放锁的效率 偏向锁大多数情况下,锁不仅不存在多线程竞争,而且总是由同一线程多次获得 过程：一个线程访问同步块并获取锁时,会在对象头和栈帧中的锁记录里存储锁偏向的线程ID,以后该线程在进入和退出同步块时不需要进行CAS操作来加锁和解锁,只需简单地测试一下对象头的Mark Word里是否存储着指向当前线程的偏向锁。如果测试成功,表示线程已经获得了锁。如果测试失败,则需要再测试一下Mark Word中偏向锁的标识是否设置成1(表示当前是偏向锁):如果没有设置,则使用CAS竞争锁;如果设置了,则尝试使用CAS将对象头的偏向锁指向当前线程。 轻量级锁 获取锁过程 线程在执行同步块之前,JVM会先在当前线程的栈桢中创建用于存储锁记录的空间,并将对象头中的Mark Word复制到锁记录中,官方称为Displaced Mark Word。然后线程尝试使用CAS将对象头中的Mark Word替换为指向锁记录的指针。如果成功,当前线程获得锁,如果失败,表示其他线程竞争锁,当前线程便尝试使用自旋来获取锁。 解锁过程 轻量级解锁时,会使用原子的CAS操作将Displaced Mark Word替换回到对象头,如果成功,则表示没有竞争发生。如果失败,表示当前锁存在竞争,锁就会膨胀成重量级锁。 因为自旋会消耗CPU,为了避免无用的自旋(比如获得锁的线程被阻塞住了),一旦锁升级成重量级锁,就不会再恢复到轻量级锁状态。当锁处于这个状态下,其他线程试图获取锁时,都会被阻塞住,当持有锁的线程释放锁之后会唤醒这些线程,被唤醒的线程就会进行新一轮的夺锁之争","date":"2024-12-31","categories":["java"],"tags":["java多线程"]},{"title":"java-多线程编程5-重温多线程编程-CAS用法","url":"/2024/12/31/java-多线程编程5-重温多线程编程-CAS用法/","content":"java 在多线线程编程中有个许许多多的优势,其中一个就是java的基于CAS理论而创建的锁ReentrantLock,这里主要是回顾一下CAS是如何实现线程安全的 CAS简介 其实cas 很简单 ,使用的是”比较交换算法”, 核心体现在AtomicReference.compareAndSet(xx,xx2) 上 , 这个函数将会比较传入的xx 和 AtomicReference 中存过的值是否发生变化如果没有发生变化,将会将xx2 的值赋给xx,否则这个函数将会返回false 通过AtomicReference.compareAndSet(xx,xx2) 这种特性我们可是设计一种回滚策略来将实现并发时,公共属性征用的问题 用java 栈来举一个简单的例子 首先定义一个新的java栈对象 接下来编写java的main方法,这里采用的多线的方法,每个线程处理一个变量并且将这个变量加入到公共的列表中 大多数情况下会产生的结果和下面的相同 但是在特殊的情况下会出现多线程的问题 我没修改push方法的这里 这里会出现空指针的问题 这里分析一下这种方法的运行过程 执行顺序 thread-0 thread-1 thread-2 1 Node newHead &#x3D; new Node<>(item); – – 2 head&#x3D;newHead; – – 3 (Resume) – – 4 – Node newHead &#x3D; new Node<>(item); – 5 – – Node newHead &#x3D; new Node<>(item); 6 – newHead.next &#x3D; head; – 7 – – newHead.next &#x3D; head; 8 – head&#x3D;newHead; – 9 – – head&#x3D;newHead; 10 – (Resume) 11 – – (Resume) 异常结果是如何产生的？ 当thread-0执行到顺序3时，head表示的链表为node(1)。 当thread-1执行到顺序10时，head表示的链表为node(2)->node(1)。 当thread-2执行到顺序11时，head表示的链表为node(3)->node(1)。 当三个线程都执行完毕之后，head的最终表示为node(3)->node(1)，也就是说thread-2将thread-1的执行结果覆盖了。 解决办法 synchronized 关键字或者 ReentrantLock 线程锁 这种加锁的方法很简单不多说了, 主要讲讲第二种CAS原子性操作的做法, 上代码 其实就像我开篇说的一样其实核心就是compareAndSet方法校验在争用环境下,我自己获取的值是否发生过修改,如果发生过修改就进行回滚的操作 其实上面的例子中还要注意一个地方就是两个error 错误点就是用cas.get()方法来尽心获取,这样并不可取因为,操作compareAndSet会导致cas.get()方法中的数据发生改变,从而导致线程安全失败","date":"2024-12-31","categories":["java"],"tags":["java多线程"]},{"title":"java版本追朔-java7","url":"/2024/12/31/java版本追朔-java7/","content":"尝试使用资源java7 针对资源使用的改进,通过这种方法,就不需要显示的声明文件关闭的语句了 基本上，是上面的代码在Java 7中等同于下面的Java 6： 但是还有一些不同的,就是使用java7 其实实在catch之前就关闭资源的 java7 捕获多个异常 java7中可以简写为： 符合 可以 写多个，符合其中之一； java7 fork join 和nio2.0这个说过很多了就不补充了 支持jvm 虚拟机中的动态类型语言回到本文的主题，来看看Java语言、虚拟机与动态类型语言之间有什么关系。Java虚拟机毫无疑问是Java语言的运行平台，但他的使命并不仅限于此，早在1997年出版的《虚拟机规范》中就规划了这样一个愿景：“在未来，我们会对Java虚拟机进行适当的扩展，以便更好地支持其他语言运行于Java虚拟机之上”。而目前确实已经有许多动态类型语言运行于Java虚拟机之上了，如Clojure、Groovy、Jython和JRuby等，能够在同一个虚拟机上可以达到静态类型语言的严谨性与动态类型语言的灵活性，这是一件很美妙的事情。但遗憾的是：Java虚拟机层面对动态类型语言的支持一直都有所欠缺，主要表现在方法调用方面：JDK 1.7以前的字节码指令集中，4条方法调用指令（invokevirtual、invokespecial、invokestatic、invokeinterface）的第一个参数都是被调用的方法的符号引用（CONSTANT_Methodref_info或者CONSTANT_InterfaceMethodref_info常量），前面已经提到过，方法的符号引用在编译时产生，而动态类型语言只有在运行期才能确定接收者类型。这样，在Java虚拟机上实现的动态类型语言就不得不使用其他方式（如编译时留个占位符类型，运行时动态生成字节码实现具体类型到占位符类型的适配）来实现，这样势必让动态类型语言实现的复杂度增加，也可能带来额外的性能或者内存开销。尽管可以利用一些办法（如Call Site Caching）让这些开销尽量变小，但这种底层问题终归是应当在虚拟机层面上去解决才最合适，因此在Java虚拟机层面上提供动态类型的直接支持就成为了java平台的发展趋势之一，这就是JDK1.7（JSR）中invokedynamic指令以及java.lang.invoke包出现的技术背景。 简单来说,就是方法在编译期间必须知道他所属的对象,但是针对一些弱类型语言并不需要知道所属的对象是谁,对一些jvm编程语言的开发造成了一定的困难 java.lang.invoke包 的methodHandle支持 MethodHandle的使用方法和效果与Reflection有众多相似之处，不过，他们还是有以下这些区别：从本质上讲，Reflection和MethodHandle机制都是在模拟方法调用，但Reflection是在模拟Java代码层次的方法调用，而MethodHandle是在模拟字节码层次的方法调用。在MethodHandles.lookup中的3个方法——findStatic()、findVirtual()、findSpecial()正是为了对应于invokestatic、invokevirtual&invokeinterface和invokespecial这几条字节码指令的执行权限校验行为，而这些底层细节在使用Reflection API时是不需要关心的。 Reflection中的java.lang.reflect.Method对象远比MethodHandle机制中的java.lang.invoke.MethodHandle对象所包含的信息多。前者是方法在Java一端的全面映像，包含了方法的签名、描述府以及方法属性表中各种属性的java端表示方式，还包含执行权限等的运行期信息。而后者仅仅包含与执行该方法相关的信息。用通俗的话来讲，Reflection是重量级，而MethodHandle是轻量级。 由于MethodHandle是对字节码的方法指令调用的模拟，所以理论上虚拟机在这方面做的各种优化（如方法内联），在MethodHandle上也应当可以采用类似思路去支持。而通过反射去调用方法则不行。MethodHandle与Reflection除了上面列举的区别外，最关键的一点还在于去掉前面讨论施加的前提“仅站在Java语言的角度来看”：Reflection API的设计目标是只为java语言服务的，而MethodHandle则设计成可服务于所有Java虚拟机之上的语言，其中也包括Java语言。","date":"2024-12-31","categories":["java"],"tags":["java版本和更新内容"]},{"title":"java版本追朔-java8","url":"/2024/12/31/java版本追朔-java8/","content":"lamberstreamjava 的 js 引擎","date":"2024-12-31","categories":["java"],"tags":["java版本和更新内容"]},{"title":"java版本追朔-java9","url":"/2024/12/31/java版本追朔-java9/","content":"模块系统","date":"2024-12-31","categories":["java"],"tags":["java版本和更新内容"]},{"title":"java-stream和lamble(1)-lamble基本语法","url":"/2024/12/31/java-stream和lamble-1--lamble基本语法/","content":"首先我们先聊一聊java的lamble表达式 这里给出一个简单的例子 注意:lamble的new的对象必须为接口，并且接口中必须只有一个函数,但是可以有默认实现函数,比如下面这样 java中lamble表达式的写法 最基本写法(只有一个参数和一行代码的时候) lamble接口类 使用样例 lamble的其他形式 lamble 表达式常用的一些接口 lamble表达式的作用域lamble表达式将会使用作用域的引用,比如下面的例子 java的实际输出结果是 item对应的值 注意在使用lamble表达式的时候,传入给内部的变量必须是不可变化的变量(使用final修饰符) lamble 其他需要注意的地方 当右侧具有返回值的时候 一个lamble接口: 当lamble表达式具有返回值的时候,应保证右侧的表达式或者代码快能返回函数中定义的变量 但使用范型的时候 注意当使用的是范型的时候,要注意lamble表达式的各种属性应该满足范型定义的各种类型(声明时T,R的类型) 注意三，使用lamble可能有函数签名不一致的问题 这个问题的本质是应为lamble表达式只是关心接口中函数的形式而这个函数所在的接口其实lamble并不关心在java内部的虚拟机实现这个方法的时候本质上是寻找有没有一个接口能够读应这个表达式,从而通过这种方法找到对应的接口 例子： 不过由于这种重载导致的问题可以使用强制转换解决,如果参数的数量相同但是类型不同，可以使用有类型的lamble表达式 总结: Lambda 表达式作为参数时， 其类型由它的目标类型推导得出， 推导过程遵循如下规则： 如果只有一个可能的目标类型， 由相应函数接口里的参数类型推导得出； 如果有多个可能的目标类型， 由最具体的类型推导得出； 如果有多个可能的目标类型且最具体的类型不明确， 则需人为指定类型。 ps ： 最后一点java中没有null这个对象","date":"2024-12-31","categories":["java"],"tags":["java的stream"]},{"title":"java-stream和lamble(2)-stream基本使用","url":"/2024/12/31/java-stream和lamble-2--stream基本使用/","content":"上一节我们梳理了java lamble相关的东西,这里梳理一下java8 stream相关的东西 java集合类型的迭代方法 外部迭代 - 循环体和迭代的逻辑在外部 本质上来讲就是迭代的流程在外部用户可见 内部迭代 这个是java8的新东西,所有的结合类都可以使用stream接口进行迭代 这里写一个例子,迭代出所有的偶数 本质上是内部实现数据的迭代更新,而外部只是实现内部的迭代逻辑罢了 内部迭代本质上可以是一种函数调用,上面这个例子干了两件事请 过滤出所有的偶数 生成一个新的集合 注意这里好像是迭代了两次但是本质上并没有,记下来说明一下实现的原理 java stream迭代的内部实现在java的实现原理中有两种类型,一种是 惰性求值 另一种是 尽早求值 惰性求值和尽早求值区分方法其实这个很好记 就是如果返回是stream类的都是惰性求值,反之就是尽早求值 惰性求值和尽早求值的差别惰性求值其实本质上是不会运行其中的代码的,只有在尽在求值的时候才是真正意义上的执行逻辑 比如这样一个代码 在这个过程中不会输出item对应的值 引申一下:其实这个过程和建造者模式类似,在真正实现之前不停的添加配置和操作,只有在最后的Build过程中才真正的执行 java stream 的常用操作1. collect(toList); 及早求值 就不罗嗦了不过这个有两种种结构 2. map类型这个方法很常用,就像他的名字映射,使用这种方法可以直接将一个流转化成另一个流 一个例子,进行一下对比 2.1 mapToLong mapToInt mapToDouble这三个方法是java 的stream 针对java对基本类型的自动装箱和自动拆箱的优化 传入的参数拿 Long类型举例 ToLongFunction -> 实现对象到long 类型的转换 这些方法将会返回一下特殊的stream 比如LongStream等等 而这种stream针对不同的输出,拥有不同的适配方法 比如long转double会有 LongToDoubleFunction接口比如long转Long 对象 会有 LongFunction接口而这种stream 实现的map接口是LongUnaryOperator,通过这种方法实现性能上的高效 ****引申 java在针对数字统计的时候,有意识的提供了额外的方法来统计数据->summaryStatistics 方法,这个方法能计算出各种各样的统计值， 如 IntStream 对象内所有元素中的最小值、 最大值、 平均值以及数值总和,例子如下 3. filter这个是过滤器,通过这种方法可以将感兴趣的stream流中的对象整合进新的对象中 4. flapMapmap 的升级版 flatMap 方法可用 Stream 替换值，然后将多个 Stream 连接成一个 Stream,说白了就是整合流用的 5. max和min max 和min一样只要有一个比较器就好了,但是java底层有关这一层的封装特性比较复杂,接下来引申一下java Comparator#comparing方法的相关实现 首先看一下接口的定义 T 只是一个普通的变量 U在这里必须是一个实现了Comparable接口的类(java类内部比较函数), 而传入的Function Lamble 表达式的返回值也是u,这样就可以理解了java Comparator的这个方法干了什么了-> 传入一个可以生成可比较类的Function lamble 表达式,返回一个Comparator 比较逻辑 ps: 封转的很牛逼! 注意这里有一个java8 的全新语法 类1 & 类2 其实这个接口相当于强制转化成一个同时 继承了类1 类2 的对象,这个用法的规则通java的继承规则 其实java 如果编写相关的逻辑的时候使用的写法类似这样的 *** : 其实看一下jdk底层的源代码就知道了,其实这个东西本质上是调用了reduce 方法接下来来说一下reduce这个方法 6. reduce一个调用的例子 reduce模式的核心就是accumulator 迭代器, 这个方法第一个参数是每次迭代的返回值,第二个参数是当前进行迭代的值, 转换成java代码就是如下的形式 reduce主要有三种形式 类型1 和 2 没什么好说的都是迭代求值,罢了 注意第二种方法是Optional类型的返回值 类型3 有两个相同的迭代函数 accumulator 和 combiner,具有这个设计的原因是这样的,Stream是支持并发操作的，为了避免竞争，对于reduce线程都会有独立的result，combiner的作用在于合并每个线程的result得到最终结果。这也说明了了第三个函数参数的数据类型必须为返回数据类型了 8. 流式迭代操作其实java的所有惰性求值都可以不停的第迭代(流式操作)))的, 比如想下面这样 9. foreach 迭代一个基本的java 迭代操作像如下的形式 使用foreach 改造成函数式编程变成如下的形式 10. sorted排序方法,和list的集合sort一样 惰性求值方法 11. unorderedunordered操作不会进行任何显式的打乱流的操作。它的工作是：消除流中必须保持的有序约束，因此允许之后的操作使用 不必考虑有序的优化。 12. 针对map的新操作 一个高级例子首先我们定义稍微复杂的对象,School 学校对象,Student 用户对象 定义初始化方法allInit 这里定义一个需求,找到这两个大学找出其中所有年纪大于14岁的所有学生的姓名和学校姓名 最基本的遍历方法 (简单双层for循环遍历) foreach 处理迭代 使用 filter过滤用户年龄使用 map 生成新的对象 进阶 使用flatmap 配合 collect(collection.toset())一次性生成 这里简单的原因是 flatMap整合了流,而collect 结合了list @FunctionalInterface 注解其实看一下所有java 内置的函数化接口其实都使用了这个注解 该注解会强制 javac 检查一个接口是否符合函数接口的标准。如果该注释添加给一个枚举类型、类或另一个注释，或者接口包含不止一个抽象方法，javac 就会报错。重构代码时，使用它能很容易发现问题。 java 收集器java stream的收集器就是为了将数据整理成同一结构的工具 java 内部类库实现的部分收集器 tolist toSet 以上两个式最简单的,就是将stream中的方法形成新的集合 toCollection 上面这个可以将 list 转化成stream方法定义接口的相同的集合实现自定义 比如stream是定义在collection接口上的,那个通过这个方法能实现list和set的相互转化 ps 这个方法的实现建议仔细的研究一下 minBy maxBy 这个方法将会把这个集合中制定参数最小的那个对象返回 averagingInt 平均数 summarizingInt 求合 这个方法可以获取这个integer数组的平均值 partitioningBy 分组筛选 数据分组， 通过一个返回boolean类型的表达式从而实现两种返回值的特殊收集器 java 字符串拼接方法 joining中的各种方法分别表示参数的分隔符，开始符号和结束符号 groupingBy mapping 自定义分组 这个方法是上面的扩展,函数将会传入两个参数，第一个函数是生成返回的key，第二个函数是生成value 例子中我们都用到了第二个收集器,用以收集最终结果的一个子集。这些收集器叫作下游收集器。收集器是生成最终结果的一剂配方,下游收集器则是生成部分结果的配方,主收集器中会用到下游收集器。这种组合使用收集器的方式,使得它们在 Stream 类库中的作用更加强大。 那些为基本类型特殊定制的函数,如 averagingInt 、 summarizingLong 等,事实上和调用特殊 Stream 上的方法是等价的,加上它们是为了将它们当作下游收集器来使用的 其他的一些api list转map – 这个非常常用，Function.identity 自定义java stream 收集器有的时候java内置的组合器其实并不能真正的实现我们的需求，这个时候我们就需要自定义收集器来实现我们想要的东西 比如我们有这样的一个业务，存在如下的一种对象和对象的list集合，我们需要遍历的list集合，取出手有的name信息，整合成以”{“符合开头,”]”符号结尾,”,”符号分隔的字符串 方法1 传统遍历法 方法2 使用foreach遍历 方法3 发现是一种迭代模型使用reduce实现 方法4 实现Collector<T, A, R> 接口 首先说一下Collector<T, A, R>接口中的三个范型 T 待收集元素的类型,这里是 String ; A 累加器的类型 StringCombiner ; R 最终结果的类型,这里依然是 String 。 几个重要的方法 supplier 初始化容器，其实也就是默认的值或者处理类 accumulator 迭代方法 combiner 双流合并方法 finish 最终生成方法 characteristics 一个优化用的列表 定义好了自定义的collect之后就简单了，直接调用就可以了","date":"2024-12-31","categories":["java"],"tags":["java的stream"]},{"title":"java-stream和lamble(3)-接口默认方法和继承问题","url":"/2024/12/31/java-stream和lamble-3--接口默认方法和继承问题/","content":"java8 stream 在collection接口中添加了一个新的方法叫做stream,之前如果自己定义的子类如果在java8 中没有实现将会报错 所以java在接口上添加了默认方法 default字段来实现更加方便的扩展 接口default 的继承性问题首先总结一下继承的规则: 类胜于接口。如果在继承链中有方法体或抽象的方法声明，那么就可以忽略接口中定义的方法。 子类胜于父类。如果一个接口继承了另一个接口，且两个接口都定义了一个默认方法，那么子类中定义的方法胜出.如果同时继承了两个具有相同方签名的接口,将会报错,因为编辑器不知道选择哪一个,但是可以使用下面的方法来兼容(应用了规则1) 如果上面两条规则不适用，子类要么需要实现该方法，要么将该方法声明为抽象方法. 注意default是针对接口的,所以不存在继承的问题,实现接口的类只是覆盖,不能想类那样使用super去调用父类的东西 java 接口中statis和 default相同点和区别 static和之前的规则相同,并不会产生继承效果并且如果要调用这个方法,只能使用定义的时候的类或者接口来进行调用 defaule 同样是一个默认方法,但是default可以使用继承的或者实现的类来进行调用,但是子类或者接口如果实现了重载 注意: 如果子类或者子接口要想调用父接口中的default方法,可以使用接口名称+super+方法名称调用","date":"2024-12-31","categories":["java"],"tags":["java的stream"]},{"title":"java-stream和lamble(4)-optional","url":"/2024/12/31/java-stream和lamble-4--optional/","content":"在java8的函数式编程中,处理流式处理还提供了空值的流式处理Optional 这个方法几个重要的方法 1. orElse(value) 和 orElseGet(function) 和 orElseThrow获取一个值, 如果没有将会 将会使用value替代,如果传入的函数将会运行函数返回指定的值,如果式异常将会跑出空值 2. isEmpty isPresent前者如果optional为空将会返回true 后者将会返回false 3. of 和 ofNullable创建一个optional 前者不能传递null 后者可以传递null","date":"2024-12-31","categories":["java"],"tags":["java的stream"]},{"title":"java-stream和lamble(5)-高级写法","url":"/2024/12/31/java-stream和lamble-5--高级写法/","content":"1. 方法引用Lambda 表达式有一个常见的用法：Lambda 表达式经常调用参数。比如想得到艺术家的姓名，Lambda 的表达式如下： java 为这种写法提供了更加简单的实现 构造函数也有同样的缩写形式，如果你想使用 Lambda 表达式创建一个 Artist 对象，可能会写出如下代码： 使用方法引用，上述代码可写为：","date":"2024-12-31","categories":["java"],"tags":["java的stream"]},{"title":"java-stream和lamble(6)-并行化计算","url":"/2024/12/31/java-stream和lamble-6--并行化计算/","content":"java 的函数式变成将一个stream操作变成并行化的过程非常简单，只要使用parallelStream() 方法替代stream()方法就好了","date":"2024-12-31","categories":["java"],"tags":["java的stream"]},{"title":"Java-泛型(1)-基本使用方法1","url":"/2024/12/31/Java-泛型-1--基本使用方法1/","content":"一.定义范型 (1)范型可以使用有界表示法extends 表示下界surper 表示上界class B {} # 表示只有A或者A的子类可接受class C{} # 表示只有F或者是F的父类可接受 注意 ：范型中的T不能进行强制转化—静态编译型语言特点 (2)范型通配符 ? 三.继承中的范型 只要保证外部的范型类型必须实现内部的 四.范型的底层实现为了实现与非泛型代码的兼容，Java语言的泛型采用擦除(Erasure)来实现，也就是泛型基本上由编译器来实现，由编译器执行类型检查和类型推断，然后在生成字节码之前将其清除掉，虚拟机是不知道泛型存在的。这样的话，泛型和非泛型的代码就可以混合运行，当然了，也显得相当混乱。 在使用泛型时，会有一个对应的类型叫做原生类型(rawtype)，泛型类型会被擦除到原生类型，如Generic会被查处到Generic，**List**会被查处到List，由于擦除，在虚拟机中无法获得任何类型信息，虚拟机只知道原生类型。下面的代码将展示Java泛型的真相–擦除： 使用javap反编译class文件，得到如下代码： 从反编译出来的字节码可以看到，泛型Erasure被擦除到了Erasure，其内部的字段T被擦除到了Object，可以看到get和set方法中都是把t作为Object来使用的。最值得关注的是，反编译代码的倒数第三行，对应到Java代码就是Stringvalue &#x3D;eras.get();编译器执行了类型转换。这就是Java泛型的本质：对传递进来的值进行额外的编译期检查，并插入对传递出去的值的转型。这样的泛型真的是泛型吗？ 即便我们可以说，Java中的泛型确实不是真正的泛型，但是它带来的好处还是显而易见的，它使得Java的类型安全前进了一大步，原本需要程序员显式控制的类型转换，现在改由编译器来实现，只要你按照泛型的规范去编写代码，总会得到安全的保障。在这里，我们不得不思考一个问题，理解Java泛型，那么其核心目的是什么？我个人认为，Java泛型的核心目的在于安全性，尤其是在理解泛型通配符时，一切奇怪的规则，归根结底都是处于安全的目的。 这里使用泛型注意擦除到的是原始类型,也就是底线,泛型如果是这样定义的 那么底线就是String 类型信息的丢失 由于擦除的原因，在泛型代码内部，无法获得任何有关泛型参数类型的信息。在运行时，虚拟机无法获得确切的类型信息，一切以来确切类型信息的工作都无法完成，比如instanceof操作，和new表达式， 五.型的相关坑点 范型不是类型绑定的所以 List 和 List 不是同一个类可以使用通配符将两个属性变成相同的 泛型类中的数组对数组的限制 不能实例化泛型数组 其次不能声明 泛型组成的数组 不能实例化范型 new T();","date":"2024-12-31","categories":["java"],"tags":["java范型反射类型系统"]},{"title":"java-泛型(2)-基本使用方法2","url":"/2024/12/31/java-泛型-2--基本使用方法2/","content":"泛型的使用泛型有三种使用方式，分别为：泛型类、泛型接口、泛型方法 泛型类和泛型接口泛型类的最基本写法 一个最普通的泛型类： 注意：范型类也可以传入范型实参，同样也会被类的定义所限制 范型方法泛型类，是在实例化类的时候指明泛型的具体类型；泛型方法，是在调用方法的时候指明泛型的具体类型 。 注意：即使在范型类中指定范型标识和类中的范型方法使用的范型标识相同，类中的范型方法使用的范型标识也相当于一个新的标识 注意：静态方法只能声明为范型函数，否则不可以带有范型 泛型上下边界和通配符java 使用extends 和 super 分别表示 范型声明时的只能是extends及其子类，和使用时必须是super及其父类的限制 上面的例子中使用了通配符 在java 的范型中通配符解决了范型多版本不兼容的问题 比如Generic不能被看作为&#96;Generic的子类。所以同一种泛型可以对应多个版本（因为参数类型是不确定的），不同版本的泛型类实例是不兼容的。 因此我们需要一个在逻辑上可以表示同时是Generic和Generic父类的引用类型。由此类型通配符应运而生。 例子如下 将程序改成这样既可以通过 类型通配符一般是使用？代替具体的类型实参，注意了，此处’？’是类型实参，而不是类型形参 ，可以把？看成所有类型的父类。是一种真实的类型。 范型数组（范型的限制）关于泛型数组要提一下看到了很多文章中都会提起泛型数组，经过查看sun的说明文档，在java中是”不能创建一个确切的泛型类型的数组”的。 也就是说下面的这个例子是不可以的： 而使用通配符创建泛型数组是可以的，如下面这个例子： 这样也是可以的： 下面使用Sun的一篇文档的一个例子来说明这个问题： 其实说白了就是java在实现范型的时候使用了擦除的方法，所以在上面代码的最后一行将会出现string &#x3D; object 这种类型不匹配的情况 下面采用通配符的方式是被允许的:数组的类型不可以是类型变量，除非是采用通配符的方式，因为对于通配符的方式，最后取出数据是要做显式的类型转换的。","date":"2024-12-31","categories":["java"],"tags":["java范型反射类型系统"]},{"title":"java-泛型(3)-匿名内部类+反射+泛型实现真实反射机制","url":"/2024/12/31/java-泛型-3--匿名内部类-反射-泛型实现真实反射机制/","content":"java针对泛型类型的实现方法,和反射之间的应用java的泛型实现因为考虑到兼容型的问题,所以一开始采用了类型擦除的机制实现相关的功能. 但是这种类型擦除的机制又带来了一个新的问题,比如这样的场景 因为T会在java运行的时候进行擦除,所以这里输出的内容是Object 如果想要获取这个参数的类型,我们只能在泛型类的内部获取到 为什么会这样的呢,这里没有得到权威的答案,但是按照我的猜想,应该是这样的过程 我们看一下java生成的class文件,发现上面这个类其实生成了三个类 第一个 和最后一个就不用多说了,重点关心一下中间这个,反编译看看 看到这里差不多来了就能知道java虚拟机底层是怎么处理这里的了 我们将代码修改一下变成这样 再看看生成的class文件信息 上面的代码中生成了4个泛型匿名内部类类,然后在java的class 文件中同样生成了四个 类名+$+编号的 动态生成class ps 这个机制其实是java实现匿名内部类的原理,jdk自己new了一个新的对象来实现类的生成 这样就是为什么在使用的时候能获取到类型的原因,和获取泛型类型的方法是Type superClass &#x3D; getClass().getGenericSuperclass();使用的是supper 类. 因为java动态生成一个继承了携带泛型真实类型的class类,在运行的时候进行了类型的动态替换 通过上面的例子其实我们可以总结一个规律:如果泛型想要拿到参数,必须在声明类的时候就将泛型对应的参数传递进入通过这种例子我们就可以在声明的时候大快人心的使用泛型了 当然我们亦可以参考开篇的分析使用匿名内部类来让jvm自动的时候这个过程,但是不推荐这样使用,因为这个时候java已经退化成一个动态语言了,jvm无法进行优化","date":"2024-12-31","categories":["java"],"tags":["java范型反射类型系统"]},{"title":"java-注解反射动态代理(1)-注解","url":"/2024/12/31/java-注解反射动态代理-1--注解/","content":"java注解，@，注解有什么用@是java注解，即annotation。 可以理解为插件，是代码级别的插件，在类的方法上写：@XXX，就是在代码上插入了一个插件。 Java注解是附加在代码中的一些元信息，用于一些工具在编译、运行时进行解析和使用，起到说明、配置的功能。注解不会也不能影响代码的实际逻辑，仅仅起到辅助性的作用 元注解和自定义注解java中有很多的注解比如@Override等等，这些注解其实是jdk内部自己声明的注解，而这些注解的声明就依赖于注解的注解的元注解 元注解类型 @target 声明注解的对象范围 注解类型包括 类型名称 限制范围 construction 构造函数 FIeld 变量域 LOCAL_VARIABLE 用于描述局部变量 METHOD 用于描述方法 PACKAGE 用于描述包 PARAMETER 用于描述参数 TYPE 用于描述类、接口(包括注解类型) 或enum声明 @Retention声明注解的保留范围 类型 保留范围 SOURCE 在源文件中有效（即源文件保留） CLASS 在class文件中有效（即class保留） RUNTIME 在运行时有效（即运行时保留） @Documented注解的文档的化 用于描述其它类型的annotation应该被作为被标注的程序成员的公共API，因此可以被例如javadoc此类的工具文档化。Documented是一个标记注解，没有成员。 @Inherited –注解的继承化 元注解是一个标记注解，@Inherited阐述了某个被标注的类型是被继承的。如果一个使用了@Inherited修饰的annotation类型被用于一个class，则这个annotation将被用于该class的子类。 注意：@Inherited annotation类型是被标注过的class的子类所继承。类并不从它所实现的接口继承annotation，方法并不从它所重载的方法继承annotation。 注意：当@Inherited annotation类型标注的annotation的Retention是RetentionPolicy.RUNTIME，则反射API增强了这种继承性。如果我们使用java.lang.reflect去查询一个@Inherited annotation类型的annotation时，反射代码检查将展开工作：检查class和其父类，直到发现指定的annotation类型被发现，或者到达类继承结构的顶层。","date":"2024-12-31","categories":["java"],"tags":["java范型反射类型系统"]},{"title":"java-注解反射动态代理(2)-反射1-基础方法","url":"/2024/12/31/java-注解反射动态代理-2--反射1-基础方法/","content":"反射的定义在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意一个方法和属性；这种动态获取的信息以及动态调用对象的方法的功能称为反射。 在java中反射是基于class信息的 关于Class:Class是一个类，一个描述类的类（也就是描述类本身），封装了描述方法的Method，描述字段的Filed，描述构造器的Constructor等属性 java反射相关的接口方法和最佳实践classclass 是java反射的最粗集合,分装了class对应的各种属性 获取class 名称或者类型名称 java的class 对象提供了两个方法获取名称字符串 getName 这个方法会返回这个class的解析名称,绝提规则如下 如果是对象,就返回对象的全限定名称(没有泛型信息)如果是数组返回如下信息,[表示维度[Z &#x3D; boolean[B &#x3D; byte[S &#x3D; short[I &#x3D; int[J &#x3D; long[F &#x3D; float[D &#x3D; double[C &#x3D; char[L &#x3D; any non-primitives(Object) getTypeName 返回类型通getName方法 如果是数组集合或者对象,返回这个的定义格式 比如如果是int[][][] 将会返回int[][][] class 继承关系状态获取 通过这种方法可以快速查找class的继承关系 class 有关java Type体系的信息获取 class 有关包信息的方法 class的访问级别 class 对象或者class类强制转化提供的功能 针对特殊类型比如arr类型枚举的特殊处理方法 各种is方法 注解使用方法 类加载器获取 资源获取 fieldfield是java参数属性的载体. field获取方法 field一些属性信息方法 field设置和获取方法 field类型获取方法 针对注解的方法 method 获取方法 method参数相关操作方法方法 method返回值相关参数方法 运行方法 注解方法 相比较其他的编程语言方法,method只是多了获取参数的注解方法 异常捕获 method属性操作相关方法 8. Constructorjava的constructer可以理解成特殊的方法(java中并不等同,处理方法,继承信息等等都是不相同的),方法都是通用的,这里记录一下不同的方法 constructor获取方法 这里不解释了,没有Declared的方法表示获取所有的public构造函数,有的将会获取这个类中直接定义的构造好书 构造函数创建实体类 array 范型 AccessibleObject AccessibleObject是 Method Fielt Construction的一个父类-都实现了此类的方法,这个类主要是扩展了访问权限的控制 尤其要注意这个地方setAccessible(true); 正是通过这个配置方法的访问权限 这里其实要记录一下有关于泛型反射的一些东西我们都知道java的泛型实现是基于类型擦除的,这将会会导致这个泛型的类型将在反射的时候返回具体类型(默认是object)但是这种情况并不是绝对了,目前我总结了三种情况下java会保证原来的类型 生成匿名内部类的时候 也就是new一个接口或者new一个抽象类 这种情况下将会保留原来的属性 实现的原理是java针对匿名内部类将会动态的生成新的class 而这个class的泛型是带有参数的 方法的返回值 所有的方法的方法如果使用泛型类,比如 List 这种情况下是能获取到泛型中的类型的 子类继承泛型父类 其实这种情况是1 类型的手动实现也就是实现下面这种情况d 总结一些,如果泛型想要拿到参数,必须在声明类的时候就将泛型对应的参数传递进入","date":"2024-12-31","categories":["java"],"tags":["java范型反射类型系统"]},{"title":"java-注解反射动态代理(2)-反射2-javaType类型体系","url":"/2024/12/31/java-注解反射动态代理-2--反射2-javaType类型体系/","content":"Java Type体系简介Type是Java 编程语言中所有类型的公共高级接口,注意这个类型并不是我们传统上常说的int、String、List、Map等数据类型而是从Java语言角度来说，对基本类型、引用类型向上的抽象 Type体系中类型的包括： 原始类型\\基本类型(Class) : 常所指的类，还包括枚举、数组、注解等 参数化类型(ParameterizedType) : 指泛型,如泛型List、Map或泛型类 数组类型(GenericArrayType) : 指带有泛型的数组，即T[] 类型变量(TypeVariable) : 类型变量，即泛型中的变量；例如：T、K、V等变量，可以表示任何类 泛型表达式(WildcardType) : 这个type类型是解决泛型? extend Number、? super Integer这样的表达式,相关类型的特殊Type子类(这个类和TypeVariable对应,TypeVariable相当于具体的类型而WildcardType相当于真实的类型) 这里具体的介绍一下相关的内容 1. ParameterizedTypeParameterizedType表示参数化类型，也就是泛型，例如List、Set等, 在ParameterizedType接口中，有3个方法，分别是getActualTypeArgumen ts()、 getRawType()、 getOwnerType(); getActualTypeArguments() 获取泛型中的实际类型，可能会存在多个泛型，例如Map<K,V>,所以会返回Type[]数组 值得注意的是，无论<>中有几层嵌套(List<Map<String,Integer>)，getActualTypeArguments()方法永远都是脱去最外层的<>(也就是List<>)，将口号内的内容（Map<String,Integer>）返回；我们经常遇到的List，通过getActualTypeArguments()方法，得到的返回值是TypeVariableImpl对象，也就是TypeVariable类型(后面介绍); getRawType() 获取声明泛型的类或者接口，也就是泛型中<>前面的那个值； getOwnerType() 通过方法的名称，我们大概了解到，此方法是获取泛型的拥有者，那么拥有者是个什么意思？Returns a {@code Type} object representing the type that this type * is a member of. For example, if this type is {@code O.I}, * return a representation of {@code O}. （摘自JDK注释）通过注解，我们得知，“拥有者”表示的含义–内部类的“父类”，通过getOwnerType()方法可以获取到内部类的“拥有者”；例如： Map 就是 Map.Entry<String,String>的拥有者； 2. GenericArrayType泛型数组类型，例如List[] 、T[]等； 在GenericArrayType接口中，仅有1个方法，就是getGenericComponentType()； getGenericComponentType() 返回泛型数组中元素的Type类型，即List[] 中的 List（ParameterizedTypeImpl）、T[] 中的T（TypeVariableImpl）； 值得注意的是，无论是几维数组，getGenericComponentType()方法都只会脱去最右边的[]，返回剩下的值 3. TypeVariable泛型的类型变量，指的是List、Map<K,V>中的T，K，V等值，实际的Java类型是TypeVariableImpl（TypeVariable的子类）；此外，还可以对类型变量加上extend限定，这样会有类型变量对应的上限 在TypeVariable接口中，有3个方法，分别为getBounds()、getGenericDeclaration()、getName() getBounds() 获得该类型变量的上限，也就是泛型中extend右边的值；例如 List ，Number就是类型变量T的上限；如果我们只是简单的声明了List（无显式定义extends），那么默认为Object； 无显式定义extends： 值得注意的是，类型变量的上限可以为多个，必须使用&符号相连接，例如 List<T extends Number & Serializable>；其中，& 后必须为接口； getGenericDeclaration() 获取声明该类型变量实体，也就是TypeVariableTest中的TypeVariableTest getName() 获取类型变量在源码中定义的名称 说到TypeVariable类，就不得不提及Java-Type体系中另一个比较重要的接口—GenericDeclaration；含义为：声明类型变量的所有实体的公共接口；也就是说该接口定义了哪些地方可以定义类型变量（泛型）； 通过查看源码发现，GenericDeclaration下有三个子类，分别为Class、Method、Constructor；也就是说，我们定义泛型只能在一个类中这3个地方自定义泛型； 此时，我们不禁要问，我们不是经常在类中的属性声明泛型吗，怎么Field没有实现 GenericDeclaration接口呢？ 其实，我们在Field中并没有声明泛型，而是在使用泛型而已 正因为是使用泛型，所以Field并没有实现GenericDeclaration接口 4. WildcardType？—通配符表达式，表示通配符泛型，但是WildcardType并不属于Java-Type中的一钟；例如：List<? extends Number> 和 List<? super Integer> 在WildcardType接口中，有2个方法，分别为getUpperBounds()、getLowerBounds(); getUpperBounds() 获取泛型变量的上边界（extends） getLowerBounds 获取泛型变量的下边界（super）","date":"2024-12-31","categories":["java"],"tags":["java范型反射类型系统"]},{"title":"java-注解反射动态代理(3)-动态代理","url":"/2024/12/31/java-注解反射动态代理-3--动态代理/","content":"代理模式是软件工程一种常见的设计模式,也是spring框架实现aop的核心方法,这里方便立即首先记录一下静态代理模式 静态代理模式首先定义一个接口和一个接口实现类 我们将通过静态代理对 UserServiceImpl 进行功能增强，在调用 select 和 update 之前记录一些日志。写一个代理类 UserServiceProxy，代理类需要实现 UserService 客户端测试 输出 通过静态代理，我们达到了功能增强的目的，而且没有侵入原代码，这是静态代理的一个优点。 静态代理的缺点 当需要代理多个类的时候，由于代理对象要实现与目标对象一致的接口，有两种方式： 只维护一个代理类，由这个代理类实现多个接口，但是这样就导致代理类过于庞大 新建多个代理类，每个目标对象对应一个代理类，但是这样会产生过多的代理类 当接口需要增加、删除、修改方法的时候，目标对象与代理类都要同时修改，不易维护。 java提供的jdk代理java 作为面向对象的坚挺实现者,必然支持动态代理,这里介绍一下java基于接口设计的动态代理模式 java动态代理的实现者 InvocationHandler 接口 每一个动态代理类都必须要实现InvocationHandler这个接口，并且每个代理类的实例都关联到了一个handler，当我们通过代理对象调用一个方法的时候，这个方法的调用就会被转发为由InvocationHandler这个接口的 invoke 方法来进行调用。 这个方法拥有三个参数分别表示 proxy:指代我们所代理的那个真实对象 method:指代的是我们所要调用真实对象的某个方法的Method对象 args:指代的是调用真实对象某个方法时接受的参数 创建代理的类Proxy 之前说了通过实现Invocationhandle接口可以创建代理,java 还提供了一个特殊的类来方便我们创建代理对象实例 proxy类有很多方法,这里我们只观察最常用的方法 这个方法有三个参数,这里列举一下这些方法的作用 loader:一个ClassLoader对象，定义了由哪个ClassLoader对象来对生成的代理对象进行加载 interfaces:一个Interface对象的数组，表示的是我将要给我需要代理的对象提供一组什么接口，如果我提供了一组接口给它，那么这个代理对象就宣称实现了该接口(多态)，这样我就能调用这组接口中的方法了 h:一个InvocationHandler对象，表示的是当我这个动态代理对象在调用方法的时候，会关联到哪一个InvocationHandler对象上 这里列一个列子 接口和实现类 InvocationHandle接口实现类 调用方法 输出结果 引申: java是动态生成代理,调用的是真实的类的方法 上面Main方法中通过动态代理生成的对象,其实并不是TestInterface对象,而是$Proxy0..2 这样的对象,这个是java一个特殊的代理对象,可以强制转化成Proxy的newProxyInstance方法第二个参数传入的所有类型 当这个方法调用对应的方法时,将会动态的桥接给实现Invocationhandle接口的代理类的invoke方法,其实method解释这个接口这个方法的Method反射","date":"2024-12-31","categories":["java"],"tags":["java范型反射类型系统"]},{"title":"junit4-使用整理(1)","url":"/2024/12/31/junit4-使用整理-1-/","content":"最用在企业中进行实习使用了junit4进行单元测试这里进行一下相关的知识点的总结 JUnit是什么JUnit是用于编写和运行可重复的自动化测试的开源测试框架， 这样可以保证我们的代码按预期工作。JUnit可广泛用于工业和作为支架(从命令行)或IDE(如Eclipse)内单独的Java程序。 JUnit的提供 断言测试预期结果。 测试功能共享通用的测试数据。 测试套件轻松地组织和运行测试。 图形和文本测试运行。 JUnit用于测试 整个对象 对象的一部分 – 交互的方法或一些方法 几个对象之间的互动(交互) JUnit的特点 JUnit是用于编写和运行测试的开源框架。 提供了注释，以确定测试方法。 提供断言测试预期结果。 提供了测试运行的运行测试。 JUnit测试让您可以更快地编写代码，提高质量 JUnit是优雅简洁。它是不那么复杂以及不需要花费太多的时间。 JUnit测试可以自动运行，检查自己的结果，并提供即时反馈。没有必要通过测试结果报告来手动梳理。 JUnit测试可以组织成测试套件包含测试案例，甚至其他测试套件。 Junit显示测试进度的，如果测试是没有问题条形是绿色的，测试失败则会变成红色。 junit4简单例子 新建一个测试类 创建一个测试类 过程：指定一个测试用的工具包右键>>new>>other>>搜索junit>>选择 Junit Test Case 创建后的类 进行测试 右键>>run as Test","date":"2024-12-31","categories":["java"],"tags":["junit4"]},{"title":"junit4-使用整理(2)","url":"/2024/12/31/junit4-使用整理-2-/","content":"JUnit4使用的基本注解JUnit4基本注释，下表列出了这些注释的概括： 注解 描述 @Test public void method() 测试注释指示该公共无效方法它所附着可以作为一个测试用例。可以指定一个异常这个已成将会捕获测试会成功，还可以指定超时时间超过指定的运行时间价将会自动判断为超时 @Before public void method() 注释表示，该方法必须在类中的每个测试之前执行，以便执行测试某些必要的先决条件 @BeforeClass public static void method() BeforeClass注释指出这是附着在静态方法必须执行一次并在类的所有测试之前。发生这种情况时一般是测试计算共享配置方法(如连接到数据库) @After public void method() 注释指示，该方法在执行每项测试后执行(如执行每一个测试后重置某些变量，删除临时变量等) @AfterClass public static void method() 当需要执行所有的测试在JUnit测试用例类后执行，AfterClass注解可以使用以清理建立方法，(从数据库如断开连接)。注意：附有此批注(类似于BeforeClass)的方法必须定义为静态 @Ignore public static void method() 当想暂时禁用特定的测试执行可以使用忽略注释。每个被注解为@Ignore的方法将不被执行 一个例子 控制台输出的结结果 总结：对于java的类来说所有BeforeClass 会在所有的测试调用前进行调用 AfterClass 将会在所有的测试使用完成后在进行调用 Before After 将会在所有的测试完成的时候进行调用 junit断言这些方法都受到 Assert 类扩展了java.lang.Object类并为它们提供编写测试，以便检测故障。下表中有一种最常用的断言方法的更详细的解释 断言 描述 void assertEquals([String message], expected value, actual value) 断言两个值相等。值可能是类型有 int, short, long, byte, char or java.lang.Object. 第一个参数是一个可选的字符串消息 void assertTrue([String message], boolean condition) 断言一个条件为真 void assertFalse([String message],boolean condition) 断言一个条件为假 void assertNotNull([String message], java.lang.Object object) 断言一个对象不为空(null) void assertNull([String message], java.lang.Object object) 断言一个对象为空(null) void assertSame([String message], java.lang.Object expected, java.lang.Object actual) 断言，两个对象引用相同的对象 void assertNotSame([String message], java.lang.Object unexpected, java.lang.Object actual) 断言，两个对象不是引用同一个对象 void assertArrayEquals([String message], expectedArray, resultArray) 断言预期数组和结果数组相等。数组的类型可能是 int, long, short, char, byte or java.lang.Object. 在以上类中我们可以看到，这些断言方法是可以工作的。 assertEquals() 如果比较的两个对象是相等的，此方法将正常返回；否则失败显示在JUnit的窗口测试将中止。 assertSame() 和 assertNotSame() 方法测试两个对象引用指向完全相同的对象。 assertNull() 和 assertNotNull() 方法测试一个变量是否为空或不为空(null)。 assertTrue() 和 assertFalse() 方法测试if条件或变量是true还是false。 assertArrayEquals() 将比较两个数组，如果它们相等，则该方法将继续进行不会发出错误。否则失败将显示在JUnit窗口和中止测试。 JUnit4套件测试测试套件是一些测试不同类用例，可以使用@RunWith和@Suite注解运行所有东西在一起。如果有很多测试类，想让它们都运行在同一时间，而不是单一地运行每个测试，这是非常有用的。 当一个类被注解为**@RunWith**， JUnit 将调用被在其中注解，以便运行测试类，而不使用内置的 JUnit 运行方法。 快速生成：包右键>>new>>JUnit test Suite>>finish 指定套件类 两个测试类 输出结果 java参数化自动测试一个测试类也可以被看作是一个参数化测试类要满足下列所有要求： 该类被注解为 @RunWith(Parameterized.class). 如前一节中所说明的, @RunWith 注解让JUnit来调用其中的注释来运行测试类，代替使用内置的JUnit运行器，Parameterized 是一个在JUnit内的运行器将运行相同的测试用例组在不同的输入。 这个类有一个构造函数，存储测试数据。 这个类有一个静态方法生成并返回测试数据，并注明@Parameters注解。 这个类有一个测试，它需要注解@Test到方法。 Junit底层通过多次生成新的类进行多次测试 测试结果","date":"2024-12-31","categories":["java"],"tags":["junit4"]},{"title":"junit4-使用整理(3)spring整合","url":"/2024/12/31/junit4-使用整理-3-spring整合/","content":"这里记录一下spring 和 junit4进行整合例子 现在情况下spring支持的junit版本不是很高 使用junit4.1.2进行开发 @RunWith：用于指定junit运行环境，是junit提供给其他框架测试环境接口扩展，为了便于使用spring的依赖注入，spring提供了org.springframework.test.context.junit4.SpringJUnit4ClassRunner作为Junit测试环境 @ContextConfiguration({“classpath:applicationContext.xml”,”classpath:spring&#x2F;buyer&#x2F;applicationContext-service.xml”}) 导入配置文件，这里我的applicationContext配置文件是根据模块来分类的。如果有多个模块就引入多个“applicationContext-service.xml”文件。如果所有的都是写在“applicationContext。xml”中则这样导入","date":"2024-12-31","categories":["java"],"tags":["junit4"]},{"title":"java-深入理解jvm虚拟机-内存分配和垃圾回收策略","url":"/2024/12/31/java-深入理解jvm虚拟机-内存分配和垃圾回收策略/","content":"内存的分配原则 对象优先在Eden分配 : 大多数情况下,对象在新生代Eden区中分配。当Eden区没有足够空间进行分配时,虚拟机将发起一次Minor GC。 大对象直接进入老年代 长期存活的对象将进入老年代 : 对象在Eden出生并经过第一次Minor GC后仍然存活,并且能被Survivor容纳的话,将被移动到Survivor空间中,并且对象年龄设为1。对象在Survivor区中每“熬过”一次Minor GC,年龄就增加1岁,当它的年龄增加到一定程度(默认为15岁),就将会被晋升到老年代中。对象晋升老年代的年龄阈值,可以通过参数**-XX:MaxTenuringThreshold设置** 动态对象年龄判定 : 虚拟机并不是永远地要求对象的年龄必须达到了MaxTenuringThreshold才能晋升老年代,如果在Survivor空间中相同年龄所有对象大小的总和大于Survivor空间的一半,年龄大于或等于该年龄的对象就可以直接进入老年代,无须等到MaxTenuringThreshold中要求的年龄 java两种位置的gc-新生代GC(Minor GC):指发生在新生代的垃圾收集动作,因为Java对象大多都具备朝生夕灭的特性,所以Minor GC非常频繁,一般回收速度也比较快。 老年代GC(Major GC&#x2F;Full GC):指发生在老年代的GC,出现了Major GC,经常会伴随至少一次的Minor GC(但非绝对的,在Parallel Scavenge收集器的收集策略里就有直接进行Major GC的策略选择过程)。Major GC的速度一般会比Minor GC慢10倍以上。 Minor GC 大多数情况下,对象在新生代Eden区中分配。当Eden区没有足够空间进行分配时,虚拟机将发起一次Minor GC。 注意这次的gc只是将对象放入一个survive区中，使用标记整理方法，一个survive区空间为0 而另一个空间无碎片。当survive 没有空间的时候将会直接的放入老年代中 Full gc 在发生Minor GC之前,虚拟机会先检查老年代最大可用的连续空间是否大于新生代所有对象总空间,如果这个条件成立,那么Minor GC可以确保是安全的。如果不成立,则虚拟机会查看HandlePromotionFailure设置值是否允许担保失败。如果允许,那么会继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小,如果大于,将尝试着进行一次Minor GC,尽管这次Minor GC是有风险的;如果小于,或者HandlePromotionFailure设置不允许冒险,那这时也要改为进行一次Full GC。","date":"2024-12-31","categories":["java"],"tags":["jvm虚拟机"]},{"title":"java-深入理解jvm虚拟机-内存划分方法","url":"/2024/12/31/java-深入理解jvm虚拟机-内存划分方法/","content":"一.运行时的数据区域 （1）程序计数器程序计数器(Program Counter Register)是一块较小的内存空间,它可以看作是当前线程所执行的字节码的行号指示器—–编译原理中指向下一条指令的位置 Java虚拟机的多线程是通过线程轮流切换并分配处理器执行时间的方式来实现的，每条线程都需要有一个独立的程序计数器,各条线程之间计数器互不影响,独立存储,我们称这类内存区域为“线程私有”的内存 注意： 如果线程正在执行的是一个Java方法,这个计数器记录的是正在执行的虚拟机字节码指令的地址 如果正在执行的是Native方法,这个计数器值则为空(Undefined) （2）Java虚拟机栈Java虚拟机栈(Java Virtual Machine Stacks)也是线程私有的,它的生命周期与线程相同。 虚拟机栈描述的是Java方法执行的内存模型:每个方法在执行的同时都会创建一个栈帧(Stack Frame )用于存储局部变量表、操作数栈、动态链接、方法出口等信息。每一个方法从调用直至执行完成的过程,就对应着一个栈帧在虚拟机栈中入栈到出栈的过程。 局部变量表存放了编译期可知的各种基本数据类型、对象引用（reference字段）和了一条字节码指令的地址，方法的开始 一个重要的异常 stackOverflowError 当内存不够的时候抛出 OutofMenoryError （3）本地方法栈区和虚拟机栈相似值不过使用的是native的方法服务 栈区大小使用 -Xss进行配置 引申：参数 -XX：PermSize -XX ：MaxPermSize — 指定非堆内存的最大容量 （4）java堆Java堆(Java Heap)是Java虚拟机所管理的内存中最大的一块。Java堆是被所有线程共享的一块内存区域,在虚拟机启动时创建，用来存放对象实例 指定java堆大小的参数 -Xms -Xmx –最大值和最小值 （5）方法区所有线程共享，存储常量静态常量，编译后的各种代码等数据 运行时常量池-方法区的一部分用来储存java的class类的各种其他信息 （6）额外部分 直接内存使用’当java使用NIO的时候，直接使用外部内存而不是使用 虚拟机的虚拟内存","date":"2024-12-31","categories":["java"],"tags":["jvm虚拟机"]},{"title":"java-深入理解jvm虚拟机-垃圾回收","url":"/2024/12/31/java-深入理解jvm虚拟机-垃圾回收/","content":"1.垃圾回收算法技术类型： 引用技术法 问题存在循环引用的问题 可达性分析算法 通过引用进行遍历搜索形成一张引用关系图，将图外的内存进行回收 1.相关算法（1）标记清除算法 老年代有应用 （2）复制算法 主要用在新生代，发生gc的时候将Eden区的数据转移到surver区 （3）标记整理算法 主要用在老年代思想就是标记+移动 （4）分代收集算法针对不同的区域使用合适的垃圾回收算法 新生代 – 复制算法 老年代-标记清楚或者标记整理 当前商业虚拟机的垃圾收集都采用“分代收集”(Generational Collection)算法,这种算法并没有什么新的思想,只是根据对象存活周期的不同将内存划分为几块。一般是把Java堆分为新生代和老年代,这样就可以根据各个年代的特点采用最适当的收集算法。在新生代中,每次垃圾收集时都发现有大批对象死去,只有少量存活,那就选用复制算法,只需要付出少量存活对象的复制成本就可以完成收集。而老年代中因为对象存活率高、没有额外空间对它进行分配担保,就必须使用“标记—清理”或者“标记—整理”算法来进行回收。 2.java的引用类型 强引用就是指在程序代码之中普遍存在的,类似“Object obj&#x3D;new Object()”这类的引用,只要强引用还存在,垃圾收集器永远不会回收掉被引用的对象。 软引用是用来描述一些还有用但并非必需的对象。对于软引用关联着的对象,在系统将要发生内存溢出异常之前,将会把这些对象列进回收范围之中进行第二次回收。如果这次回收还没有足够的内存,才会抛出内存溢出异常。在JDK 1.2之后,提供了SoftReference类来实现软引用。 弱引用也是用来描述非必需对象的,但是它的强度比软引用更弱一些,被弱引用关联的对象只能生存到下一次垃圾收集发生之前。当垃圾收集器工作时，无论当前内存是否足够,都会回收掉只被弱引用关联的对象。在JDK1.2之后,提供了WeakReference类来实现弱引用。 虚引用也称为幽灵引用或者幻影引用,它是最弱的一种引用关系。一个对象是否有虚引用的存在,完全不会对其生存时间构成影响,也无法通过虚引用来取得一个对象实例。为一个对象设置虚引用关联的唯一目的就是能在这个对象被收集器回收时收到一个系统通知。在JDK 1.2之后,提供PhantomReference类来实现虚引用。 3.finilize方法对象在将要被gc的时候才会被触发，可以使用这个方法进行自救行为，但是注意这个方法只能使用一次，这个对象第二次被gc的时候将不会执行这个方法 4.垃圾收集器 讨论的收集器基于JDK 1.7 Update 14之后的HotSpot虚拟机 明确一个观点:虽然我们是在对各个收集器进行比较,但并非为了挑选出一个最好的收集器。因为直到现在为止还没有最好的收集器出现,更加没有万能的收集器,所以我们选择的只是对具体应用最合适的收集器。这点不需要多加解释就能证明:如果有一种放之四海皆准、任何场景下都适用的完美收集器存在,那HotSpot虚拟机就没必要实现那么多不同的收集器了。 （1）Serial收集器（串行收集器）只会使用一个CPU或一条收集线程去完成垃圾收集工作,更重要的是在它进行垃圾收集时,必须暂停其他所有的工作线程,直到它收集结束 适合在单线成环境下进行使用，缺点性能太低 （2）ParNew收集器（串行收集器多线程版本）CMS作为老年代的收集器,却无法与JDK 1.4.0中已经存在的新生代收集器Parallel Scavenge配合工作 ,所以在JDK 1.5中使用CMS来收集老年代的时候,新生代只能选择ParNew或者Serial收集器中的一个是许多运行在Server模式下的虚拟机中首选的新生代收集器,其中有一个与性能无关但很重要的原因是,除了Serial收集器外,目前只有它能与CMS收集器配合工作 （3）Parallel Scavenge收集器特点是它的关注点与其他收集器不同,CMS等收集器的关注点是尽可能地缩短垃圾收集时用户线程的停顿时间,而Parallel Scavenge收集器的目标则是达到一个可控制的吞吐量(Throughput) （4）serial old相当于serial收集器的老年代版本 （5）parold相当于parnew 收集器的老年代版本 （6）CMS收集器CMS(Concurrent Mark Sweep)收集器是一种以获取最短回收停顿时间为目标的收集器。目前很大一部分的Java应用集中在互联网站或者B&#x2F;S系统的服务端上,这类应用尤其重视服务的响应速度,希望系统停顿时间最短,以给用户带来较好的体验。 特点： 绝大部分新生成的对象都放在Eden区，当Eden区将满，JVM会因申请不到内存，而触发Young GC ,进行Eden区+有对象的Survivor区(设为S0区)垃圾回收，把存活的对象用复制算法拷贝到一个空的Survivor(S1)中，此时Eden区被清空，另外一个Survivor S0也为空。下次触发Young GC回收Eden+S0，将存活对象拷贝到S1中。新生代垃圾回收简单、粗暴、高效。 若发现Survivor区满了，则将这些对象拷贝到old区或者Survivor没满但某些对象足够Old,也拷贝到Old区(每次Young GC都会使Survivor区存活对象值+1，直到阈值)。 3.Old区也会进行垃圾收集(Young GC),发生一次 Major GC 至少伴随一次Young GC，一般比Young GC慢十倍以上。 JVM在Old区申请不到内存，会进行Full GC。Old区使用一般采用Concurrent-Mark–Sweep策略回收内存。缺点： GC过程中会出现STW(Stop-The-World)，若Old区对象太多，STW耗费大量时间。 CMS收集器对CPU资源很敏感。 CMS收集器无法处理浮动垃圾，可能出现“Concurrent Mode Failure”失败而导致另一次Full GC的产生。 CMS导致内存碎片问题。 （7）g1垃圾回收在G1中，堆被划分成 许多个连续的区域(region)。每个区域大小相等，在1M~32M之间。JVM最多支持2000个区域，可推算G1能支持的最大内存为2000*32M&#x3D;62.5G。区域(region)的大小在JVM初始化的时候决定，也可以用-XX:G1HeapReginSize设置。 在G1中没有物理上的Yong(Eden&#x2F;Survivor)&#x2F;Old Generation，它们是逻辑的，使用一些非连续的区域(Region)组成的。 新生代收集 G1的新生代收集跟ParNew类似，当新生代占用达到一定比例的时候，开始出发收集。 被圈起的绿色部分为新生代的区域(region)，经过Young GC后存活的对象被复制到一个或者多个区域空闲中，这些被填充的区域将是新的新生代；当新生代对象的年龄(逃逸过一次Young GC年龄增加１)已经达到某个阈值(ParNew默认15)，被复制到老年代的区域中。 回收过程是停顿的(STW,Stop-The-Word);回收完成之后根据Young GC的统计信息调整Eden和Survivor的大小，有助于合理利用内存，提高回收效率。 回收的过程多个回收线程并发收集。 老年代收集 和CMS类似，G1收集器收集老年代对象会有短暂停顿。 标记阶段，首先初始标记(Initial-Mark),这个阶段是停顿的(Stop the World Event)，并且会触发一次普通Mintor GC。对应GC log:GC pause (young) (inital-mark) Root Region Scanning，程序运行过程中会回收survivor区(存活到老年代)，这一过程必须在young GC之前完成。 Concurrent Marking，在整个堆中进行并发标记(和应用程序并发执行)，此过程可能被young GC中断。在并发标记阶段，若发现区域对象中的所有对象都是垃圾，那个这个区域会被立即回收(图中打X)。同时，并发标记过程中，会计算每个区域的对象活性(区域中存活对象的比例)。 Remark, 再标记，会有短暂停顿(STW)。再标记阶段是用来收集 并发标记阶段 产生新的垃圾(并发阶段和应用程序一同运行)；G1中采用了比CMS更快的初始快照算法:snapshot-at-the-beginning (SATB)。 Copy&#x2F;Clean up，多线程清除失活对象，会有STW。G1将回收区域的存活对象拷贝到新区域，清除Remember Sets，并发清空回收区域并把它返回到空闲区域链表中。 复制&#x2F;清除过程后。回收区域的活性对象已经被集中回收到深蓝色和深绿色区域。 关于Remembered Set概念：G1收集器中，Region之间的对象引用以及其他收集器中的新生代和老年代之间的对象引用是使用Remembered Set来避免扫描全堆。G1中每个Region都有一个与之对应的Remembered Set，虚拟机发现程序对Reference类型数据进行写操作时，会产生一个Write Barrier暂时中断写操作，检查Reference引用的对象是否处于不同的Region之间(在分代中例子中就是检查是否老年代中的对象引用了新生代的对象)，如果是便通过CardTable把相关引用信息记录到被引用对象所属的Region的Remembered Set中。当内存回收时，在GC根节点的枚举范围加入Remembered Set即可保证不对全局堆扫描也不会有遗漏。 G1虽然保留了CMS关于代的概念，但是代已经不是物理上连续区域，而是一个逻辑的概念。在标记过程中，每个区域的对象活性都被计算，在回收时候，就可以根据用户设置的停顿时间，选择活性较低的区域收集，这样既能保证垃圾回收，又能保证停顿时间，而且也不会降低太多的吞吐量。Remark阶段新算法的运用，以及收集过程中的压缩，都弥补了CMS不足。引用Oracle官网的一句话：“G1 is planned as the long term replacement for the Concurrent Mark-Sweep Collector (CMS)”。","date":"2024-12-31","categories":["java"],"tags":["jvm虚拟机"]},{"title":"java-深入理解jvm虚拟机-类加载机制","url":"/2024/12/31/java-深入理解jvm虚拟机-类加载机制/","content":"类加载过程类从被加载到虚拟机内存中开始,到卸载出内存为止,它的整个生命周期包括:**加载(Loading)、验证(Verification)、准备(Preparation)、解析(Resolution)、初始化(Initialization)、使用(Using)和卸载(Unloading)**7个阶段。其中验证、准备、解析3个部分统称为连接(Linking) （1）加载阶段触发条件 主动引用 使用new关键字实例化对象的时候、读取或设置一个类的静态字段(被final修饰、已在编译期把结果放入常量池的静态字段除外)的时候,以及调用一个类的静态方法的时候。 使用java.lang.reflect包的方法对类进行反射调用的时候,如果类没有进行过初始化,则需要先触发其初始化。 当初始化一个类的时候,如果发现其父类还没有进行过初始化,则需要先触发其父类的初始化。 当虚拟机启动时,用户需要指定一个要执行的主类(包含main()方法的那个类),虚拟机会先初始化这个主类。 当使用JDK1.7的动态语言支持时,如果一个java.lang.invoke.MethodHandle实例最后的解析结果REF_getStatic、REF_putStatic、REF_invokeStatic的方法句柄,并且这个方法句柄所对应的类没有进行过初始化,则需要先触发其初始化。 对于这5种会触发类进行初始化的场景,虚拟机规范中使用了一个很强烈的限定语:“有且只有”,这5种场景中的行为称为对一个类进行主动引用。除此之外,所有引用类的方式都不会触发初始化,称为被动引用。 被动引用举例 被动使用类字段: 通过子类引用父类的静态字段,不会导致子类初始化 对应上面的触发条件（1），调用了谁的static 触发谁 （2）类加载器 bootstrapClassLoader：加载核心jar Extension ClassLoader：加载lib&#x2F;ext中的jar Application ClassLoader：加载path中的jar User ClassLoader：用户自定义的jar java中存在这么多类加载机制的主要原因就是要保证核心类加载的过程相同，识别相同，因为java 在 判读一个类是否是相同的判断方法除了使用类名称还使用类的加载机制来进行相关的判断的 2.验证阶段验证是连接阶段的第一步,这一阶段的目的是为了确保Class文件的字节流中包含的信息符合当前虚拟机的要求,并且不会危害虚拟机自身的安全 文件格式验证是否是魔数0xCAFEBABE开头 元数据验证 字节码验证 符号引用验证 3.准备阶段准备阶段是正式为类变量分配内存并设置类变量初始值的阶段,这些变量所使用的内存都将在方法区中进行分配。这个阶段中有两个容易产生混淆的概念需要强调一下,首先,这时候进行内存分配的仅包括类变量(被static修饰的变量),而不包括实例变量,实例变量将会在对象实例化时随着对象一起分配在Java堆中。 注意这里初始的是初始值 值的初始化的时间将是在初始化阶段赋值，当前阶段只是使用默认值0 4.解析解析各种类属性 5.初始化最后阶段各种属性最终赋值","date":"2024-12-31","categories":["java"],"tags":["jvm虚拟机"]},{"title":"maven 实战笔记 （二）maven常用插件","url":"/2024/12/31/maven-实战笔记--二-maven常用插件/","content":"","date":"2024-12-31","categories":["java"],"tags":["maven"]},{"title":"maven-实战笔记(1)基本配置","url":"/2024/12/31/maven-实战笔记-1-基本配置/","content":"maven 实战笔记（一）基本配置在java世界中maven应该是应用最为广泛的项目构建工具，之前使用过maven但是没哟进行细致系统的学习，这次开一个专题进行一下系统的整理 maven文件目录结构 bin：maven的运行时目录，放置maven运行时的脚本，目录中还有一个mvnDebug文件，相比较mvn文件添加了调试功能，文件夹中的m2.conf，是classworlds的配置文件 boot：中只有一个文件plexus-classworlds-2.5.2.jar（maven3.5）这个是maven自己的实现的类加载器（这个其实是实现jar下载等功能所必需的） conf：其中的settting.xml文件全局定义maven的行为，在使用上更加倾向于见此文件拷贝至~&#x2F;.m2文件夹中进行私有化定制lib：防止maven的各种依赖文件 一个命令：mvn help:system 打印出所有java系统属性和环境变量 maven使用代理 有的时候需要进行配置相关的代理才能正常的访问外部仓库，maven提供这样的配置 maven pom 文件 maven 项目的核心是 pom文件 定义了项目的基本信息例如下面的配置文件 pom 文件核心标签project标签-表示一个maven项目的最外层标签子标签 标签名称 作用 modelversion 指定POM模型的版本现在只能使用4.0.0。gounpId,artifactId,version,packaging,classifier 三要素，定义了一个项目的坐标，groupId定义项目属于哪一个组，artifactId定义当前项目在组中的唯一id，version指定当前的项目的版本号，packaging指定打包的方式默认使用jar包，classifier用来昂住定义构建输出一些附属构建 name 声名一个对用户友好的名称，不是必须的 dependencies dependenc 标签的副标签，可以包含多个dependency标签，相对于dependencyManagement，所有生命在dependencies里的依赖都会自动引入，并默认被所有的子项目继承。 dependencyManagement 一个依赖管理者一般都是不是在顶层的maven 模块上面，子模块通过继承父maven项目，可以在自己的依赖中不用指明版本号而直接使用父maven项目中的dependencyManagement配置的版本号。在我们项目顶层的POM文件中，我们会看到dependencyManagement元素。通过它元素来管理jar包的版本，让子项目中引用一个依赖而不用显示的列出版本号。 dependencyManagement和dependencies区别 dependencies:即使在子项目中不写该依赖项，那么子项目仍然会从父项目中继承该依赖项（全部继承） dependencyManagement:里只是声明依赖，并不实现引入，因此子项目需要显示的声明需要用的依赖。如果不在子项目中声明依赖，是不会从父项目中继承下来的；只有在子项目中写了该依赖项，并且没有指定具体版本，才会从父项目中继承该项，并且version和scope都读取自父pom;另外如果子项目中指定了版本号，那么会使用子项目中指定的jar版本。 dependency标签-表示一个依赖坐标子标签 标签名称 作用 gounpId,artifactId,version 三要素，定义了一个项目的坐标，groupId定义项目属于哪一个组，artifactId定义当前项目在组中的唯一id，version指定当前的项目的版本号 scope （编译指的是maven进行编译项目）表示依赖范围 1.compile：编译依赖范围 编译测试运行都有效2.test：测试依赖范围 编译和运行时无法使用，测试下使用（junit）3.provided 已提供依赖范围 运行时无效（各种web service依赖） 4.runtime 编译时无效 （如jdbc）5.system 系统依赖范围和provided类似但是 需要使用systenPath标签手动指定依赖范围6,import：一般和dependencyManagement组合使用 type 声名依赖类型，默认是使用jar optional 标记依赖是否可选 exclusions 排除传递性依赖 是定groupId和artifactId将会将指定的依赖驱除掉，见下面例子 repositories标签-添加远程仓库到指定的位置中 聚合与继承（一般一起使用因为父级打包方式强制要求pom方式）聚合：聚合，顾名思义，就是把多个模块或项目聚合到一起，我们可以建立一个专门负责聚合工作的Maven project — aggregator。 建立该project的时候，我们要注意以下几点： 该aggregator本身也做为一个Maven项目，它必须有自己的POM,它的打包方式必须为： pom 引入了新的元素：modules—module 中 写的是地址 版本：聚合模块的版本和被聚合模块版本一致 中指定的版本号 relative path：每个module的值都是一个当前POM的相对目录 目录名称：为了方便的快速定位内容，模块所处的目录应当与其artifactId一致(Maven约定而不是硬性要求)，总之，模块所处的目录必须和模块所处的目录相一致。 也就是说子模块应该在父模块pom.xml文件所在目录的下面的指定的相关目录地址 习惯约定：为了方便构建，通常将聚合模块放在项目目录层的最顶层，其它聚合模块作为子目录存在。这样当我们打开项目的时候，第一个看到的就是聚合模块的POM 聚合模块减少的内容：聚合模块的内容仅仅是一个pom.xml文件，它不包含src&#x2F;main&#x2F;java、src&#x2F;test&#x2F;java等目录，因为它只是用来帮助其它模块构建的工具，本身并没有实质的内容。 聚合模块和子模块的目录：他们可以是父子类，也可以是平行结构，当然如果使用平行结构，那么聚合模块的POM也需要做出相应的更改。 继承：做面向对象编程的人都会觉得这是一个没意义的问题，是的，继承就是避免重复，maven的继承也是这样，它还有一个好处就是让项目更加安全 配置继承： 继承肯定是一个父子结构，那么我们在aggregator中来创建一个parent project 使用 标签 : 作为父模块的POM，其打包类型也必须为POM结构：父模块只是为了帮助我们消除重复，所以它也不需要src&#x2F;main&#x2F;java、src&#x2F;test&#x2F;java等目录 新的元素： ， 它是被用在子模块中的元素的属性：: 表示父模块POM的相对路径，在构建的时候，Maven会先根据relativePath检查父POM，如果找不到，再从本地仓库查找 指定去那里寻找 父pom.xml relativePath的默认值： ..&#x2F;pom.xml 子模块省略groupId和version： 使用了继承的子模块中可以不声明groupId和version, 子模块将隐式的继承父模块的这两个元素 继承的POM元素 groupId:项目组ID,项目坐标的核心元素 version: 项目版本, 项目坐标的核心元素 description: 项目的描述信息 organization: 项目的组织信息 inceptionYear: 项目的创始年份 url: 项目的URL地址 developers: 项目开发者信息 contributors: 项目的贡献者信息 distributionManagement: 项目的部署配置 issueManagement: 项目的缺陷跟踪系统信息 ciManagement: 项目的持续集成系统信息 scm: 项目的版本控制系统信息 mailingLists: 项目的邮件列表信息 properties: 自定义的maven属性 dependencies: 项目的依赖配置 dependencyManagement: 项目的依赖管理配置 repositories: 项目的仓库配置 build: 包括项目的源码目录配置、输出目录配置、插件配置、插件管理配置等 reporting: 包括项目的报告输出目录配置、报告插件配置等 聚合继承下的依赖管理依赖继承管理：dependencyManagement dependencyManagement的特性：在dependencyManagement中配置的元素既不会给parent引入依赖，也不会给它的子模块引入依赖，仅仅是它的配置是可继承的 父pom 配置依赖的版本 子pom 选择性的继承 插件继承管理： 这个元素和相类似，它是用来进行插件管理的。在我们项目开发的过程中，也会频繁的引入插件，所以解决这些复杂配置的方法就是使用插件管理 父pom 配置插件依赖 子pom 选择插件继承 聚合与继承的总结当我们明白聚合与继承的细节之后，我们会发现：对于聚合模块来说，它知道有哪些被聚合的模块，而对于被聚合的模块来说，它们不知道被谁聚合了，也不知道它的存在对于继承关系的父POM来说，它不知道自己被哪些子模块继承了，对于子POM来说，它必须知道自己的父POM是谁在一些最佳实践中我们会发现：一个POM既是聚合POM，又是父POM，这么做主要是为了方便。","date":"2024-12-31","categories":["java"],"tags":["maven"]},{"title":"maven-实战笔记(2)maven常用插件","url":"/2024/12/31/maven-实战笔记-2-maven常用插件/","content":"maven 实战笔记 （二）maven常用插件maven-source-plugin 这样在mvn package后便会在target目录下生成*-sources.jar源码包。 maven-javadoc-plugin 这样在mvn package后便会在target目录下生成*-javadoc.jar文档包。 maven-compiler-plugin指定JDK版本和编码maven 2.1默认用jdk 1.3来编译，maven3 貌似是用jdk 1.5，如果项目用的jdk 1.6也会有问题，compiler插件可以指定JDK版本为1.6。windows默认使用GBK编码，java项目经常编码为utf8，也需要在compiler插件中指出，否则中文乱码可能会出现编译错误。 maven-shade-pluginmvn clean package后，target下可以看到mvn-test-0.0.1-SNAPSHOT.jar和original-mvn-test-0.0.1-SNAPSHOT.jar两个jar包，前者是带有依赖包(会解压)和Main-Class信息的可运行jar包，后者是原始的jar包(不包含依赖及Main-Class信息)。打开mvn-test-0.0.1-SNAPSHOT.jar&#x2F;META-INF&#x2F;MANIFEST.MF可看到Main-Class: com.adu.mvn_test.HelloWorld信息。可运行jar包的执行：$java -jar mvn-test-0.0.1-SNAPSHOT.jar 或者直接双击。","date":"2024-12-31","categories":["java"],"tags":["maven"]},{"title":"mybatis-深入理解mybatis(1)基础配置","url":"/2024/12/31/mybatis-深入理解mybatis-1-基础配置/","content":"title: 深入理解mybatis(一) 基础配置date: 2018-10-09 15:48:08updated: 2018-10-09 15:48:08tags: - mybatis - ormcategories:- 数据库- orm框架- mybatis深入理解mybatis(一) 基础配置MyBatis 是一款优秀的持久层框架，它支持定制化 SQL、存储过程以及高级映射。MyBatis 避免了几乎所有的 JDBC 代码和手动设置参数以及获取结果集。MyBatis 可以使用简单的 XML 或注解来配置和映射原生信息，将接口和 Java 的 POJOs(Plain Old Java Objects,普通的 Java对象)映射成数据库中的记录。 mybatis基础配置和使用 相关标签注意事项typeAliases 别名配置引申如果使用package的标签的时候将会使用将类名首字母小写和对应的对象完整类名映射，可以通过@Alias(name) 注解来更改映射的规则 properties标签除了使用xml进行相关属性配置，还可以使用java api的形式可以使用如下的方法进行配置 使用java代码动态的导入属性 如果属性在不只一个地方进行了配置，那么 MyBatis 将按照下面的顺序来加载： 在 properties 元素体内指定的属性首先被读取。 然后根据 properties 元素中的 resource 属性读取类路径下属性文件或根据 url 属性指定的路径读取属性文件，并覆盖已读取的同名属性。 最后读取作为方法参数传递的属性，并覆盖已读取的同名属性。 因此，通过方法参数传递的属性具有最高优先级，resource&#x2F;url 属性中指定的配置文件次之，最低优先级的是 properties 属性中指定的属性。 使用输入流进行构建 typeHandlers-重写类型处理器或创建你自己的类型处理器来处理不支持的或非标准的类型具体做法为：实现 org.apache.ibatis.type.TypeHandler 接口， 或继承一个很便利的类 org.apache.ibatis.type.BaseTypeHandler， 然后可以选择性地将它映射到一个 JDBC 类型。 mybatis在使用自定义的type的时候，需要在结果属性（resultMap中的property或者id属性中）使用javatype和JDBCtype属性进行相关联，或者直接指定typehandler 枚举类型的特殊处理若想映射枚举类型 Enum，则需要从 EnumTypeHandler 或者 EnumOrdinalTypeHandler 中选一个来使用。默认情况下，MyBatis 会利用 EnumTypeHandler 来把 Enum 值转换成对应的名字。 注意 EnumTypeHandler 在某种意义上来说是比较特别的，其他的处理器只针对某个特定的类，而它不同，它会处理任意继承了 Enum 的类。 自动映射器（auto-mapper）会自动地选用 EnumOrdinalTypeHandler 来处理， 所以如果我们想用普通的 EnumTypeHandler，就必须要显式地为那些 SQL 语句设置要使用的类型处理器。 objectFactory-覆盖对象工厂的默认行为，创建自己的对象工厂。MyBatis 每次创建结果对象的新实例时，它都会使用一个对象工厂（ObjectFactory）实例来完成。 plugins-MyBatis 允许你在已映射语句执行过程中的某一点进行拦截调用默认情况下，MyBatis 允许使用插件来拦截的方法调用包括（格式：类名称（类下的方法））： Executor (update, query, flushStatements, commit, rollback, getTransaction, close, isClosed) ParameterHandler (getParameterObject, setParameters) ResultSetHandler (handleResultSets, handleOutputParameters) StatementHandler (prepare, parameterize, batch, update, query) 通过 MyBatis 提供的强大机制，使用插件是非常简单的，只需实现 Interceptor 接口，并指定了想要拦截的方法签名即可 下面的插件将会拦截在 Executor 实例中所有的 “update” 方法调用， 这里的 Executor 是负责执行低层映射语句的内部对象。注意@Intercepts注解 environments-MyBatis可以通过这个配置项配置成适应多种环境这种机制有助于将 SQL 映射应用于多种数据库之中， 现实情况下有多种理由需要这么做。例如，开发、测试和生产环境需要有不同的配置；或者共享相同 Schema 的多个生产数据库， 想使用相同的 SQL 映射。许多类似的用例。 不过要记住：尽管可以配置多个环境，每个 SqlSessionFactory 实例只能选择其一。为了指定创建哪种环境，只要将它作为可选的参数传递给 SqlSessionFactoryBuilder 即可 在环境配置中有两个重要的配置一个是事务管理器transactionManager，另一个是数据源dataSource （具体的配置信息可以参考之前使用的xml 配置方法） transactionManager事务管理器 任何在 XML 中配置的属性在实例化之后将会被传递给 setProperties() 方法。你也需要创建一个 Transaction 接口的实现类，这个接口也很简单： databaseIdProviderMyBatis 可以根据不同的数据库厂商执行不同的语句，这种多厂商的支持是基于映射语句中的 databaseId 属性。 MyBatis 会加载不带 databaseId 属性和带有匹配当前数据库 databaseId 属性的所有语句。 如果同时找到带有 databaseId 和不带 databaseId 的相同语句，则后者会被舍弃。 mappers表示映射器注意：一个config文件可以导入多个配置文件 可以使用url地址或者resource 引入配置文件或者 使用包名称或者class文件","date":"2024-12-31","categories":["java"],"tags":["mybatis"]},{"title":"mybatis-深入理解mybatis(2)映射文件","url":"/2024/12/31/mybatis-深入理解mybatis-2-映射文件/","content":"深入理解mybatis（二）映射文件MyBatis 的真正强大在于它的映射语句，也是它的魔力所在。由于它的异常强大，映射器的 XML 文件就显得相对简单。如果拿它跟具有相同功能的 JDBC 代码进行对比，你会立即发现省掉了将近 95% 的代码。MyBatis 就是针对 SQL 构建的，并且比普通的方法做的更好。 配置文件总览 SQL 映射文件有很少的几个顶级元素（按照它们应该被定义的顺序）： cache – 给定命名空间的缓存配置。 cache-ref – 其他命名空间缓存配置的引用。 resultMap – 是最复杂也是最强大的元素，用来描述如何从数据库结果集中来加载对象。 parameterMap – 已废弃！老式风格的参数映射。内联参数是首选,这个元素可能在将来被移除，这里不会记录。 sql – 可被其他语句引用的可重用语句块。 insert – 映射插入语句 update – 映射更新语句 delete – 映射删除语句 select – 映射查询语句 标签使用详解select insert update deleteselete标签相关的属性 参数名 参数作用 id 在命名空间中唯一的标识符，可以被用来引用这条语句。 parameterType 将会传入这条语句的参数类的完全限定名或别名。这个属性是可选的，因为 MyBatis 可以通过 TypeHandler 推断出具体传入语句的参数，默认值为 unset。 parameterMap 这是引用外部 parameterMap 的已经被废弃的方法。使用内联参数映射和 parameterType 属性。 resultType 从这条语句中返回的期望类型的类的完全限定名或别名。注意如果是集合情形，那应该是集合可以包含的类型，而不能是集合本身。使用 resultType 或 resultMap，但不能同时使用。 resultMap 外部 resultMap 的命名引用。结果集的映射是 MyBatis 最强大的特性，对其有一个很好的理解的话，许多复杂映射的情形都能迎刃而解。使用 resultMap 或 resultType，但不能同时使用。 flushCache 将其设置为 true，任何时候只要语句被调用，都会导致本地缓存和二级缓存都会被清空，默认值：false。 useCache 将其设置为 true，将会导致本条语句的结果被二级缓存，默认值：对 select 元素为 true。 timeout 这个设置是在抛出异常之前，驱动程序等待数据库返回请求结果的秒数。默认值为 unset（依赖驱动）。 fetchSize 这是尝试影响驱动程序每次批量返回的结果行数和这个设置值相等。默认值为 unset（依赖驱动）。 statementType STATEMENT，PREPARED 或 CALLABLE 的一个。这会让 MyBatis 分别使用 Statement，PreparedStatement 或 CallableStatement，默认值：PREPARED。 resultSetType FORWARD_ONLY，SCROLL_SENSITIVE 或 SCROLL_INSENSITIVE 中的一个，默认值为 unset （依赖驱动）。 databaseId 如果配置了 databaseIdProvider，MyBatis 会加载所有的不带 databaseId 或匹配当前 databaseId的语句；如果带或者不带的语句都有，则不带的会被忽略。 resultOrdered 这个设置仅针对嵌套结果 select 语句适用：如果为 true，就是假设包含了嵌套结果集或是分组了，这样的话当返回一个主结果行的时候，就不会发生有对前面结果集的引用的情况。这就使得在获取嵌套的结果集的时候不至于导致内存不够用。默认值：false。 resultSets 这个设置仅对多结果集的情况适用，它将列出语句执行后返回的结果集并每个结果集给一个名称，名称是逗号分隔的。 insert update delete属性相关标签 参数名 参数作用 id 命名空间中的唯一标识符，可被用来代表这条语句。 parameterType 将要传入语句的参数的完全限定类名或别名。这个属性是可选的，因为 MyBatis 可以通过 TypeHandler parameterMap 这是引用外部 parameterMap 的已经被废弃的方法。使用内联参数映射和 parameterType 属性。 flushCache 将其设置为 true，任何时候只要语句被调用，都会导致本地缓存和二级缓存都会被清空，默认值：true（对应插入、更新和删除语句）。 timeout 这个设置是在抛出异常之前，驱动程序等待数据库返回请求结果的秒数。默认值为 unset（依赖驱动）。 statementType STATEMENT，PREPARED 或 CALLABLE 的一个。这会让 MyBatis 分别使用 Statement，PreparedStatement 或 CallableStatement，默认值：PREPARED。 useGeneratedKeys （仅对 insert 和 update 有用）这会令 MyBatis 使用 JDBC 的 getGeneratedKeys keyProperty （仅对 insert 和 update 有用）唯一标记一个属性，MyBatis 会通过 getGeneratedKeys 的返回值或者通过 insert 语句的 selectKey 子元素设置它的键值，默认：unset。如果希望得到多个生成的列，也可以是逗号分隔的属性名称列表。 keyColumn （仅对 insert 和 update 有用）通过生成的键值设置表中的列名，这个设置仅在某些数据库（像 PostgreSQL）是必须的，当主键列不是表中的第一列的时候需要设置。如果希望得到多个生成的列，也可以是逗号分隔的属性名称列表。 databaseId 如果配置了 databaseIdProvider，MyBatis 会加载所有的不带 databaseId 或匹配当前 databaseId 的语句；如果带或者不带的语句都有，则不带的会被忽略。 注意：useGeneratedKeys keyProperty keyColumn parameterType主要用来进行 自增加主键的 回调操作，就是当进行插入操作的之后，自动生成的主键重新注入到传入的 parameterType对应的javabean中，（底层使用jbdc的getGeneratedKeys方法） selectKey 和 keyProperty keyColumn 结合使用进行主键回填操作在insert 和 update 标签中才会有用的一个标签 会在执行语句之前先进行一次查询，并将得到的数据放入 keyProperty属性指定名称的字段中（调用传入的变量的keyproperty指定名称的地方）作为后面进行操作的数据，一般用在 对于不支持自动生成类型的数据库或可能不支持自动生成主键 JDBC 驱动，来生成主键。 参数名 参数作用 keyProperty selectKey 语句结果应该被设置的目标属性。如果希望得到多个生成的列，也可以是逗号分隔的属性名称列表。 keyColumn 匹配属性的返回结果集中的列名称。如果希望得到多个生成的列，也可以是逗号分隔的属性名称列表。 注意 Property和Column一一对应 resultType 结果的类型。MyBatis 通常可以推算出来，但是为了更加确定写上也不会有什么问题。MyBatis 允许任何简单类型用作主键的类型，包括字符串。如果希望作用于多个生成的列，则可以使用一个包含期望属性的 Object 或一个 Map。 order 这可以被设置为 BEFORE 或 AFTER。如果设置为 BEFORE，那么它会首先选择主键，设置 keyProperty 然后执行插入语句。如果设置为 AFTER，那么先执行插入语句，然后是 selectKey 元素 – 这和像 Oracle 的数据库相似，在插入语句内部可能有嵌入索引调用。 statementType 与前面相同，MyBatis 支持 STATEMENT，PREPARED 和 CALLABLE 语句的映射类型，分别代表 PreparedStatement 和 CallableStatement 类型。 sql标签和include 标签sql和include 标签一般进行组合使用，形成各种参数化配置 结果集映射方法查询语句 结果集 resultmap标签 表示一个映射结果集 相关属性： 参数名 参数作用 id 当前命名空间中的一个唯一标识，用于标识一个result map. type 类的全限定名, 或者一个类型别名 (内置的别名可以参考上面的表格). autoMapping 如果设置这个属性，MyBatis将会为这个ResultMap开启或者关闭自动映射。这个属性会覆盖全局的属性autoMappingBehavior。默认值为：unset。 resultmap标签子标签id&result设置对应结果集中类的各种参数，注意要有set方法，区别 id 表示的标识属性（一般就是表的主键） 相关属性 参数名 参数作用 property 映射到列结果的字段或属性。如果匹配的是存在的,和给定名称相同 的 JavaBeans 的属性,那么就会使用。否则 MyBatis 将会寻找给定名称 property 的字段。这两种情形你可以使用通常点式的复杂属性导航。比如,你 可以这样映射一些东西: “username” ,或者映射到一些复杂的东西: “address.street.number” 。 column 从数据库中得到的列名,或者是列名的重命名标签。这也是通常和会 传递给 resultSet.getString(columnName)方法参数中相同的字符串。 javaType 一个 Java 类的完全限定名,或一个类型别名(参考上面内建类型别名 的列表) 。如果你映射到一个 JavaBean,MyBatis 通常可以断定类型。 然而,如果你映射到的是 HashMap,那么你应该明确地指定 javaType 来保证所需的行为。 jdbcType 在这个表格之后的所支持的 JDBC 类型列表中的类型。JDBC 类型是仅 仅需要对插入,更新和删除操作可能为空的列进行处理。这是 JDBC jdbcType 的需要,而不是 MyBatis 的。如果你直接使用 JDBC 编程,你需要指定 这个类型-但仅仅对可能为空的值。 typeHandler 我们在前面讨论过默认的类型处理器。使用这个属性,你可以覆盖默 认的类型处理器。这个属性值是类的完全限定名或者是一个类型处理 器的实现,或者是类型别名。 constructor标签constructor标签 使用构造方法进行注入，注意必须提供相对应的构造方法，参数的书写顺序和构造函数的顺序相同（3.4.3起可以不同但是要对构造函数添加@Param注解，和启用useActualParamname） 相关属性 参数名 参数作用 column 来自数据库的类名,或重命名的列标签。这和通常传递给 resultSet.getString(columnName)方法的字符串是相同的。 javaType 一个Java 类的完全限定名,或一个类型别名(参考上面内建类型别名的列表)。如果你映射到一个 JavaBean,MyBatis 通常可以断定类型。然而,如 果你映射到的是 HashMap,那么你应该明确地指定 javaType 来保证所需的 行为。 jdbcType 在这个表格之前的所支持的 JDBC 类型列表中的类型。JDBC 类型是仅仅 需要对插入, 更新和删除操作可能为空的列进行处理。这是 JDBC 的需要, jdbcType 而不是 MyBatis 的。如果你直接使用 JDBC 编程,你需要指定这个类型-但 仅仅对可能为空的值。 typeHandler 我们在前面讨论过默认的类型处理器。使用这个属性,你可以覆盖默认的 类型处理器。这个属性值是类的完全限定名或者是一个类型处理器的实现, 或者是类型别名。 select 用于加载复杂类型属性的映射语句的ID,从column中检索出来的数据，将作为此select语句的参数。具体请参考Association标签。 resultMap ResultMap的ID，可以将嵌套的结果集映射到一个合适的对象树中，功能和select属性相似，它可以实现将多表连接操作的结果映射成一个单一的ResultSet。这样的ResultSet将会将包含重复或部分数据重复的结果集正确的映射到嵌套的对象树中。为了实现它, MyBatis允许你 “串联” ResultMap,以便解决嵌套结果集的问题。想了解更多内容，请参考下面的Association元素。 name 构造方法形参的名字。通过指定具体的名字你可以以任意顺序写入arg元素。参看上面的解释。从3.4.3版本起。 cache-二级缓存默认情况下只是默认开启了一级缓存，也就是对同一个sqlsession缓存相关的各种数据，开起二级缓存将会在sqlsessionFactory级别进行缓存，缓存的数据所有的sqlsession共享 参数名 参数作用 eviction 回收策略，LRU – 最近最少使用的:移除最长时间不被使用的对象。 FIFO – 先进先出:按对象进入缓存的顺序来移除它们。 SOFT – 软引用:移除基于垃圾回收器状态和软引用规则的对象。WEAK – 弱引用:更积极地移除基于垃圾收集器状态和弱引用规则的对象。 flushInterval(刷新间隔) 可以被设置为任意的正整数,而且它们代表一个合理的毫秒 形式的时间段。默认情况是不设置,也就是没有刷新间隔,缓存仅仅调用语句时刷新。 size(引用数目) 可以被设置为任意正整数,要记住你缓存的对象数目和你运行环境的 可用内存资源数目。默认值是 1024 readOnly(只读) 属性可以被设置为 true 或 false。只读的缓存会给所有调用者返回缓 存对象的相同实例。因此这些对象不能被修改。这提供了很重要的性能优势。可读写的缓存 会返回缓存对象的拷贝(通过序列化) 。这会慢一些,但是安全,因此默认是 false。 引申mybatis使用自定义缓存除了这些自定义缓存的方式, 你也可以通过实现你自己的缓存或为其他第三方缓存方案 创建适配器来完全覆盖缓存行为。 这个示 例展 示了 如何 使用 一个 自定义 的缓 存实 现。type 属 性指 定的 类必 须实现 org.mybatis.cache.Cache 接口。这个接口是 MyBatis 框架中很多复杂的接口之一,但是简单 给定它做什么就行。 要配置你的缓存, 简单和公有的 JavaBeans 属性来配置你的缓存实现, 而且是通过 cache 元素来传递属性, 比如, 下面代码会在你的缓存实现中调用一个称为 “setCacheFile(String file)” 的方法: 你可以使用所有简单类型作为 JavaBeans 的属性,MyBatis 会进行转换。 And you can specify a placeholder(e.g. ${cache.file}) to replace value defined at configuration properties. 从3.4.2版本开始，MyBatis已经支持在所有属性设置完毕以后可以调用一个初始化方法。如果你想要使用这个特性，请在你的自定义缓存类里实现 org.apache.ibatis.builder.InitializingObject 接口。","date":"2024-12-31","categories":["java"],"tags":["mybatis"]},{"title":"mybatis-深入理解mybatis(3)高级映射文件","url":"/2024/12/31/mybatis-深入理解mybatis-3-高级映射文件/","content":"深入理解mybatis（三）高级映射文件mybatis高级部分解决各种高级映射问题 association 一对一关联 关联中不同的是你需要告诉 MyBatis 如何加载关联。MyBatis 在这方面会有两种不同的 方式: 嵌套查询:通过执行另外一个 SQL 映射语句来返回预期的复杂类型。 嵌套结果:使用嵌套结果映射来处理重复的联合结果的子集。基本属性（和id和resultful相同） 参数属性 属性信息 property 映射到列结果的字段或属性。如果匹配的是存在的,和给定名称相同的 property JavaBeans 的属性, 那么就会使用。否则 MyBatis 将会寻找给定名称的字段。 这两种情形你可以使用通常点式的复杂属性导航。比如,你可以这样映射 一 些 东 西 :“ username ”, 或 者 映 射 到 一 些 复 杂 的 东 西 : “address.street.number” 。 javaType 一个Java 类的完全限定名,或一个类型别名(参考上面内建类型别名的列 表) 。如果你映射到一个 JavaBean,MyBatis 通常可以断定类型。然而,如 javaType 果你映射到的是 HashMap,那么你应该明确地指定 javaType 来保证所需的 行为。 jdbcType 在这个表格之前的所支持的 JDBC 类型列表中的类型。JDBC 类型是仅仅 需要对插入, 更新和删除操作可能为空的列进行处理。这是 JDBC 的需要, jdbcType 而不是 MyBatis 的。如果你直接使用 JDBC 编程,你需要指定这个类型-但 仅仅对可能为空的值。 typeHandler 我们在前面讨论过默认的类型处理器。使用这个属性,你可以覆盖默认的 typeHandler 类型处理器。 这个属性值是类的完全限定名或者是一个类型处理器的实现, 或者是类型别名。 使用嵌套查询方法 参数属性 属性信息 column 来自数据库的类名,或重命名的列标签。这和通常传递给 resultSet.getString(columnName)方法的字符串是相同的。 column 注 意 : 要 处 理 复 合 主 键 , 你 可 以 指 定 多 个 列 名 通 过 column&#x3D; ” {prop1&#x3D;col1,prop2&#x3D;col2} ” 这种语法来传递给嵌套查询语 句。这会引起 prop1 和 prop2 以参数对象形式来设置给目标嵌套查询语句。 select 另外一个映射语句的 ID,可以加载这个属性映射需要的复杂类型。获取的 在列属性中指定的列的值将被传递给目标 select 语句作为参数。表格后面 有一个详细的示例。select 注 意 : 要 处 理 复 合 主 键 , 你 可 以 指 定 多 个 列 名 通 过 column&#x3D; ” {prop1&#x3D;col1,prop2&#x3D;col2} ” 这种语法来传递给嵌套查询语 句。这会引起 prop1 和 prop2 以参数对象形式来设置给目标嵌套查询语句。 fetchType 可选的。有效值为 lazy和eager。如果使用了，它将取代全局配置参数lazyLoadingEnabled。 这种方式很简单, 但是对于大型数据集合和列表将不会表现很好。 问题就是我们熟知的 “N+1 查询问题”。概括地讲,N+1 查询问题可以是这样引起的: 你执行了一个单独的 SQL 语句来获取结果列表(就是“+1”)。 对返回的每条记录,你执行了一个查询语句来为每个加载细节(就是“N”)。 使用嵌套结果方法 参数属性 属性信息 resultMap 这是结果映射的 ID,可以映射关联的嵌套结果到一个合适的对象图中。这 是一种替代方法来调用另外一个查询语句。这允许你联合多个表来合成到 resultMap 一个单独的结果集。这样的结果集可能包含重复,数据的重复组需要被分 解,合理映射到一个嵌套的对象图。为了使它变得容易,MyBatis 让你“链 接”结果映射,来处理嵌套结果。一个例子会很容易来仿照,这个表格后 面也有一个示例。 columnPrefix 当连接多表时，你将不得不使用列别名来避免ResultSet中的重复列名。指定columnPrefix允许你映射列名到一个外部的结果集中。请看后面的例子。 notNullColumn 默认情况下，子对象仅在至少一个列映射到其属性非空时才创建。通过对这个属性指定非空的列将改变默认行为，这样做之后Mybatis将仅在这些列非空时才创建一个子对象。可以指定多个列名，使用逗号分隔。默认值：未设置(unset)。 autoMapping 如果使用了，当映射结果到当前属性时，Mybatis将启用或者禁用自动映射。该属性覆盖全局的自动映射行为。 注意它对外部结果集无影响，所以在select or resultMap属性中这个是毫无意义的。默认值：未设置(unset)。 使用columnPrefix进行去重操作 注意使用了新列CA×× 包含一个新的前缀co_ 再次调用Author的resultMap将定义如下： 因为结果中的列名与resultMap中的列名不同。 你需要指定columnPrefix去重用映射co-author结果的resultMap。 集合collection一对多关联和一对一关联的唯一差别就是有一个oftype属性，这个属性表示的List中xxx的类型 最后一定要注意一点无论是一对一还是一对多一定要写 主键关联 否则将会导致整个结果集扫描进行属性关联，严重降低性能 鉴别器有时一个单独的数据库查询也许返回很多不同 (但是希望有些关联) 数据类型的结果集。 鉴别器元素就是被设计来处理这个情况的, 还有包括类的继承层次结构。 鉴别器非常容易理 解,因为它的表现很像 Java 语言中的 switch 语句。 同上面的例子 使用外部关联版 结合版","date":"2024-12-31","categories":["java"],"tags":["mybatis"]},{"title":"mybatis-深入理解mybatis(4)动态sql","url":"/2024/12/31/mybatis-深入理解mybatis-4-动态sql/","content":"深入理解mybatis（四）动态sql通常使用动态 SQL 不可能是独立的一部分,MyBatis 当然使用一种强大的动态 SQL 语言来改进这种情形,这种语言可以被用在任意的 SQL 映射语句中。 MyBatis 3 大大提升了它们,现在用不到原先一半的元素就可以了。MyBatis 采用功能强大的基于 OGNL 的表达式来消除其他元素。 if choose (when, otherwise) trim (where, set) foreach 引申:ognl表达式介绍 MyBatis常用OGNL表达式 e1 or e2 e1 and e2 e1 &#x3D;&#x3D; e2,e1 eq e2 e1 !&#x3D; e2,e1 neq e2 e1 lt e2：小于 e1 lte e2：小于等于，其他gt（大于）,gte（大于等于） e1 in e2 e1 not in e2 e1 + e2,e1 * e2,e1&#x2F;e2,e1 – e2,e1%e2 !e,not e：非，求反 e.method(args)调用对象方法 e.property对象属性值 e1[ e2 ]按索引取值，List,数组和Map @class@method(args)调用类的静态方法 @class@field调用类的静态字段值 mybatis中的# 和 $ ‘#’会将字符串进行转译 ‘$’直接进行字符串替换 if 关键字 choose, when, otherwise 关键字相当于java中的switch语句 trim, where, set 关键字解决拼接字符串的时候，where and 等字段多余的问题 比如这样的语句 如果这些条件没有一个能匹配上，最终的结果竟会变成这样 使用where标签包裹将会解决这个问题 类似的 trim 相比 where 只是多了定制属性 参数 作用 prefix 指定包裹块生成sql之后要添加的前缀 prefixOverrides 忽略第一个指定的标识符，可以使用 suffix 指定包裹块生成sql之后要添加的后缀 suffixOverrides 忽略最后一个指定的标识符，可以使用 set语句一般用在进行动态添加列属性 foreach动态 SQL 的另外一个常用的必要操作是需要对一个集合进行遍历，通常是在构建 IN 条件语句的时候。比如： foreach 元素的功能是非常强大的，它允许你指定一个集合，声明可以用在元素体内的集合项和索引变量。它也允许你指定开闭匹配的字符串以及在迭代中间放置分隔符。这个元素是很智能的，因此它不会偶然地附加多余的分隔符。 注意：你可以将任何可迭代对象（如列表、集合等）和任何的字典或者数组对象传递给foreach作为集合参数。当使用可迭代对象或者数组时，index是当前迭代的次数，item的值是本次迭代获取的元素。当使用字典（或者Map.Entry对象的集合）时，index是键，item是值。","date":"2024-12-31","categories":["java"],"tags":["mybatis"]},{"title":"mybatis-深入理解mybatis(5)java api","url":"/2024/12/31/mybatis-深入理解mybatis-5-java-api/","content":"深入理解mybatis（五）java apiSqlSessionFactoryBuilder、SqlSessionFactory、SqlSession可以说每个MyBatis都是以一个SqlSessionFactory实例为中心的。SqlSessionFactory实例可以通过SqlSessionFactoryBuilder来构建。一是可以通过XML配置文件的方式来构建SqlSessionFactory，二是可以通过Java API的方式来构建。但不管通过什么方式都有一个Configuration贯穿始终，各种配置正是通过Configuration实例来完成实现。 sqlSessionfactoryBuiler生成SqlSessionFactory的方法 使用java类配置实现(SqlSessionFactoryBuilder) 使用xml文档 输入流实现 SqlSessionFactory 创建SqlSession实例 相关参数配置： 事物隔离级别 NONE(无),READ_UNCOMMITTED(读未提交),READ_COMMITTED(读已提交),REPEA TABLE_READ(可重复阅读),SERIALIZA BLE(序列化) ExecutorType(执行方式) ExecutorType.SIMPLE: 这个执行器类型不做特殊的事情。它为每个语句的执行创建一个新的预处理语句。 ExecutorType.REUSE: 这个执行器类型会复用预处理语句。 ExecutorType.BATCH: 这个执行器会批量执行所有更新语句,如果 - - SELECT 在它们中间执行还会标定它们是 必须的,来保证一个简单并易于理解的行为。 SqlSession 最核心的会话操作类 语句执行方法 这些方法被用来执行定义在 SQL 映射的 XML 文件中的 SELECT,INSERT,UPDA E T 和 DELETE 语句。它们都会自行解释,每一句都使用语句的 ID 属性和参数对象,参数可以 是原生类型(自动装箱或包装类) ,JavaBean,POJO 或 Map。 注意：selectOne 和 selectList 的不同仅仅是 selectOne 必须返回一个对象。 如果多余一个, 或者 没有返回 (或返回了 null) 那么就会抛出异常。 , 如果你不知道需要多少对象, 使用 selectList。 最后,还有查询方法的三个高级版本,它们允许你限制返回行数的范围,或者提供自定 义结果控制逻辑,这通常用于大量的数据集合。 RowBounds 参数会告诉 MyBatis 略过指定数量的记录,还有限制返回结果的数量。 ResultHandler 参数允许你按你喜欢的方式处理每一行。 它的接口很简单。 ResultContext 参数允许你访问结果对象本身、被创建的对象数目、以及返回值为 Boolean 的 stop 方法，你可以使用此 stop 方法来停止 MyBatis 加载更多的结果。使用 ResultHandler 的时候需要注意以下两种限制： 从被 ResultHandler 调用的方法返回的数据不会被缓存。 当使用结果映射集（resultMap）时，MyBatis 大多数情况下需要数行结果来构造外键对象。如果你正在使用 ResultHandler，你可以给出外键（association）或者集合（collection）尚未赋值的对象。 批量立即更新方法(Flush Method) 有一个方法可以刷新（执行）存储在JDBC驱动类中的批量更新语句。当你将ExecutorType.BATCH作为ExecutorType使用时可以采用此方法。 事务控制方法 注意：如果你已经选择了自动提交或你正在使用外部事务管理器,这就没有任何效果了 清理Session级的缓存 SqlSession 实例有一个本地缓存在执行 update,commit,rollback 和 close 时被清理。要 明确地关闭它(获取打算做更多的工作) ,你可以调用 clearCache()。 确保 SqlSession 被关闭 高级应用 使用映射器 和常用注释操作 type是mybatis的映射器类，是一个java接口，可以将接口类比成mybatis的map文件，包名称相当于xml中的namespace，而 select delete insert update 中的id属性 对应的 接口中的方法 最常用注解，用来进行动态的注入属性 注解 使用对象 相对应的XML 描述 @CacheNamespace 类 <cache> 为给定的命名空间（比如类）配置缓存。属性有：implemetation,eviction,flushInterval,size,readWrite,blocking和properties。 @Property N&#x2F;A <property> 指定参数值或占位值（placeholder）（能被mybatis-config.xml内的配置属性覆盖）。属性有：name,value。（仅在MyBatis3.4.2以上版本生效） @CacheNamespaceRef 类 <cacheRef> 参照另外一个命名空间的缓存来使用。属性有：value,name。如果你使用了这个注解，你应设置value或者name属性的其中一个。value属性用于指定Java类型而指定命名空间（命名空间名就是指定的Java类型的全限定名），name属性（这个属性仅在MyBatis3.4.2以上版本生效）直接指定了命名空间的名字。 @ConstructorArgs 方法 <constructor> 收集一组结果传递给一个结果对象的构造方法。属性有：value，它是形式参数数组。 @Arg N&#x2F;A <arg> 单参数构造方法，是ConstructorArgs集合的一部分。属性有：id,column,javaType,jdbcType,typeHandler,select和resultMap。id属性是布尔值，来标识用于比较的属性，和<idArg>XML元素相似。 <idArg> @TypeDiscriminator 方法 <discriminator> 一组实例值被用来决定结果映射的表现。属性有：column,javaType,jdbcType,typeHandler和cases。cases属性是实例数组。 @Case N&#x2F;A <case> 单独实例的值和它对应的映射。属性有：value,type,results。results属性是结果数组，因此这个注解和实际的ResultMap很相似，由下面的Results注解指定。 @Results 方法 <resultMap> 结果映射的列表，包含了一个特别结果列如何被映射到属性或字段的详情。属性有：value,id。value属性是Result注解的数组。这个id的属性是结果映射的名称。 @Result N&#x2F;A <result> 在列和属性或字段之间的单独结果映射。属性有：id,column,javaType,jdbcType,typeHandler,one,many。id属性是一个布尔值，来标识应该被用于比较（和在XML映射中的<id>相似）的属性。one属性是单独的联系，和<association>相似，而many属性是对集合而言的，和<collection>相似。它们这样命名是为了避免名称冲突。 <id> @One N&#x2F;A <association> 复杂类型的单独属性值映射。属性有：select，已映射语句（也就是映射器方法）的全限定名，它可以加载合适类型的实例。fetchType会覆盖全局的配置参数lazyLoadingEnabled。注意联合映射在注解API中是不支持的。这是因为Java注解的限制,不允许循环引用。 @Many N&#x2F;A <collection> 映射到复杂类型的集合属性。属性有：select，已映射语句（也就是映射器方法）的全限定名，它可以加载合适类型的实例的集合，fetchType会覆盖全局的配置参数lazyLoadingEnabled。注意联合映射在注解API中是不支持的。这是因为Java注解的限制，不允许循环引用 @MapKey 方法 这是一个用在返回值为Map的方法上的注解。它能够将存放对象的List转化为key值为对象的某一属性的Map。属性有：value，填入的是对象的属性名，作为Map的key值。 @Options 方法 映射语句的属性 这个注解提供访问大范围的交换和配置选项的入口，它们通常在映射语句上作为属性出现。Options注解提供了通俗易懂的方式来访问它们，而不是让每条语句注解变复杂。属性有：useCache&#x3D;true,flushCache&#x3D;FlushCachePolicy.DEFAULT,resultSetType&#x3D;FORWARD_ONLY,statementType&#x3D;PREPARED,fetchSize&#x3D;-1,timeout&#x3D;-1,useGeneratedKeys&#x3D;false,keyProperty&#x3D;”id”,keyColumn&#x3D;””,resultSets&#x3D;””。值得一提的是，Java注解无法指定null值。因此，一旦你使用了Options注解，你的语句就会被上述属性的默认值所影响。要注意避免默认值带来的预期以外的行为。 注意：keyColumn属性只在某些数据库中有效（如Oracle、PostgreSQL等）。请在插入语句一节查看更多关于keyColumn和keyProperty两者的有效值详情。 @Insert 方法 <insert> 这四个注解分别代表将会被执行的SQL语句。它们用字符串数组（或单个字符串）作为参数。如果传递的是字符串数组，字符串之间先会被填充一个空格再连接成单个完整的字符串。这有效避免了以Java代码构建SQL语句时的“丢失空格”的问题。然而，你也可以提前手动连接好字符串。属性有：value，填入的值是用来组成单个SQL语句的字符串数组。 @Update <update> @Delete <delete> @Select <select> @InsertProvider 方法 <insert> 允许构建动态SQL。这些备选的SQL注解允许你指定类名和返回在运行时执行的SQL语句的方法。（自从MyBatis3.4.6开始，你可以用CharSequence代替String来返回类型返回值了。）当执行映射语句的时候，MyBatis会实例化类并执行方法，类和方法就是填入了注解的值。你可以把已经传递给映射方法了的对象作为参数，”Mapperinterfacetype”和”Mappermethod”会经过ProviderContext（仅在MyBatis3.4.5及以上支持）作为参数值。（MyBatis3.4及以上的版本，支持多参数传入）属性有：type,method。type属性需填入类。method需填入该类定义了的方法名。注意接下来的小节将会讨论类，能帮助你更轻松地构建动态SQL。 @UpdateProvider <update> @DeleteProvider <delete> @SelectProvider <select> @Param 参数 N&#x2F;A 如果你的映射方法的形参有多个，这个注解使用在映射方法的参数上就能为它们取自定义名字。若不给出自定义名字，多参数（不包括RowBounds参数）则先以”param”作前缀，再加上它们的参数位置作为参数别名。例如#{param1},#{param2}，这个是默认值。如果注解是@Param(“person”)，那么参数就会被命名为#{person}。 @SelectKey 方法 <selectKey> 这个注解的功能与<selectKey>标签完全一致，用在已经被@Insert或@InsertProvider或@Update或@UpdateProvider注解了的方法上。若在未被上述四个注解的方法上作@SelectKey注解则视为无效。如果你指定了@SelectKey注解，那么MyBatis就会忽略掉由@Options注解所设置的生成主键或设置（configuration）属性。属性有：statement填入将会被执行的SQL字符串数组，keyProperty填入将会被更新的参数对象的属性的值，before填入true或false以指明SQL语句应被在插入语句的之前还是之后执行。resultType填入keyProperty的Java类型和用Statement、PreparedStatement和CallableStatement中的STATEMENT、PREPARED或CALLABLE中任一值填入statementType。默认值是PREPARED。 @ResultMap 方法 N&#x2F;A 这个注解给@Select或者@SelectProvider提供在XML映射中的<resultMap>的id。这使得注解的select可以复用那些定义在XML中的ResultMap。如果同一select注解中还存在@Results或者@ConstructorArgs，那么这两个注解将被此注解覆盖。 @ResultType 方法 N&#x2F;A 此注解在使用了结果处理器的情况下使用。在这种情况下，返回类型为void，所以Mybatis必须有一种方式决定对象的类型，用于构造每行数据。如果有XML的结果映射，请使用@ResultMap注解。如果结果类型在XML的<select>节点中指定了，就不需要其他的注解了。其他情况下则使用此注解。比如，如果@Select注解在一个将使用结果处理器的方法上，那么返回类型必须是void并且这个注解（或者@ResultMap）必选。这个注解仅在方法返回类型是void的情况下生效。 @Flush 方法 N&#x2F;A 如果使用了这个注解，定义在Mapper接口中的方法能够调用SqlSession#flushStatements()方法。（Mybatis3.3及以上）","date":"2024-12-31","categories":["java"],"tags":["mybatis"]},{"title":"mybatis-深入理解mybatis(6)mybatis插件和底层实现原理","url":"/2024/12/31/mybatis-深入理解mybatis-6-mybatis插件和底层实现原理/","content":"深入理解mybatis（六）mybatis插件和底层实现原理插件（plugins）MyBatis 允许你在已映射语句执行过程中的某一点进行拦截调用。默认情况下，MyBatis 允许使用插件来拦截的方法调用包括： Executor (update, query, flushStatements, commit, rollback, getTransaction, close, isClosed) ParameterHandler (getParameterObject, setParameters) ResultSetHandler (handleResultSets, handleOutputParameters) StatementHandler (prepare, parameterize, batch, update, query) 这些类中方法的细节可以通过查看每个方法的签名来发现，或者直接查看 MyBatis 的发行包中的源代码。 假设你想做的不仅仅是监控方法的调用，那么你应该很好的了解正在重写的方法的行为。 因为如果在试图修改或重写已有方法的行为的时候，你很可能在破坏 MyBatis 的核心模块。 这些都是更低层的类和方法，所以使用插件的时候要特别当心。 通过 MyBatis 提供的强大机制，使用插件是非常简单的，只需实现 Interceptor 接口，并指定了想要拦截的方法签名即可。","date":"2024-12-31","categories":["java"],"tags":["mybatis"]},{"title":"mybatis-深入理解mybatis(7)mybatis和spring结合","url":"/2024/12/31/mybatis-深入理解mybatis-7-mybatis和spring结合/","content":"深入理解mybatis（七）mybatis和spring结合实现mybatis和spring的整合的时候注意spring在和mybatis整合的时候需要使用额外的依赖配置 SqlSessionFactoryBean在基本的 MyBatis 中,session 工厂可以使用 SqlSessionFactoryBuilder 来创建。而在 MyBatis-Spring 中,则使用 SqlSessionFactoryBean 来替代。 要注意这个配置文件不需要是一个完整的 MyBatis 配置。确切地说,任意环境,数据源 和 MyBatis 的事务管理器都会被忽略。SqlSessionFactoryBean 会创建它自己的,使用这些 值定制 MyBatis 的 Environment 时是需要的。 如果MyBatis映射器XML文件在和映射器类相同的路径下不存在,那么另外一个需要 配置文件的原因就是它了。使用这个配置,有两种选择。第一是手动在 MyBatis 的 XML 配 置文件中使用部分来指定类路径。第二是使用工厂bean的mapperLocations属性。 以上配置文件相当于以下java语句 要注意 SqlSessionFactoryBean 实现了 Spring 的 FactoryBean 接口(请参考 Spring 文 档的 3.8 章节)这就说明了由 Spring 最终创建的 bean 不是 SqlSessionFactoryBean 本身, 。 而是工厂类的 getObject()返回的方法的结果。 添加事物管理很简单只要将声明并且传入SqlSessionFactoryBean中的datasource添加上相关事务性操作的就好了 和 一般的spring事务配置相同 使用 SqlSessionTemplate 代替sqlSessionSqlSessionTemplate 对象可以使用 SqlSessionFactory 作为构造方法的参数来代替Sqlsession。应为它实现了Sqlsession接口&#96; 将SqlSession配置到bean中 java代码使用注入进来的SqlSession SqlSessionTemplate 有一个使用 ExecutorType 作为参数的构造方法。这允许你用来 创建对象,比如,一个批量 SqlSession,但是使用了下列 Spring 配置的 XML 文件: 现在你所有的语句可以批量操作了,下面的语句就可以在 DAO 中使用了。 MapperFactoryBean为了代替手工使用 SqlSessionDaoSupport 或 SqlSessionTemplate 编写数据访问对象 (DAO)的代码,MyBatis-Spring 提供了一个动态代理的实现:MapperFactoryBean。这个类 可以让你直接注入数据映射器接口到你的 service 层 bean 中。当使用映射器时,你仅仅如调 用你的 DAO 一样调用它们就可以了,但是你不需要编写任何 DAO 实现的代码,因为 MyBatis-Spring 将会为你创建代理。 配置方法 通过配置得到的bean等价于使用sqlSession.getMapper(xxx.class);方法的返回结果 注意：传入的接口信息，应该可以在传入的SqlSessionFactory配置的mapper映射文件中有相应的namespace 注意,当 MapperFactoryBean 需要 SqlSessionFactory 或 SqlSessionTemplate 时。 这些可以通过各自的 SqlSessionFactory 或 SqlSessionTemplate 属性来设置, 或者可以由 Spring 来自动装配。如果两个属性都设置了,那么 SqlSessionFactory 就会被忽略,因为 SqlSessionTemplate 是需要有一个 session 工厂的设置; 那个工厂会由 MapperFactoryBean. 来使用。 可以直接在 business&#x2F;service 对象中以和注入任意 Spring bean 的相同方式直接注入映 射器: 使用MapperScannerConfigurer简化配置使用mapperFactoryBean的时候感觉每一个映射接口都要进行相关的配置，所以我们需要为一个可以自动进行所有配置的整合配置方法 这个注解可以替代mapperFactoryBean，而且，只要指定包名称，框架将会自动按照spring注解的命名规则将相关接口注入到spring的命名空间中（可以使用spring的自动扫描注解，指定映射名称） 2018年8月补充 通过阅读源代码发现 @Mapper这个注解在spring-mybatis和mybatis 本身中更本不起道任何作用，只有在spring boot中有过使用（不过大多数的都是直接使用@MapperScan） 一个实例 mybatis的相关配置文件 mybatis-config.xml mymapper.xml bean相关配置文件和数据库连接池相关配置 datasource.properties bean.xml 使用mapperconfig进行配置 javabean–org.user包 mybatis映射接口-org.dao包 这里要注意：mybatis参数传递的规则，默认只能传入一个类，如果是基本类型 使用#{xxx}(xxx为任意字段)就可以进行导入，如果是一个类#{xxx}(xxx为类中的任意属性)，当使用@Param注解时候，使用的时候必须相对应的名称（底层mybatis 把他参数变成map类型了） 数据库应用服务类-org.service main函数","date":"2024-12-31","categories":["java"],"tags":["mybatis"]},{"title":"mybatis-深入理解mybatis(8)mybatis反射层设计","url":"/2024/12/31/mybatis-深入理解mybatis-8-mybatis反射层设计/","content":"mybatis 整体反射框架设计 可以看到，Mybatis对这一块抽象的比较复杂，我们可以看到有几个比较主要的部分：Reflector、Invoker、PropertyTokenizer、MetaClass，MetaObject和ObjectWrapper，下面我们一个一个解析这几个部分，最后合并在一起看看他们是如何协作工作的。 Reflector我对Reflector的理解是 Reflector是对类的描述 ，我们从一段UT开始（代码位于ReflectorTest）： 这个测试方法首先创建了个ReflectorFactory对象，然后用这个factory创建了一个Section类的Reflector，然后判断Section类中id的setter方法是Long类型的。 ReflectorFactory是对Reflector做的一个简单的工厂，提供类反射的缓存（所以反射这块的开销基本上可以不计了，既灵活又快捷） DefaultReflectorFactory 是 Reflector的默认实现类，用一个ConcurrentMap缓存所有的Reflector示例，它的findForClass方法如下，它首先尝试从map中获取Reflector，获取失败调用Reflector的构造方法创建示例，缓存并返回： 之后是Reflector的构造方法： 这一块不再细细展开，方法名见名知义，首先将type成员设置为原始的class对象，之后获取class的构造方法，getter&#x2F;setter属性，成员字段，之后将属性名转大写存放到caseInsensitivePropertyMap中，为了后面的查找，大小写不敏感。 Reflector的其他方法就是对我们保存的这些类的描述做查找， 其中有两个特别的，也就是我们接下来要讨论的 getSetInvoker和 getGetInvoker InvokerInvoker，顾名思义，就是调用，可以调用的东西，他有一个invoke方法，意思就是调用，参数是target和args，就是调用的对象和调用的参数。 我们来看下它的几个实现类： MethodInvoker: 方法调用 SetFieldInvoker：Setter方法调用 GetFieldInvoker：Getter方法调用MethodInvoker中invoke方法的实现： 就是简单的method.invoke PropertyTokenizer这个就比较牛逼了，他可以处理属性表达式，PropertyTokenizer还实现了Iterator接口，这意味着他可以处理复杂的嵌套属性 字段的含义，name表示当前对象的名字，indexedName是当前对象的名字加上后面的索引（[]）如果有的话，index是索引下标，children是延伸属性（子对象）比如：用PropertyTokenizer去解析 “richType.richList[0].value”，那么 name&#x3D;richType, indexedName&#x3D;richType，index&#x3D;null，children&#x3D;richList[0].value之后执行tokenizer.next()得到新的tokenizer，此时 name&#x3D;richList, indexdName&#x3D;richList[0],index&#x3D;0, children&#x3D;value之后我们会结合MetaClass和MetaObject看看他有多牛逼 MetaClassMetaClass实际上是对Reflector和ProeprtyTokenizer的一种结合，是我们可以用复杂的属性表达式来获取类型的描述。 同样的，我们结合UT来看看它是怎样工作的，首先是一个示例的复杂类型 RichType 这段代码说明了metaClass.hasGetter方法可以接受一个复杂的属性表达式来找到对应的类型描述（利用PropertyTokenizer），这个神奇的功能是这么实现的： 首先检查tokenizer的name字段对应的属性是不是有getter方法，之后迭代子属性，直到最后，children为空。 MetaClass中的还有几个方法的实现和这个类似，hasSetter, getGetterType, getSetterType 以上都是类级别的反射抽象，下面看看对象级别的 ObjectWrapperObjectWrapper是对对象的描述的抽象，它抽象出一系列对对象描述的查询和更新的接口 ObjectWrapper有个几个实现类 BeanWrapper，包装Javabean的描述， MapWrapper，包装Map（键值对）的描述 CollectionWrapper，包装Collection（集合）的描述 ObjectWrapperFactory 了操作实例化ObjectWrapper的工厂方法的抽象，可自定义实现 MetaObjectMetaObject也是对对象的描述，它代理了objectWrapper的大部分方法，和MetaClass一样，它利用PropertyTokenizer做了对复杂属性表达式的处理 MetaObject有5个成员字段， originalObject：原始对象， objectWrapper，被包装的objectWrapper， objectFactory，对象创建工厂，用于在setValue方法中创建不存在的对象属性实例， objectWrapperFactory，创建特定的objectWrapper， reflectorFactory，用来自定一反射类生成的，如果需要可以自行的扩展 协作反射层的类互相协作，最终根据入参制作出来一个完美的MetaObject和MetaClass给其他组件使用，这其中，比较重要的方法有： Configuration.newMetaObject，根据传入的object和配置的factory创建对象描述 实际上，ObjectFactory，ObjectWrapperFactory，ReflectorFactory是可以在XML中配置成自定义的，工厂对象全局单例（Configuration对象中） XMlConfigBuilder.settingsAsProperties方法使用MetaClass检查Properties参数有没有非法的key MetaObject和MetaClass在Session的执行周期（executor, mapping, builder…）中还具有广泛的应用","date":"2024-12-31","categories":["java"],"tags":["mybatis"]},{"title":"mybatis-深入理解mybatis(9)mybatis运行原理和分析","url":"/2024/12/31/mybatis-深入理解mybatis-9-mybatis运行原理和分析/","content":"mybatis 构建过程sessionfactory 构建sqlsessionmybatis 使用sqlSession同数据库进行相关的操作所以在使用之前需要获取这个类，在mybatis 中使用sessionfactory 进行 通常情况下，mybatis使用的是SqlSessionFactoryBuilder的builder方法获取sessionfacotry ，其实这是一种封装，查看源代码可知，其实他是封装了一个DefaultSqlSessionFactory类,通过传入的Config进行初始化的 其实通过上面的分析就能知道其实在使用的Configuration类（这个类包含了mybatis几乎所有的配置），来初始化SqlSessionfacotory的 configuration对象mybatis总体的配置文件包括如下的部分 propertise：参数名称 setting ：设置 typeAliases：别名 typeHandler：类型处理器 ObjectFacotry: 返回值处理工厂对象 plugin：插件 enviroment：环境变量 DataBaseIdProivder：数据库标识 Mapper：映射器 mybatis最核心运行过程SqlSession首先看一下SqlSession这个接口的源代码 SqlSession定义了对数据库的基本操作，CURD操作，其中有一个方法需要注意getMapper 这个方法将会返回一个Mapper接口对象，通过这个对象的方法就可以进行增删改从而并不需要使用原来的方法进行处理,mapper此时就成为操作数据库的方法了，接下来分析一下mapper 深入分析mapper初始化机制 mybatis的mapper是在一开始的时候就通过config就生成了 留心configuration 对象中的mapperRegistry 这个属性的对象，mybatis初始化所有的mapper都储存在这个对象之中 跟进addMapper方法可以发现，这个方法的处理方式其实是让class作为健，生成一个新的代理对象MapperProxyFactory作为值值存入knownMappers这个HashMap 深入分析mapper的初始化调用机制在SqlSession.getMapper获取Mapper的时候同样是通过这个方式反方向方法获取MapperProxyFactory这个对象 从代码中可以看看出来其实此时使用的mapper是通过proxyfactory生成的代理对象 我们都知道代理是指定invoke方法来实现回调的这里我们跟进一下看一下mapperProxy的代码 其实跟踪到这里已经非常明确的 其实他是使用了mapperMethod.execute(sqlSession, args)来执行语句，看一下 代码逻辑非常清晰其实就是使用sqlSession的对应方法，mapper的报名加上类名构成了一开始的名称 分析SqlSession运行相关数据库操作的详细分析通过源代码分析其实最终情况下sql将操作分成了两种类型 doupdate类型和query这里先看一下query 真正执行相关的query操作的方法是deQuery方法(executor类的doQuery) 从上面的方法中我们可以看见一个方法叫MappedStatement,观察一下他的产生 MappedStatement 产生过程通过获取方法可以判断出,这个类其实是一开就在configration配置成功的 如果跟踪一开始初始化的状态需要跟踪到SqlSessionFactoryBuilder类中,一开始mybatiss将所有状态初始化的过程 SqlSessionFactoryBuilder在默认情况下会使用XMLConfigBuilder来生成configration 接下来跟踪进去发现了初始化一个类XPathParser,并且使用嵌套构造函数的方法进行方法嵌套使用 这个类其实就是将xml文件整理成node节点的形式方便之后的调用,其实是一个解析用的工具 然后上调用者(sqlSessionFactoryBuilder)将会调用XMLConfigBuilder的parse方法进行生产configration对象这里才是整个mybatis配置真正产生的地方 终于到了关键的地方MappedStatement的生成部分,MappedStatement由名称就能知道是从mapperElement()方法中产生的 在XMLConfigBuilder中定义的这个方法 我截取了一段代码这段代码是处理使用包名扫描mapper的代码,进入到其中的configuration.addMappers(mapperPackage);方法,发现这里使用了configration中的addMappers方法,进入到mapperRegistry类的这个方法中 跟踪到这里其实思路比较清晰了,这里就是初始化一个mapper和mapper中各种mathod的地方（MapperRegistry） 注意到MapperStatues和Method是一一对应的 而MappedStatement的初始化就在这个MapperAnnotationBuilder.parse() 从上面我们就可以得出MapperStatement的作用就是配置一个sql的各种信息，比如操作方法（select|upadte等），配置的sql ， sql的id ，缓存信息，resultMap，parameterType，resultType，languageDriver，等","date":"2024-12-31","categories":["java"],"tags":["mybatis"]},{"title":"mybatis-源码阅读记录(1)-building包","url":"/2024/12/31/mybatis-源码阅读记录-1--building包/","content":"building 包是用来构造Mapper的包含如下的类 building包负责接口 mapper 模式的代理类,其中mapperProxy实用了java7 的放射新特性这个需要注意一下 building 包","date":"2024-12-31","categories":["java"],"tags":["mybatis"]},{"title":"netty-(0)-netty demo 和netty的基本组建","url":"/2024/12/31/netty--0--netty-demo-和netty的基本组建/","content":"netty(0)-netty demo 和netty的基本组建一.首先展示这个demo的目录结构EchoClient 和 EchoServer 分别表示 客户端和服务端的运行程序，对应的Handler 数据调用的时候相关的操作方法 二.代码 choClient.class EchoClientHandler.class EchoServer.class EchoServerHandler.class 介绍一下netty的基本组成有那些Bootstrap这个基本类是一个便利的基本工厂用来实现netty 快速配置的方法类 bootstrap实现netty快速配置的过程是这样的 netty 其实提供了两组的bootstrap ， 和serverBootstrap 分别对应client端和server端 eventloop groupnetty 是基于事件循环的，并且是muilt-eventLoop （多线程版eventloop） 在服务端，netty提供了两套线程组类来供eventloop使用 bossloopGroup 和 wordloopGroup ！ 分离的取决于netty的架构设计，之前我们说过netty的服务端channel分为parent channel 和childern channel，分别负责创建链接和使用链接 loopGroup也是这样进行分组的 bossLoopGroup 表示服务器连接监听线程组，专门接受 accept 新的客户端client 连接 workerGroup 表示处理每一条连接的数据收发的线程组 在线程组和启动器都创建完成后，就可以开始设置线程组：通过 b.group(bossGroup, workerGroup) 方法，给引导器配置两大线程组。配置完成之后，整个引导类的 reactor 线程正式确定。这里确定的工作模式，为父子线程的模型。 如果只设置一个线程组，具体的方法为 —— b.group( workerGroup) 。配置完成一个线程组，则所有的 channel ，包括服务监听通道父亲channel 和所有的子channel ，都工作在同一个线程组中。 channel Channelhandle ChannelHandleContext ChannelPipelinechannel在netty中其实是对应一种链接类型（或者说socket的类型） netty 目前的链接类型大约是如下的几种 NioSocketChannel, 代表异步的客户端 TCP Socket 连接. NioServerSocketChannel, 异步的服务器端 TCP Socket 连接. NioDatagramChannel, 异步的 UDP 连接 NioSctpChannel, 异步的客户端 Sctp 连接. NioSctpServerChannel, 异步的 Sctp 服务器端连接. OioSocketChannel, 同步的客户端 TCP Socket 连接. OioServerSocketChannel, 同步的服务器端 TCP Socket 连接. OioDatagramChannel, 同步的 UDP 连接 OioSctpChannel, 同步的 Sctp 服务器端连接. OioSctpServerChannel, 同步的客户端 TCP Socket 连接. netty 其实可以说90%以上的设计点都是在channel方法上，这里说明一下netty的chanel有那些 1. netty channelnetty在设计的时候其实channel分为parent channel和childern channel parent channel:服务器连接监听的channel,用来坚监听服务器链接和创建链接childern channel:一个socket 对应的channel,由parent channel 创建的channel 总结channel其实就是netty实际链接的一种抽象 2. channelhandleChannelHandler用于处理Channel对应的事件 （netty定义了大量的事件，通过这些事件调用对应的handle方法）ChannelHandler接口里面只定义了三个生命周期方法，我们主要实现它的子接口ChannelInboundHandler和ChannelOutboundHandler，为了便利，框架提供了ChannelInboundHandlerAdapter，ChannelOutboundHandlerAdapter和ChannelDuplexHandler这三个适配类，在使用的时候只需要实现你关注的方法即可 ChannelInboundHandler 事件： 回调方法 触发时机 client server channelRegistered 当前channel注册到EventLoop true true channelUnregistered 当前channel从EventLoop取消注册 true true channelActive 当前channel激活的时候 true true channelInactive 当前channel不活跃的时候，也就是当前channel到了它生命周期末 true true channelRead 当前channel从远端读取到数据 true true channelReadComplete channel read消费完读取的数据的时候被触发 true true userEventTriggered 用户事件触发的时候 FALSE FALSE channelWritabilityChanged channel的写状态变化的时候触发 FALSE FALSE ChannelOutboundHandler： 回调方法 触发时机 client server bind bind操作执行前触发 false true connect connect 操作执行前触发 true false disconnect disconnect 操作执行前触发 true false close close操作执行前触发 false true deregister deregister操作执行前触发 read read操作执行前触发 true true write write操作执行前触发 true true flush flush操作执行前触发 true true 3. channelhandlecontext这个是 channelhandle的一种封装，可以理解为中间层，相当于netty的数据层和调度层的中间层 当一个channelHandle 加入到netty中的时候，netty将会自动的将这个channelhandle包装成 channelhandlecontext，供上方使用 注意channelhandleContext的数据其实非常封闭的，他只能处理自己的和之后的数据 并不能应修改这个流程中的所有数据，如果进行修改的是channel或者channlePipeline 的话，从头重新掉用数据 3. ChannelPipeline这个就是一个channelhandlecontext处理节点的一个集合， 一个channel（数据源）对应一个channelPopeline，一个channelPipeline对应一组channelhandleContext. channelPipeline > 其实就是eventloop处理队列的一个抽象，修改channelPipeline中的数据将会影响整个netty对应这个channel的处理方式 4 .整体总结 一个channel对应一个channelPopeline 一个channelPipeline 对应一组 channelContext 一个channelContext承载一个channelhandle 处理进入的时候走的是ChannelInboundHandler接口的处理实现类，吐出的时候走的是ChannelOutboundHandler接口的实现类 netty提供了两个通用的handle HeadContext HeadContext实现了ChannelOutboundHandler，ChannelInboundHandler这两个接口 因为在头部，所以说HeadContext中关于in和out的回调方法都会触发关于ChannelInboundHandler，HeadContext的作用是进行一些前置操作，以及把事件传递到下一个ChannelHandlerContext的ChannelInboundHandler中去 TailContextTailContext实现了ChannelInboundHandler接口，会在ChannelInboundHandler调用链最后执行，只要是对调用链完成处理的情况进行处理，看下channelRead实现 ChannelFuture这个已经是netty最后一个组建，这个方法用来通知netty中channel的状态，因为netty是异步的，所以通过这种方法来实现一个监控 ps 从这里我们也可以明白了其实如果netty需要增加一个链接一定会再实现上面的这一套操作创建一个新的future","date":"2024-12-31","categories":["java"],"tags":["netty"]},{"title":"netty-(1)传输api","url":"/2024/12/31/netty--1-传输api/","content":"传输API的核心是interfaceChannel，它被用于所有的I&#x2F;O操作。Channel类的层次结构如图 每个Channel都将会被分配一个ChannelPipeline和ChannelConfig。ChannelConfig包含了该Channel的所有配置设置，并且支持热更新。由于特定的传输可能具有独特的设置，所以它可能会实现一个ChannelConfig的子类型。（请参考ChannelConfig实现对应的Javadoc。） 由于Channel是独一无二的，所以为了保证顺序将Channel声明为java.lang.Comparable的一个子接口。因此，如果两个不同的Channel实例都返回了相同的散列码，那么AbstractChannel中的compareTo()方法的实现将会抛出一个Error。 ChannelPipeline持有所有将应用于入站和出站数据以及事件的ChannelHandler实例，这些ChannelHandler实现了应用程序用于处理状态变化以及数据处理的逻辑。 ChannelHandler的典型用途包括： 将数据从一种格式转换为另一种格式； 提供异常的通知； 提供Channel变为活动的或者非活动的通知； 提供当Channel注册到EventLoop或者从EventLoop注销时的通知； 提供有关用户自定义事件的通知 channel 的方法 方法名 描述 eventLoop 返回分配给Channel的EventLoop pipeline 返回分配给Channel的ChannelPipeline isActive 如果Channel是活动的，则返回true。活动的意义可能依赖于底层的传输。例如，一个Socket传输一旦连接到了远程节点便是活动的，而一个Datagram传输一旦被打开便是活动的 localAddress 返回本地的SocketAddress remoteAddress 返回远程的SocketAddress write 将数据写到远程节点。这个数据将被传递给ChannelPipeline，并且排队直到它被冲刷 flush 将之前已写的数据冲刷到底层传输，如一个Socket writeAndFlush 一个简便的方法，等同于调用write()并接着调用flush()","date":"2024-12-31","categories":["java"],"tags":["netty"]},{"title":"netty-(2)byteBuf","url":"/2024/12/31/netty--2-byteBuf/","content":"netty byteBufNetty的数据处理API通过两个组件暴露——abstract class ByteBuf和interface ByteBufHolder。 下面是一些ByteBuf API的优点： 它可以被用户自定义的缓冲区类型扩展； 通过内置的复合缓冲区类型实现了透明的零拷贝； 容量可以按需增长（类似于JDK的StringBuilder）； 在读和写这两种模式之间切换不需要调用ByteBuffer的flip()方法； 读和写使用了不同的索引； 支持方法的链式调用； 支持引用计数； 支持池化。 byteBuf 的工作方式byteBuf拥有两个不同的索引- 记录读位置的readerIndex 和记录写位置的writerIndex 可读的位置就是 readerIndex 到writerIndex 之间的位置 可写的位置就是writerIndex 到这个缓存的结尾 注意netty在实现的时候并没有使用循环队列来优化性能 可以看一下 distuptor框架 byteBuf 的 三种使用模式 使用堆缓冲区 最常用的ByteBuf模式是将数据存储在JVM的堆空间中。这种模式被称为支撑数组（backing array），它能在没有使用池化的情况下提供快速的分配和释放。非常适合于有遗留的数据需要处理的情况。 直接缓冲区 直接缓冲区是另外一种ByteBuf模式。我们期望用于对象创建的内存分配永远都来自于堆中，但这并不是必需的——NIO在JDK 1.4中引入的ByteBuffer类允许JVM实现通过本地调用来分配内存。这主要是为了避免在每次调用本地I&#x2F;O操作之前（或者之后）将缓冲区的内容复制到一个中间缓冲区（或者从中间缓冲区把内容复制到缓冲区）。 直接缓冲区的主要缺点是，相对于基于堆的缓冲区，它们的分配和释放都较为昂贵。如果处理遗留的数据将必须将数据从直接缓冲区赋值到堆空间中 复合缓冲区 举例说明，让我们考虑一下一个由两部分——头部和主体——组成的将通过HTTP协议传输的消息。这两部分由应用程序的不同模块产生，将会在消息被发送的时候组装。该应用程序可以选择为多个消息重用相同的消息主体。当这种情况发生时，对于每个消息都将会创建一个新的头部。 因为我们不想为每个消息都重新分配这两个缓冲区，所以使用CompositeByteBuf是一个完美的选择。它在消除了没必要的复制的同时，暴露了通用的ByteBuf API bytebuffer分配方式 ByteBufAllocator接口的实现类实现分配 名 称 描 述 buffer() 返回一个基于堆或者直接内存存储的ByteBuf buffer(intinitialCapacity); buffer(intinitialCapacity,intmaxCapacity); heapBuffer() 返回一个基于堆内存存储的ByteBuf heapBuffer(intinitialCapacity) heapBuffer(intinitialCapacity,intmaxCapacity) directBuffer() 返回一个基于直接内存存储的ByteBuf directBuffer(intinitialCapacity) directBuffer(intinitialCapacity,intmaxCapacity) compositeBuffer() 返回一个可以通过添加最大到指定数目的基于堆的或者直接内存存储的缓冲区来扩展的CompositeByteBuf compositeBuffer(intmaxNumComponents) compositeDirectBuffer() compositeDirectBuffer(intmaxNumComponents); compositeHeapBuffer() compositeHeapBuffer(intmaxNumComponents); ioBuffer()[8] 返回一个用于套接字的I&#x2F;O操作的ByteBuf 获取方法 通过channel 或者绑定到ChannelHandler的ChannelHandlerContext Netty提供了两种ByteBufAllocator的实现 PooledByteBufAllocator:池化了ByteBuf的实例以提高性能并最大限度地减少内存碎片。此实现使用了一种称为jemalloc[9]的已被大量现代操作系统所采用的高效方法来分配内存 Unpooled-ByteBufAllocator:不池化ByteBuf实例，并且在每次它被调用时都会返回一个新的实例。 Unpooled 缓冲区 可能某些情况下，你未能获取一个到ByteBufAllocator的引用。对于这种情况，Netty提供了一个简单的称为Unpooled的工具类，它提供了静态的辅助方法来创建未池化的ByteBuf 名 称 描 述 buffer() 返回一个未池化的基于堆内存存储的ByteBuf buffer(intinitialCapacity) buffer(intinitialCapacity,intmaxCapacity) directBuffer() 返回一个未池化的基于直接内存存储的ByteBuf directBuffer(intinitialCapacity) directBuffer(intinitialCapacity,intmaxCapacity) wrappedBuffer() 返回一个包装了给定数据的ByteBuf copiedBuffer() 返回一个复制了给定数据的ByteBuf Unpooled类还使得ByteBuf同样可用于那些并不需要Netty的其他组件的非网络项目，使得其能得益于高性能的可扩展的缓冲区API。 ByteBufUtil类 ByteBufUtil提供了用于操作ByteBuf的静态的辅助方法。因为这个API是通用的，并且和池化无关，所以这些方法已然在分配类的外部实现。 这些静态方法中最有价值的可能就是hexdump()方法，它以十六进制的表示形式打印ByteBuf的内容。这在各种情况下都很有用，例如，出于调试的目的记录ByteBuf的内容。十六进制的表示通常会提供一个比字节值的直接表示形式更加有用的日志条目，此外，十六进制的版本还可以很容易地转换回实际的字节表示。 另一个有用的方法是boolean equals(ByteBuf, ByteBuf)，它被用来判断两个ByteBuf实例的相等性。如果你实现自己的ByteBuf子类，你可能会发现ByteBufUtil的其他有用方法。","date":"2024-12-31","categories":["java"],"tags":["netty"]},{"title":"netty-(3)channelHandler","url":"/2024/12/31/netty--3-channelHandler/","content":"之前有一个netty的数据流处理图 在这张图中可以体现出在ChannelPipeline中将ChannelHandler链接在一起以组织处理逻辑,并且使用ChannelHandlerContext传递参数 ChannelHandler家族 channnel的生命周期 状 态 描 述 ChannelUnregistered Channel已经被创建，但还未注册到EventLoop ChannelRegistered Channel已经被注册到了EventLoop ChannelActive Channel处于活动状态（已经连接到它的远程节点）。它现在可以接收和发送数据了 ChannelInactive Channel没有连接到远程节点 channelHandler的状态扭转如下 channelHandler生命周期 在ChannelHandler被添加到ChannelPipeline中或者被从ChannelPipeline中移除时会调用这些操作。这些方法中的每一个都接受一个ChannelHandlerContext参数 channelHandlerContext在生命周期中的回调方法 类 型 描 述 handlerAdded 当把ChannelHandler添加到ChannelPipeline中时被调用 handlerRemoved 当从ChannelPipeline中移除ChannelHandler时被调用 exceptionCaught 当处理过程中在ChannelPipeline中有错误产生时被调用 ChannelInactive Channel没有连接到远程节点 Netty定义了下面两个重要的ChannelHandler子接口： ChannelInboundHandler——处理入站数据以及各种状态变化； ChannelOutboundHandler——处理出站数据并且允许拦截所有的操作。 channelInBoundHandler 用来处理输入流的处理方法,相比较原始的channelhandler接口,本方法扩展了一写一channel状态相关的方法,这些方法将会在数据被接收时或者与其对应的Channel状态发生改变时被调用。正如我们前面所提到的，这些方法和Channel的生命周期密切相关 类 型 描 述 channelRegistered 当Channel已经注册到它的EventLoop并且能够处理I&#x2F;O时被调用 channelUnregistered 当Channel从它的EventLoop注销并且无法处理任何I&#x2F;O时被调用 channelActive 当Channel处于活动状态时被调用；Channel已经连接&#x2F;绑定并且已经就绪 channelInactive 当Channel离开活动状态并且不再连接它的远程节点时被调用 channelReadComplete 当Channel上的一个读操作完成时被调用[1] channelRead 当从Channel读取数据时被调用 ChannelWritabilityChanged 当Channel的可写状态发生改变时被调用。用户可以确保写操作不会完成得太快（以避免发生OutOfMemoryError）或者可以在Channel变为再次可写时恢复写入。可以通过调用Channel的isWritable()方法来检测Channel的可写性。与可写性相关的阈值可以通过Channel.config().setWriteHighWaterMark()和Channel.config().setWriteLowWaterMark()方法来设置 userEventTriggered 当ChannelnboundHandler.fireUserEventTriggered()方法被调用时被调用，因为一个POJO被传经了ChannelPipeline 当某个ChannelInboundHandler的实现重写channelRead()方法时，它将负责显式地释放与池化的ByteBuf实例相关的内存。Netty为此提供了一个实用方法ReferenceCount-Util.release() 这样手动的进行资源的释放是比较麻烦的,netty提供了SimpleChannelInboundHandler可以自动的释放资源 由于SimpleChannelInboundHandler会自动释放资源，所以你不应该存储指向任何消息的引用供将来使用，因为这些引用都将会失效。 ChannelOutBoundHandler 出站操作和数据将由ChannelOutboundHandler处理。它的方法将被Channel、Channel- Pipeline以及ChannelHandlerContext调用。 ChannelOutboundHandler的一个强大的功能是可以按需推迟操作或者事件，这使得可以通过一些复杂的方法来处理请求。例如，如果到远程节点的写入被暂停了，那么你可以推迟冲刷操作并在稍后继续。 类 型 描 述 bind(ChannelHandlerContext,SocketAddress,ChannelPromise) 当请求将Channel绑定到本地地址时被调用 connect(ChannelHandlerContext, 当请求将Channel连接到远程节点时被调用 SocketAddress,SocketAddress,ChannelPromise) disconnect(ChannelHandlerContext,ChannelPromise) 当请求将Channel从远程节点断开时被调用 close(ChannelHandlerContext,ChannelPromise) 当请求关闭Channel时被调用 deregister(ChannelHandlerContext,ChannelPromise) 当请求将Channel从它的EventLoop注销时被调用 read(ChannelHandlerContext) 当请求从Channel读取更多的数据时被调用 flush(ChannelHandlerContext) 当请求通过Channel将入队数据冲刷到远程节点时被调用 write(ChannelHandlerContext,Object,ChannelPromise) 当请求通过Channel将数据写到远程节点时被调用 ChannelPromise与ChannelFuture ChannelOutboundHandler中的大部分方法都需要一个ChannelPromise参数，以便在操作完成时得到通知。ChannelPromise是ChannelFuture的一个子类，其定义了一些可写的方法，如setSuccess()和setFailure()，从而使ChannelFuture不可变[2]。 netty 内置处理器实现类ChannelInboundHandlerAdapter和ChannelOutboundHandlerAdapter你可以使用ChannelInboundHandlerAdapter和ChannelOutboundHandlerAdapter类作为自己的ChannelHandler的起始点。这两个适配器分别提供了ChannelInboundHandler和ChannelOutboundHandler的基本实现。通过扩展抽象类ChannelHandlerAdapter，它们获得了它们共同的超接口ChannelHandler的方法。 ChannelHandlerAdapter还提供了实用方法isSharable() 如果其对应的实现被标注为Sharable，那么这个方法将返回true，表示它可以被添加到多个ChannelPipeline中 在ChannelInboundHandlerAdapter和ChannelOutboundHandlerAdapter中所提供的方法体调用了其相关联的ChannelHandlerContext上的等效方法，从而将事件转发到了ChannelPipeline中的下一个ChannelHandler中。 netty资源的标准写法 注意点: 使用ReferenceCountUtil.release(msg); 释放资源 使用promise.setSuccess();通知ChannelPromise数据已经被处理了 netty 内置资源泄漏检测工具每当通过调用ChannelInboundHandler.channelRead()或者ChannelOutbound- Handler.write()方法来处理数据时，你都需要确保没有任何的资源泄漏。你可能还记得在前面的章节中所提到的，Netty使用引用计数来处理池化的ByteBuf。所以在完全使用完某个ByteBuf后，调整其引用计数是很重要的。 为了帮助你诊断潜在的（资源泄漏）问题，Netty提供了class ResourceLeakDetector[3]，它将对你应用程序的缓冲区分配做大约1%的采样来检测内存泄露。 netty 的 channelPipeLine 接口channelPipline 其实是一个调度链处理器,用来处理channel和对应的handle的执行过程 每一个channel都被分配了一个唯一的pipeline,这项关联是永久性的；Channel既不能附加另外一个ChannelPipeline，也不能分离其当前的。 根据事件的起源，事件将会被ChannelInboundHandler或者ChannelOutboundHandler处理。随后，通过调用ChannelHandlerContext的实现，它将被转发给同一超类型的下一个ChannelHandler ChannelHandlerContext:ChannelHandlerContext使得ChannelHandler能够和它的ChannelPipeline以及其他的ChannelHandler交互。ChannelHandler可以通知其所属的ChannelPipeline中的下一个ChannelHandler，甚至可以动态修改它所属的ChannelPipeline。 当你完成了通过调用ChannelPipeline.add*()方法将入站处理器（ChannelInboundHandler）和出站处理器（ChannelOutboundHandler）混合添加到ChannelPipeline之后，每一个ChannelHandler从头部到尾端的顺序位置正如同我们添加的时候那样。 在ChannelPipeline传播事件时，它会测试ChannelPipeline中的下一个Channel- Handler的类型是否和事件的运动方向相匹配。如果不匹配，ChannelPipeline将跳过该ChannelHandler并前进到下一个，直到它找到和该事件所期望的方向相匹配的为止。 channelPipeline接口的方法 名 称 描 述 addFirst 将一个ChannelHandler添加到ChannelPipeline中 addBeforeaddAfteraddLast remove 将一个ChannelHandler从ChannelPipeline中移除 replace 将ChannelPipeline中的一个ChannelHandler替换为另一个ChannelHandler 有时可能需要与那些使用阻塞API的遗留代码进行交互。对于这种情况，ChannelPipeline有一些接受一个EventExecutorGroup的add()方法。如果一个事件被传递给一个自定义的EventExecutor- Group，它将被包含在这个EventExecutorGroup中的某个EventExecutor所处理，从而被从该Channel本身的EventLoop中移除。对于这种用例，Netty提供了一个叫DefaultEventExecutor- Group的默认实现。 channelPipeline 来访问ChannelHandler的方法 名 称 描 述 get 通过类型或者名称返回ChannelHandler context 返回和ChannelHandler绑定的ChannelHandlerContext names 返回ChannelPipeline中所有ChannelHandler的名称 channelPipeline 事件触发方法 入站事件 方法名称 描 述 fireChannelRegistered 调用ChannelPipeline中下一个ChannelInboundHandler的channelRegistered(ChannelHandlerContext)方法 fireChannelUnregistered 调用ChannelPipeline中下一个ChannelInboundHandler的channelUnregistered(ChannelHandlerContext)方法 fireChannelActive 调用ChannelPipeline中下一个ChannelInboundHandler的channelActive(ChannelHandlerContext)方法 fireChannelInactive 调用ChannelPipeline中下一个ChannelInboundHandler的channelInactive(ChannelHandlerContext)方法 fireExceptionCaught 调用ChannelPipeline中下一个ChannelInboundHandler的exceptionCaught(ChannelHandlerContext,Throwable)方法 fireUserEventTriggered 调用ChannelPipeline中下一个ChannelInboundHandler的userEventTriggered(ChannelHandlerContext,Object)方法 fireChannelRead 调用ChannelPipeline中下一个ChannelInboundHandler的channelRead(ChannelHandlerContext,Objectmsg)方法 fireChannelReadComplete 调用ChannelPipeline中下一个ChannelInboundHandler的channelReadComplete(ChannelHandlerContext)方法 fireChannelWritabilityChanged 调用ChannelPipeline中下一个ChannelInboundHandler的channelWritabilityChanged(ChannelHandlerContext)方法 出站事件 方法名称 描 述 bind 将Channel绑定到一个本地地址，这将调用ChannelPipeline中的下一个ChannelOutboundHandler的bind(ChannelHandlerContext,SocketAddress,ChannelPromise)方法 connect 将Channel连接到一个远程地址，这将调用ChannelPipeline中的下一个ChannelOutboundHandler的connect(ChannelHandlerContext,SocketAddress,ChannelPromise)方法 disconnect 将Channel断开连接。这将调用ChannelPipeline中的下一个ChannelOutboundHandler的disconnect(ChannelHandlerContext,ChannelPromise)方法 close 将Channel关闭。这将调用ChannelPipeline中的下一个ChannelOutboundHandler的close(ChannelHandlerContext,ChannelPromise)方法 deregister 将Channel从它先前所分配的EventExecutor（即EventLoop）中注销。这将调用ChannelPipeline中的下一个ChannelOutboundHandler的deregister(ChannelHandlerContext,ChannelPromise)方法 flush 冲刷Channel所有挂起的写入。这将调用ChannelPipeline中的下一个ChannelOutboundHandler的flush(ChannelHandlerContext)方法 write 将消息写入Channel。这将调用ChannelPipeline中的下一个ChannelOutboundHandler的write(ChannelHandlerContext,Objectmsg,ChannelPromise)方法。注意：这并不会将消息写入底层的Socket，而只会将它放入队列中。要将它写入Socket，需要调用flush()或者writeAndFlush()方法 writeAndFlush 这是一个先调用write()方法再接着调用flush()方法的便利方法 read 请求从Channel中读取更多的数据。这将调用ChannelPipeline中的下一个ChannelOutboundHandler的read(ChannelHandlerContext)方法 总结一下： ChannelPipeline保存了与Channel相关联的ChannelHandler； ChannelPipeline可以根据需要，通过添加或者删除ChannelHandler来动态地修改； ChannelPipeline有着丰富的API用以被调用，以响应入站和出站事件。 channelHandlerContext接口ChannelHandlerContext代表了ChannelHandler和ChannelPipeline之间的关联，每当有ChannelHandler添加到ChannelPipeline中时，都会创建ChannelHandler- Context。ChannelHandlerContext的主要功能是管理它所关联的ChannelHandler和在同一个ChannelPipeline中的其他ChannelHandler之间的交互 ChannelHandlerContext有很多的方法，其中一些方法也存在于Channel和Channel- Pipeline本身上，但是有一点重要的不同。如果调用Channel或者ChannelPipeline上的这些方法，它们将沿着整个ChannelPipeline进行传播。而调用位于ChannelHandlerContext上的相同方法，则将从当前所关联的ChannelHandler开始，并且只会传播给位于该ChannelPipeline中的下一个能够处理该事件的ChannelHandler。 方法名称 描 述 alloc 返回和这个实例相关联的Channel所配置的ByteBufAllocator bind 绑定到给定的SocketAddress，并返回ChannelFuture channel 返回绑定到这个实例的Channel close 关闭Channel，并返回ChannelFuture connect 连接给定的SocketAddress，并返回ChannelFuture deregister 从之前分配的EventExecutor注销，并返回ChannelFuture disconnect 从远程节点断开，并返回ChannelFuture executor 返回调度事件的EventExecutor fireChannelActive 触发对下一个ChannelInboundHandler上的channelActive()方法（已连接）的调用 fireChannelInactive 触发对下一个ChannelInboundHandler上的channelInactive()方法（已关闭）的调用 fireChannelRead 触发对下一个ChannelInboundHandler上的channelRead()方法（已接收的消息）的调用 fireChannelReadComplete 触发对下一个ChannelInboundHandler上的channelReadComplete()方法的调用 fireChannelRegistered 触发对下一个ChannelInboundHandler上的fireChannelRegistered()方法的调用 fireChannelUnregistered 触发对下一个ChannelInboundHandler上的fireChannelUnregistered()方法的调用 fireChannelWritabilityChanged 触发对下一个ChannelInboundHandler上的fireChannelWritabilityChanged()方法的调用 fireExceptionCaught 触发对下一个ChannelInboundHandler上的fireExceptionCaught(Throwable)方法的调用 fireUserEventTriggered 触发对下一个ChannelInboundHandler上的fireUserEventTriggered(Objectevt)方法的调用 handler 返回绑定到这个实例的ChannelHandler isRemoved 如果所关联的ChannelHandler已经被从ChannelPipeline中移除则返回true name 返回这个实例的唯一名称 pipeline 返回这个实例所关联的ChannelPipeline read 将数据从Channel读取到第一个入站缓冲区；如果读取成功则触发[5]一个channelRead事件，并（在最后一个消息被读取完成后）通知ChannelInboundHandler的channelReadComplete(ChannelHandlerContext)方法 write 通过这个实例写入消息并经过ChannelPipeline writeAndFlush 通过这个实例写入并冲刷消息并经过ChannelPipeline 当使用ChannelHandlerContext的API的时候，请牢记以下两点： ChannelHandlerContext和ChannelHandler之间的关联（绑定）是永远不会改变的，所以缓存对它的引用是安全的； 如同我们在本节开头所解释的一样，相对于其他类的同名方法，ChannelHandlerContext的方法将产生更短的事件流，应该尽可能地利用这个特性来获得最大的性能。 netty 的异常处理异常处理是任何真实应用程序的重要组成部分，它也可以通过多种方式来实现。因此，Netty提供了几种方式用于处理入站或者出站处理过程中所抛出的异常。 如果在处理入站事件的过程中有异常被抛出，那么它将从它在ChannelInboundHandler里被触发的那一点开始流经ChannelPipeline。要想处理这种类型的入站异常，你需要在你的ChannelInboundHandler实现中重写下面的方法。 因为异常会向入站方向流动,，所以实现了前面所示逻辑的ChannelInboundHandler通常位于ChannelPipeline的最后,确保了所有的入站异常都总是会被处理，无论它们可能会发生在ChannelPipeline中的什么位置 你应该如何响应异常，可能很大程度上取决于你的应用程序。你可能想要关闭Channel（和连接），也可能会尝试进行恢复。如果你不实现任何处理入站异常的逻辑（或者没有消费该异常），那么Netty将会记录该异常没有被处理的事实[7]。 总结一下： ChannelHandler.exceptionCaught()的默认实现是简单地将当前异常转发给ChannelPipeline中的下一个ChannelHandler； 如果异常到达了ChannelPipeline的尾端，它将会被记录为未被处理； 要想定义自定义的处理逻辑，你需要重写exceptionCaught()方法。然后你需要决定是否需要将该异常传播出去。 用于处理出站操作中的正常完成以及异常的选项，都基于以下的通知机制。 每个出站操作都将返回一个ChannelFuture。注册到ChannelFuture的Channel- FutureListener将在操作完成时被通知该操作是成功了还是出错了。 几乎所有的ChannelOutboundHandler上的方法都会传入一个ChannelPromise的实例。作为ChannelFuture的子类，ChannelPromise也可以被分配用于异步通知的监听器。但是，ChannelPromise还具有提供立即通知的可写方法： 添加ChannelFutureListener只需要调用ChannelFuture实例上的addListener(ChannelFutureListener)方法，并且有两种不同的方式可以做到这一点。 方法一是，调用出站操作（如write()方法）所返回的ChannelFuture上的addListener()方法。 第二种方式是将ChannelFutureListener添加到即将作为参数传递给ChannelOut- boundHandler的方法的ChannelPromise 这种方式可以在全局上做了监听,也就是说如果整个ChannelOutboundHandler出现了问题,将会在ChannelPromise注册的监听其中进行监听","date":"2024-12-31","categories":["java"],"tags":["netty"]},{"title":"netty-(4)EventLoop和线程模型","url":"/2024/12/31/netty--4-EventLoop和线程模型/","content":"线程池模型概述基本的线程池化模式可以描述为： 从池的空闲线程列表中选择一个Thread，并且指派它去运行一个已提交的任务（一个Runnable的实现）；当任务完成时，将该Thread返回给该列表，使其可被重用。 netty中线程模型(eventLoop)使用 EventLoop接口 运行任务来处理在连接的生命周期内发生的事件是任何网络框架的基本功能。与之相应的编程上的构造通常被称为事件循环——一个Netty使用了interface io.netty.channel. EventLoop来适配的术语。 一段代码表示,事件循环的基本思想，其中每个任务都是一个Runnable的实例 netty 中EventLoop和channel之间的交互状态的合并方式 Netty的EventLoop是协同设计的一部分，它采用了两个基本的API：并发和网络编程。首先，io.netty.util.concurrent包构建在JDK的java.util.concurrent包上，用来提供线程执行器。其次，io.netty.channel包中的类，为了与Channel的事件进行交互，扩展了这些接口&#x2F;类。 在这个模型中，一个EventLoop将由一个永远都不会改变的Thread驱动，同时任务（Runnable或者Callable）可以直接提交给EventLoop实现，以立即执行或者调度执行。 根据配置和可用核心的不同，可能会创建多个EventLoop实例用以优化资源的使用，并且单个EventLoop可能会被指派用于服务多个Channel。 在Netty 4中，所有的I&#x2F;O操作和事件都由已经被分配给了EventLoop的那个Thread来处理 jdk延时任务和netty的延时任务在执行延时任务的时候jdk的做法和netty的做法是不同的 java使用多线程机制实现延迟操作ScheduledExecutorService等java.util.concurrent.Executors 类 一个例子 在netty所有的事件本质上都是使用时间轮训的方法实现的 所以Netty通过Channel的EventLoop实现任务调度解决了这一问题 经过60秒之后，Runnable实例将由分配给Channel的EventLoop执行。如果要调度任务以每隔60秒执行一次，请使用scheduleAtFixedRate()方法 注意:Netty的EventLoop扩展了ScheduledExecutorService（见图7-2），所以它提供了使用JDK实现可用的所有方法，包括在前面的示例中使用到的schedule()和scheduleAtFixedRate()方法 要想取消或者检查（被调度任务的）执行状态，可以使用每个异步操作所返回的Scheduled- Future","date":"2024-12-31","categories":["java"],"tags":["netty"]},{"title":"netty-(5)bootstrap","url":"/2024/12/31/netty--5-bootstrap/","content":"其实对于任何框架来说,都会有一个公共的入口,在netty中这个入口被叫做 bootstrap 引导 引导类的层次结构包括一个抽象的父类和两个具体的引导子类 针对上图我们能看出来,特定于客户端或者服务器的引导步骤则分别由Bootstrap或ServerBootstrap处理 注意: 你有时可能会需要创建多个具有类似配置或者完全相同配置的Channel。为了支持这种模式而又不需要为每个Channel都创建并配置一个新的引导类实例，AbstractBootstrap被标记为了Cloneable[5]。在一个已经配置完成的引导类实例上调用clone()方法将返回另一个可以立即使用的引导类实例。 注意:这种方式只会创建引导类实例的EventLoopGroup的一个浅拷贝，所以，后者[6]将在所有克隆的Channel实例之间共享。这是可以接受的，因为通常这些克隆的Channel的生命周期都很短暂，一个典型的场景是——创建一个Channel以进行一次HTTP请求。 客户端引导程序Bootstrap类负责为客户端和使用无连接协议的应用程序创建Channel 服务端引导程序下图展示了ServerBootstrap在bind()方法被调用时创建了一个ServerChannel，并且该ServerChannel管理了多个子Channel。 当从channel中生成引导使用新的EventLoop在netty中一个EventLoop对应一个线程,为了降低系统的性能损耗,在netty中使用了Bootstrap的group方法来重用EventLoop netty handle的初始化操作Netty提供了一个特殊的ChannelInboundHandlerAdapter子类 提实现了一个特殊方法 这个子类或者说handle 一旦被注册到了它的EventLoop之后，就会调用你的initChannel()方法。在该方法返回之后，ChannelInitializer的实例将会从Channel-Pipeline中移除它自己。 netty 的配置属性和传递性参数属性netty的bootstrap 提供了 option() 方法来将ChannelOption 配置进系统中,提供了方便的配置方法 并且netty的引导还提供了AttributeMap(一个由Channel和引导类提供的集合)和AttributeKey(一个用于插入和获取属性值的泛型类),通过这种方法,就可以将相关的属性传递到服务中的channel中去了 netty的关闭netty的优雅关闭 你需要关闭EventLoopGroup，它将处理任何挂起的事件和任务，并且随后释放所有活动的线程。这就是调用EventLoopGroup.shutdownGracefully()方法的作用。这个方法调用将会返回一个Future，这个Future将在关闭完成时接收到通知。需要注意的是，shutdownGracefully()方法也是一个异步的操作，所以你需要阻塞等待直到它完成，或者向所返回的Future注册一个监听器以在关闭完成时获得通知。 或者，你也可以在调用EventLoopGroup.shutdownGracefully()方法之前，显式地在所有活动的Channel上调用Channel.close()方法。但是在任何情况下，都请记得关闭EventLoopGroup本身。","date":"2024-12-31","categories":["java"],"tags":["netty"]},{"title":"netty-高性能网络框架-Netty的组建和设计","url":"/2024/12/31/netty-高性能网络框架-Netty的组建和设计/","content":"netty高性能网络框架-Netty的组建和设计 在Netty框架中有三大关键组建 ChannelEventLoopchannelFutureChannelHandlerchannelPipelinebootstrap 如果和网络中的相关概念相互对应将会是如下的结果： Channel——Socket EventLoop——控制流、多线程处理、并发 ChannelFuture——异步通知 一.Chanel接口这个接口实现基本的io操作(bind()、connect()、read()、write())，映射传统javaSocket的编程过程，大大降低相关的直接使用socket的复杂程度。相关的实现类工程师 NioSocketChannel NioSocketServerChannel 二.EventLoop接口这个接口相当于一个人容器，控制链接生命周期所发生的相关所有的事件。 Channel、EventLoop、Thread、EventLoopGroup之间的关系如下图： 三.ChannelFuure接口在netty中处理I&#x2F;O操作异步返回值的方法就是ChannelFutrue接口，这个接口的addListener方法可以注册一个ChannelFutureListener方法，方便在某个操作完成的时候得到通知 四.ChannelHandler接口管理数据流和执行应用程序逻辑的重要组件，ChannelHandler充当了所有处理入站和出站相关应用的逻辑处理的容器 有两个非常常用的子接口 ChannelInBoundHandler和ChannelOutboundHandler分别代表出站和入站的相关处理逻辑 五.ChannelPipeline接口chanelPpeline接口为ChannelHandler链提供了容器，并定义了用在该链上传播入站和出站时间流的api，当channl被创建的时候将自动的分配到他专属的ChannelPipeline中被相关的channlhandle处理 ChnanelPipeline中流动的是事件（事件中可能附加数据）。Netty定义了两种事件类型：入站（inbound）事件和出站（outbound）事件。ChannelPipeline使用拦截过滤器模式使用户可以掌控ChannelHandler处理事件的流程。注意：事件在ChannelPipeline中不自动流动而需要调用ChannelHandlerContext中诸如fileXXX()或者read()类似的方法将事件从一个ChannelHandler传播到下一个ChannelHandler 特殊东西ChannelHandlerContext Context指上下文关系，ChannelHandler的Context指的是ChannleHandler之间的关系以及ChannelHandler与ChannelPipeline之间的关系。ChannelPipeline中的事件传播主要依赖于ChannelHandlerContext实现，由于ChannelHandlerContext中有ChannelHandler之间的关系，所以能得到ChannelHandler的后继节点，从而将事件传播到下一个ChannelHandler 六.引导bootstrap 引导字如其意，就是进行相关链接处理的工具方法 类别 BootStrap ServerBootStrap 网络中编程的作用 链接远程的主机和端口 绑定到一个本地端口 EventLoopGroup的数量 1 2 注意:这里服务端的LoopGroup需要两个，,默认有两个eventloop分别是bossGroup和 workGroup，其中boss用来监控tcp链接,worker用来处理io事件。","date":"2024-12-31","categories":["java"],"tags":["netty"]},{"title":"spirng回顾(1)-spring-core","url":"/2024/12/31/spirng回顾-1--spring-core/","content":"spring 从3.0 开始支持java配置所以这里整理一下spring 3之后重要的知识 spring application回顾一下application的东西，负责初始化spring bean容器的非常重要的类 bean在spring中所有的javabean都将会初始化为BeanDefinition对象 这个类其实承载了javabean中所有的元数据包括： javabean中的各种属性，类名等 javabean的实例化对象 Bean行为配置元素，用于声明Bean在容器中的行为（作用域，生命周期回调等） 对其他bean进行工作所需的引用。这些引用也称为协作者或依赖项 除了包含有关如何创建特定bean的信息的bean定义之外，通过applicationcontext方法getBeanFactory()返回的BeanFactory 实现DefaultListableBeanFactory可以实现javabean的手动注入。DefaultListableBeanFactory 通过registerSingleton(..)和 registerBeanDefinition(..)方法支持此注册。 bean的使用范围 范围 内容 singleton (默认)将每个Spring IoC容器的单个bean定义范围限定为单个对象实例 prototype 将单个bean定义的作用域限定为任意数量的对象实例 request 将单个bean定义的范围限定为单个HTTP请求的生命周期 session 将单个bean定义的作用域限定为HTTP会话的生命周期 application 将单个bean定义的作用域限定为ServletContext的生命周期 websocket 将单个bean定义的作用域限定为WebSocket的生命周期 在java配置方法中可以使用@ApplicationScope来制定作用环境 bean作用域自定义暂时省略 spring bean 生命周期控制spring bean 生命周期回调钩子spring 提供了两种生命周期回调钩子 接扣和注解 实现Spring InitializingBean和DisposableBean接口，分别提供了afterPropertiesSet()和destroy()后者使bean在初始化和销毁​​bean时执行某些操作 使用@PostConstruct和@PreDestroy 注解 监听容器和spring 状态的钩子 Lifecycle接口，用来监听spring bean状态的钩子 LifecycleProcessor 用来对ApplicationContext的重启和关闭做反应的接口 Phased 和SmartLifecycle Phased接口提供提供了优先级，数字越小越早运行越晚关闭 SmartLifecycle 该LifecycleProcessor接口还定义了用于刷新和关闭上下文的回调方法。后者驱动关闭过程，就好像stop()已经显式调用了它一样，但是它在上下文关闭时发生。另一方面，“刷新”回调启用了SmartLifecyclebean的另一个功能 。刷新上下文时（在所有对象都被实例化和初始化之后），该回调将被调用。此时，默认的生命周期处理器将检查每个SmartLifecycle对象的isAutoStartup()方法返回的布尔值 。如果为true，则在该点启动该对象，而不是等待上下文或其自身的显式调用start()方法（与上下文刷新不同，对于标准上下文实现，上下文启动不会自动发生） 优雅关闭spring 非web项目使用 使用applicationContext 的 registerShutdownHook() 方法即可 ApplicationContextAware和BeanNameAware这两个接口 一个获取上下文信息 一个获取bean的名称 其他Aware 名称 注入依赖 在…中解释 ApplicationContextAware 宣告ApplicationContext。 ApplicationContextAware 和 BeanNameAware ApplicationEventPublisherAware 附件的事件发布者ApplicationContext。 的其他功能 ApplicationContext BeanClassLoaderAware 类加载器，用于加载Bean类。 实例化豆 BeanFactoryAware 宣告BeanFactory。 ApplicationContextAware 和 BeanNameAware BeanNameAware 声明bean的名称。 ApplicationContextAware 和 BeanNameAware BootstrapContextAware BootstrapContext容器在其中运行的资源适配器。通常仅在支持JCA的ApplicationContext实例中可用。 JCA CCI LoadTimeWeaverAware 定义的编织器，用于在加载时处理类定义。 在Spring Framework中使用AspectJ进行加载时编织 MessageSourceAware 解决消息的已配置策略（支持参数化和国际化）。 的其他功能 ApplicationContext NotificationPublisherAware Spring JMX通知发布者。 通知事项 ResourceLoaderAware 配置的加载程序，用于对资源的低级别访问。 资源资源 ServletConfigAware 当前ServletConfig容器在其中运行。仅在可感知网络的Spring中有效 ApplicationContext。 春季MVC ServletContextAware 当前ServletContext容器在其中运行。仅在可感知网络的Spring中有效 ApplicationContext。 春季MVC BeanPostProcessor这个接口可以在spring bean初始化完成之后之前的回调接口 PropertySourcesPlaceholderConfigurer 和 BeanFactoryPostProcessor 暂时不研究FactoryBeanFactoryBean界面提供了三种方法： Object getObject()：返回此工厂创建的对象的实例。实例可以共享，具体取决于该工厂是否返回单例或原型。 boolean isSingleton()：true如果FactoryBean返回单例或false其他，则返回 。 Class getObjectType()：返回getObject()方法返回的对象类型，或者null如果类型未知，则返回该对象类型。 spring 资源化处理Resource InputStreamSource 核心实现UrlResource：能获取网络等所有的属性ClassPathResource：能获取jar包中的文件FileSystemResource：基本文件操作ServletContextResourceInputStreamResourceByteArrayResource 使用applicationContext 的 ResourceLoader 获取Resource注意所有的applicationContext都实现了ResourceLoader接口 针对ClassPathXmlApplicationContext，该代码返回ClassPathResource。如果对FileSystemXmlApplicationContext实例执行相同的方法，则将返回 FileSystemResource。对于WebApplicationContext，它将返回 ServletContextResource。类似地，它将为每个上下文返回适当的对象 另一方面，ClassPathResource无论应用程序上下文类型如何，您都可以通过指定特殊classpath:前缀来强制使用 还可以使用ResourceLoaderAware 接口直接获得上下文的ResourceLooader类似applicationAware的模式 spring 自动化配置@Required注释适用于bean属性setter方法， 5.1 弃用 @Autoweiter使用位置这个注解是spring 推荐使用的注解,支持三个位置的注入 属性 构造函数 注意 : 从Spring Framework 4.3开始，@Autowired如果目标bean只定义一个构造函数，则不再需要在该构造函数上添加注释。但是，如果有几个构造函数可用，则必须至少注释一个构造函数，@Autowired以指示容器使用哪个构造函数。 普通方法 多个相同的类型支持实用AutoWrite 默认情况下是使用类型进行注入的,如果有多个类型就会报错,所有spring支持数组接收方法 spring 还支持map 注入的方法 默认情况下，当给定注入点没有匹配的候选bean可用时，自动装配将失败。对于声明的数组，集合或映射，至少应有一个匹配元素。???&#x2F; @AutoWrite spring5 新注解从Spring Framework 5.0开始，您还可以使用@Nullable注释（任何包中的任何注释，例如，javax.annotation.Nullable来自JSR-305 的注释） 其他您还可以使用@Autowired对于那些众所周知的解析依赖接口：BeanFactory，ApplicationContext，Environment，ResourceLoader， ApplicationEventPublisher，和MessageSource。这些接口及其扩展接口（例如ConfigurableApplicationContext或ResourcePatternResolver）将自动解析，而无需进行特殊设置。以下示例自动装配ApplicationContext对象： @AutoWrite 其他内容@AutoWrite注解在方法上的场景??? @Primary用于当有多个候选者的时候进行装配优先是使用的标记 @Qualifier可用于自定义构造限制条件的注解,相当于一个自定义构造注入的扩展点 比如针对如下的定义 自动装配泛型支持spring 其实还支持泛型的自动装配,不同的泛型类型在java中算是不同的类型??? Resource和 @AutoWrite 类型相同 , 只是这个可以携带上name关键字表示实用名称来进行注入 spring 注解高级玩法元注解组合在spring中可以讲一些注解进行任意的组合生成新的注解,并包含组合的注解的功能 @AliasFor注解表示使用继承的注解的哪个属性,对外暴漏,这个时候暴漏的熟悉属性可以在外层被覆盖掉 spring 自动化扫描功能支持 @Configuration @ComponentScan(basePackages &#x3D; “org.example”)注意这个功能必须@Configuration和ComponentScan连用才可以 指定扫描范围 指定范围扫描自定义策略 ??? scoped-proxy ??? 注意: 这个还支持过滤功能 ??? 增加支持过滤的表格信息 @Bean 注册bean 定义的时候提供限制符号 spring 自动化配置 ??? 移动到上面和bean整理到一起spring bean自动化配置的承载类 AnnotationConfigApplicationContext这个ApplicationContext其实承载了java config类的解析功能 @bean 描述依赖关系在使用@Bean来描述依赖关系的时候,可以使用对应方法中的参数来进行依赖 @Bean 生命周期依赖 bean范围??? @Scope 和 scoped-proxySpring提供了一种通过作用域代理处理作用域依赖性的便捷方法 。使用XML配置时创建此类代理的最简单方法是aop:scoped-proxy/元素。使用@Scope注释在Java中配置bean 可以为该proxyMode属性提供同等的支持。默认值为无代理（ScopedProxyMode.NO），但您可以指定ScopedProxyMode.TARGET_CLASS或ScopedProxyMode.INTERFACES。 自定义Bean命名 bean @Description提供bean的描述功能 @Configurationconfiguration import 加载其他类定义 灵活的注入配置@Conditional","date":"2024-12-31","categories":["java"]},{"title":"spring-boot2.0-(1)启动、运行和结束","url":"/2024/12/31/spring-boot2-0--1-启动-运行和结束/","content":"导引： spring boot的简单配置，基本启动方法，和特殊关闭模式 springboot 配置的核心元素spring 只用maven的parent继承的方法（gradle类似）进行依赖管理和使用maven插件（gradle同样类似）的方法生成可执行的jar文件 当不使用parent进行依赖配置的时候可以使用dependencyManagement来进行管理 springBoot starter-springBoot可以实现自动化配置的核心方法springBoot提供了一系列的starter方便进行自动化配置，比如spring-boot-starter-web springBoot starter 明明规则：如果是官法的starter 将会按照spring-boot-starter-* 来进行划分，如果是自己实现的starter方法，建议按照*-spring-boot-starter 来进行命名 这里介绍一个特边的非应用starter，spring-boot-starter-actuator，这个starter实现了监控的功能 spring boot 提供了非常多的配置选项，具体的可以到官网上查询 spring boot 热部署工具 Developer Toolsspring boot 实现了一定程度下的热部署功能，暂时不做过多的讨论配置 spring boot 的启动方法 注意这里传递过去的参数是一个配置源，也就是说这个类必须是@Configuration或者是一个继承@Configuration注解标记的类 当然，也可以通过使用application.properties文件来配置SpringApplication，但是这里涉及到springboot外部化配置，见下面 springboot 还可以使用流式布局构建application的上下文关系，具体的实现如下 注意：Web组件必须包含在子上下文中，并且父环境和子环境都使用相同的环境 #spring boot 带有状态的结束 使用exit 运行程序 在调用SpringApplication.exit（）时希望返回特定的退出代码，那么bean可以实现org.springframework.boot.ExitCodeGenerator接口。 ，然后可以将此退出代码传递给System.exit（）以将其作为状态代码返回，","date":"2024-12-31","categories":["java"],"tags":["spring"]},{"title":"spring-boot2.0-(2)监听器、事件","url":"/2024/12/31/spring-boot2-0--2-监听器-事件/","content":"导引： 1. spring boot监听事件的使用 2. spring boot 内置监听事件初始化applicationContext 3. CommandLineRunner接口和ApplicationRunner类 spring boot 中的上下文监听器和事件spring boot的事件和 spring原生的事件没有什么差别但是，spring boot 有一些自己的事件 ApplicationStartingEvent在运行开始时但在任何处理之前发送，除了注册侦听器和初始化器之外。 ApplicationEnvironmentPreparedEvent当在上下文中使用的环境是已知的但在创建上下文之前发送。 ApplicationPreparedEvent在刷新开始之前但在bean定义加载之后发送。 ApplicationStartedEvent在刷新上下文之后但在调用任何应用程序和命令行参赛者之前发送。 ApplicationReadyEvent在任何应用程序和命令行参数被调用后发送。 它表示应用程序已准备好为请求提供服务。 ApplicationFailedEvent如果启动时出现异常，则发送。 注意有些事件可能是在applicatincontext之前就出发了，所以不能使用@bean的方法注册在容器中，在spring boot 中可以应如下的方法注册监听器,使用方法：SpringApplication.addListeners(…​)， SpringApplicationBuilder.listeners(…​)，配置文件中添加：META-INF&#x2F;spring.factories 中 org.springframework.context.ApplicationListener 注意了：因为spring拥有上下文的关系，而监听器除了监听自己的时间还会监听子元素的事件，所以在实现监听器的时候官方推荐同时实现ApplicationContextAware接口，来比较事件的context和和自身context的关系。 总结添加事件的三种方法 @bean SpringApplication.addListeners(…​)， SpringApplicationBuilder.listeners(…​) 配置文件中添加：META-INF&#x2F;spring.factories 中 org.springframework.context.ApplicationListener spring boot 应用环境的判断构建ApplicationContext我们在没有使用spring boot 单纯的使用spring 或者使用spring mvc 的时候通常使用的application是XmlServletWebServerApplicationContext，或者ClasspathXmlApplicationContext ， 同样spring boot 基于注解方法使用了一套特殊的context 首先spring boot 判断这个环境是不是spring mvc 环境如果是马么webappcationcontext 选择AnnotationConfigServletWebServerApplicationContext 否则判断是否是webflux 环境如果是 就使用AnnotationConfigReactiveWebApplicationContext 最后如果都不是，使用AnnotationConfigApplicationContext 这样有一个问题，如果是spring mvc 和spring webflux 同时使用的时候将会造成使用springmvc 的环境，为了解决这个问题，可以使用setWebApplicationType(WebApplicationType)方法，或者使用setApplicationContextClass(…​)有待确认. spring bootW启动前执行特定的程序在spring boot中 ， spring boot 会在容器彻底启动前运行实现现CommandLineRunner接口的bean或者ApplicationRunner接口的bean 注意这两个接口其中第二个接口传递命令的额外参数使用的是ApplicationArguments这个类（第一个和main相同） 注意如果系统想使用，运行的时候传入的数据的话，可以使用ApplicationArguments 这个参数将相关的值注入进来或者使用ApplicationRunner接口或者CommandLineRunner初始化","date":"2024-12-31","categories":["java"],"tags":["spring"]},{"title":"spring-boot2.0-(3)配置读取","url":"/2024/12/31/spring-boot2-0--3-配置读取/","content":"导引： spring boot 总体上提供了三种外化配置 1. 使用@Value注释将属性值直接注入到bean中 2. pring的Environment抽象访问 3. 通过@ConfigurationProperties绑定到结构化对象，这里介绍一下 spring boot 可以提供参数方式和覆盖原则spring boot 可以通过如下的方法倒入相应的属性之，并且按照如下从上到下的顺序进行覆盖 Devtools 主目录上的全局设置属性（~&#x2F;.spring-boot-devtools.properties当devtools处于活动状态时）。 @TestPropertySource 测试上的注释。 @SpringBootTest#properties 测试中的注释属性。 命令行参数。 来自SPRING_APPLICATION_JSON（嵌入在环境变量或系统属性中的内联JSON）的属性。 ServletConfig init参数。 ServletContext init参数。 JNDI属性来自java:comp&#x2F;env。 Java系统属性（System.getProperties()）。 OS环境变量。 一RandomValuePropertySource，只有在拥有性能random.*。 特定于配置文件的应用程序属性在打包的jar（application-{profile}.properties和YAML变体）之外。 打包在jar中的特定于配置文件的应用程序属性（application-{profile}.properties 以及YAML变体）。 应用程序属性在打包的jar之外（application.properties和YAML变体）。 打包在jar中的应用程序属性（application.properties和YAML变体）。 @PropertySource 你的@Configuration课上的注释。 默认属性（由设置指定SpringApplication.setDefaultProperties）。 提供一个简单的使用内置属性的列子 @Value 将会将环境中的name参数注入到MyBean的name属性中 注意: 这里解释一下上面的第五条： SPRING_APPLICATION_JSON 这个属性是一个环境变量属性，可以在环境中进行相关的配置，或者直接使用命令行$ SPRING_APPLICATION_JSON&#x3D;’{“acme”:{“name”:”test”}}’ java -jar myapp.jar。 注意： json的方式解析在spring boot 中拥有两个地方，第一个上面的第9条：java -Dspring.application.json&#x3D;’{“name”:”test”}’ -jar myapp.jar 。第二个上面的第4条，$ java -jar myapp.jar –spring.application.json&#x3D;’{“name”:”test”}’ 注意： 解释上面的11 条随机参数 这个方法可以生成随机参数可以生成整数，长整数，uuids或字符串，如以下示例所示： 注意最后两个使用了表达式的方法，int[start,end] 表示从start到end区间随机取数 int(max) 这种方式表示取的值不大于max","date":"2024-12-31","categories":["java"],"tags":["spring"]},{"title":"spring-boot2.0-(4)自定义配置(starter)","url":"/2024/12/31/spring-boot2-0--4-自定义配置-starter-/","content":"spring boot 提供了非常方便的方法可以自动配置javabean spring boot自动的配置的核心一个基本注解@Configuration和条件注解@Conditionalspring的自动配置是使用上面的两个bean提供功能的 @Configuration 标记这个类是一个config配置类 @Conditional 这个是控制自动配置条件的类 spring boot spispring boot自己实现了一套spi机制, spring boot 要求在META-INF中生成一个配置文件","date":"2024-12-31","categories":["java"],"tags":["spring"]},{"title":"spring5-(2)-core核心注解使用","url":"/2024/12/31/spring5--2--core核心注解使用/","content":"spring 注解配置方法 所谓零配置，并不是说一点配置都没有了，而是配置很少而已。通过约定来减少需要配置的数量，提高开发效率。更厉害的是spring boot xml配置文件 该隐式注册的后处理器包括 AutowiredAnnotationBeanPostProcessor， CommonAnnotationBeanPostProcessor， PersistenceAnnotationBeanPostProcessor，以及前述 RequiredAnnotationBeanPostProcessor spring 类上的注解这几个注解在使用上其实是一样只是使用不同的名称来体现可读性 类属性的注解 @Scope注解 引申: 这个注解存在一个特殊的属性proxyMode 这个属性相当于 aop:scoped-proxy/ 这个属性 不过使用的更加方便, 其中的ScopedProxyMode 提供了三种属性 DEFAULT 默认值(使用cglib代理,详单于CLASS_TYPE属性),NO不进行代理(作用于控制取决于父辈),INTERFACE(使用jdk代理),CLASS_TYPE(使用cglib代理) 类中元素上的注解 @Autowired 使用方法引申 ：这个注解可以使用在变量 set 或者普通方法上，他会自动的将相关的属性注入进去，如果有多个匹配，可以使用list，数组，set，map进行接收，否则将会报错，如果想要诸如的bean按照顺序，可以使用@order或者@Priority在bean上指定顺序 类中方法上的注解 @Resource, @Autowired 和 @Qualifier自动注入使用 @Resource： 默认使用name属性进行自动装配，@Resource 没有指定 name 属性，那么使用 byName 匹配失败后，会退而使用 byType 继续匹配，如果再失败，则抛出异常，将其标注在 BeanFactory 类型、ApplicationContext 类型、ResourceLoader 类型、ApplicationEventPublisher 类型、MessageSource 类型上，那么 Spring 会自动注入这些实现类的实例，不需要额外的操作。 @Autowired 和 @Qualifier 注解执行自动装配： 只能是根据类型进行匹配 可以用于 Setter 方法、构造函数、字段，甚至普通方法，前提是方法必须有至少一个参数 可以用于数组和使用泛型的集合类型。然后 Spring 会将容器中所有类型符合的 Bean 注入进来。 标注作用于 Map 类型，将容器中所有类型符合 Map 的 value 对应的类型的 Bean 增加进来，用 Bean 的 id 或 name 作为 Map 的 key 5.@Autowired 后面增加一个 @Qualifier 标注，提供一个 String 类型的值作为候选的 Bean 的名字 5.0 新注解 使用@Nullable 可以忽略@Autowired 应在函数上面的时候的参数 @Autowired对于那些众所周知的解析依赖接口：BeanFactory，ApplicationContext，Environment，ResourceLoader， ApplicationEventPublisher，和MessageSource。这些接口及其扩展接口（如ConfigurableApplicationContext或ResourcePatternResolver）会自动解析，无需特殊设置。 @Autowired，@Inject，@Resource，和@Value注释由Spring处理 BeanPostProcessor实现，也就是说不可以使用以上的注解去自动装配BeanPostProcessor或BeanFactoryPostProcessor类型（如果有的话）。这些类型必须通过XML或使用Spring @Bean方法明确地手动地进行配置 。 5,0新注解 使用@Primary微调基于注释的自动装配 这里指定了相关的java配置类返回的参数 在xml文件中进行如下配置 @Qualifier注解 为spring 容器自动装配提供更多选项这个注解可以进行派生 可以让autowirte和自定义的genre动态的进行连用，从而提高自动装配的灵活性 或者使用xml进行配置，qualitier 这个标签的attribute中使用kye，value唯一定位一个属性如果，自定义注解的时候没有相关的内部属性，可以直接使用type来唯一的限定一个标记 @Autowrite注解关联范型（模糊类型增强）举例javabean 新service 旧service 范型注入改进 改进：不需要在set方法上在进行一次封装，直接使用@Autowrite进行注入就好了 提供map和list注入 map会这样注入：key是bean名字；value就是所有实现了BaseService的Bean list会这样注入：这样会注入所有实现了BaseService的Bean；但是顺序是不确定的，如果我们想要按照某个顺序获取；在Spring4中可以使用@Order或实现Ordered接口来实现（指定加载的顺序） spring5.0 的增强注解 @AliasFor 这个注解实现了注解继承中的属性继承,从此 spring组合注解不在像之前需要在继承的注解的中写入值,而是可以在注解中进行自定义的操作 这个注解还有一些属性 比如 annotation 哪一个接口(注意这个接口必须是继承的否则会出错) value和attribute(这两个属性表示指定继承注解中哪一个属性,如果没有那就和指定的方法名称同名) 生命周期控制 配置类注解@Configuration AnnotationConfigApplicationContext 和使用xml进行配置的方法不同,使用configuration基于java配置的时候需要使用这个context,这个context可以在new传入几个javabean,每一个javabean将会按照顺序的进行配置,或者不传入使用手动注册 使用配置文件的方法只是一种简单的方法,这种方法无法使用xml中的模版依赖这种特性,spring不知用cglib这种代理,而是用一种类似工厂方法的方式将需要的bean注入到容器中. 相关的注解 @ComponentScan 类扩展@ComponentScan 这个直接和配置文件中的自动扫描配置相同 支持的方法有： 类型 示例表达 描述 annotation(包名称) org.example.SomeAnnotation 要在目标组件中的类型级别出现的注释。 assignable(指定类)org.example.SomeClass 目标组件可分配给（扩展&#x2F;实现）的类（或接口）。 aspectJ(aop 路径配置) org.example..*Service+ 要由目标组件匹配的AspectJ类型表达式。 regex(正则表达式) org.example.Default.* 要由目标组件类名匹配的正则表达式。 custom org.example.MyTypeFilter org.springframework.core.type .TypeFilter接口的自定义实现。 例子 注意上面这里有一个内部接口注解 @Filter 这个方法可以指定过滤的级别 自定的扩展: 如果你不想依赖默认的bean命名策略，你可以提供一个自定义的bean命名策略。首先，实现 BeanNameGenerator 接口，并确保包含一个默认的无参数构造函数。 要为范围解析提供自定义策略，而不是依赖基于注释的方法，请实现 ScopeMetadataResolver 接口，并确保包含默认的无参数构造函数 scopedProxy :当使用非单利作用域的时候,可以指定代理的方法 no ,interface ,targer_class xml配置方法 @Bean 使用扩展 这个注解相当于 xml中的标签 在类中使用的方法 看上面的例子, 注意 @Bean 使用指定的方法,可以存在参数,spring将会自动的注入需要的属性,同样可以使用@Autowrite @Bean 注解可以指定函数的构造方法或者析构方法 可以指定这个并的作用域@Scope @Bean 中可以指定一组的名称指定别名 config 类中实现javaBean的依赖可以直接的调用相关的参数,可以使用@Qualitfier使用id指定需要注入的元素，否则将使用type进行依赖的注入 请注意，单个类可以@Bean为同一个bean 保存多个方法，作为根据运行时可用依赖项使用多个工厂方法的安排。这与在其他配置方案中选择“最贪婪”构造函数或工厂方法的算法相同：将在构造时选择具有最多可满足依赖项的变体，类似于容器在多个@Autowired构造函数之间进行选择的方式。 特殊性 configuration 类 在spring内部使用的cglib方法进行构建的,导致下面的ClientDao 将不会出现数据对象的情况 注意 @configuration 注解比较特殊 系统使用cglib进行初始化而component系列的使用的和xml相同的方法 @value 标签使用spEL 表达式进行配置 使用@Conditional注解实现更加颗粒化的控制(在springboot 自定义starter中还会有说)之前看过了一个注解@Profile 这个注解看一下源代码 这个注解其实是一个组合注解,有一个重点的注解就是上面的@Conditional和传入的里面的ProfileCondition.class这个类,接下来看看这个类 这个类实现了一个Condition 接口,这个接口有一个方法matches() 通过返回false或者true来告诉spring容器要不要注入的这个bean,context 就是spring的上下文信息,我可以通过这个方法进行相关容器的配置.metadata实现了注解的相关的数据. @Profile 实现环境控制上面我们已经知道了 这个注解其实是一个Condition注解的一个子注解,他可以通过 环境来判断那些属性需要被注入的相关的属性中 注意这个方法context.getEnvironment() 这个方法将会将会获取系统中相关的环境信息获取出来 添加环境变量属性 第一种方法, 使用context 添加相关的属性 spring.profiles.active 使用配置文件配置属性 使用java 的启动参数 -Dspring.profiles.active&#x3D;”profile1,profile2” 建议使用注解的方法 ,@PropertySource(“classpath:&#x2F;com&#x2F;myco&#x2F;app.properties”)ctx.getEnvironment().setActiveProfiles(“profile1”, “profile2”); 使用value 实现特殊的表达式注入xml文件 配置文件 一个例子 注意:BeanPostProcessor和BeanFactoryPostProcessor 应该声明为static @Bean方法 , 防止processor生效之前有数据被初始化 spring lookup 方法注入注解@Lookup具体的功能和xml相同，LookUp提供了两种参数当有参数的时候将会自动寻找容器中beanid对应参数的bean，否则将会基于返回类型进行添加","date":"2024-12-31","categories":["java"],"tags":["spring"]},{"title":"spring5-(3)-生命周期,容器扩展,lookup模式,事件","url":"/2024/12/31/spring5--3--生命周期-容器扩展-lookup模式-事件/","content":"spring javabean的生命周期\\容器扩展\\lookup方法\\spring事件javabean 生命周期控制 使用配置文件的方法 注意 标签中可以配置全局使用的初始化方法和销毁方法 使用注解和接口相关的配置方法 注意：接口的执行优先级无论是初始化或者销毁都比配置文件放有限执行, SmartInitializingSingleton 所有非lazy单例Bean实例化完成后的回调方法 spring 中bean的生命周期的配置除了使用 xml 指定 的方法之外还能使用注解的方法进行相关的配置 同时spring还提供了让bean和容器进行相关关联bean 方法 所有的bean都可以实现这些接口，然后在容器进行增加或者删除的时候将会自动的执行相关的方法，注意SmartLifecycle 接口中的stop方法将会传入一个实现runnabe接口的类，容器在进行销毁之后将会自动的异步执行这个类中的相关的run方法，Phased的getphase将会设置一个等级，用来表示优先级 引申 ：spring容器可以使用ctx.registerShutdownHook(); 方法实现优雅停机操作 spring 后置处理器 (容器扩展方法) BeanPostProcessors接口 如果这个接口的某个实现类被注册到某个容器，那么该容器的每个受管Bean在调用初始化方法之前，都会获得该接口实现类的一个回调。容器调用接口定义的方法时会将该受管Bean的实例和名字通过参数传入方法，进过处理后通过方法的返回值返回给容器。 要使用BeanPostProcessor回调，就必须先在容器中注册实现该接口的类，BeanFactory和ApplicationContext容器的注册方式不大一样： 若使用BeanFactory，则必须要显示的调用其addBeanPostProcessor()方法进行注册，参数为BeanPostProcessor实现类的实例. 如果是使用ApplicationContext，那么容器会在配置文件在中自动寻找实现了BeanPostProcessor接口的Bean，然后自动注册，我们要做的只是配置一个BeanPostProcessor实现类的Bean就可以了。 假如我们使用了多个的BeanPostProcessor的实现类，只要实现Ordered接口，设置order属性就可以确定不同实现类的处理顺序了。 注意其中的after和before的方法,这两个方法将会在声明周期的初始化之前和初始化之后进行调用,传入的bean 就是 需要使用的Bean 传入的beanName就是配置的beanname , 一般就是对指定的相关的bean进行处理 BeanFactoryPostProcessor接口 BeanFactoryPostProcessor接口实现类可以在当前BeanFactory初始化后，bean实例化之前对BeanFactory做一些处理。BeanFactoryPostProcessor是针对于bean容器的，在调用它时，BeanFactory只加载了bean的定义，还没有对它们进行实例化，所以我们可以通过对BeanFactory的处理来达到影响之后实例化bean的效果。跟BeanPostProcessor一样，ApplicationContext也能自动检测和调用容器中的BeanFactoryPostProcessor。接口的信息如下: 在spring中有一些特殊的操作就是使用BeanFactoryPostProcessor接口，比如类名替换PropertyPlaceholderConfigurer。他可以自动将相关的java properties信息替换成需要的,这里分析一下这个类: 接口:InitializingBean — 只有一个方法afterPropertiesSet 当bean实例化之后将会调用这个方法,我们可以在这里初始化一下属性接口:order 可以指定优先级接口:BeanNameAware和BeanFactory 配置配置回调将容器的和配置的相关参数传入类: PropertiesLoaderSupport,PropertyResourceConfigurer 将配置文件中的参数进行绑定,提供配置文件解析功能类:PlaceholderConfigurerSupport 真正的实现插入方法,这个类最核心的方法: 遍历给定的BeanDefinition对象和它们中包含的MutablePropertyValues和ConstructorArgumentValues。 注意其中有BeanFactoryPostProcessor的使用方法 其中的postProcessBeanFactory这个方法中的参数是决定BeanFactoryPostProcessor强于BeanPostProcessor的关键之处,其中有一个参数:ConfigurableListableBeanFactory,这个参数可以进行配置相关的配置首先具体见下 这个接口只有一个实现类:DefaultListableBeanFactory 看一下关系继承图 重点关注这个接口:ConfigurableListableBeanFactory,这个接口继承了ListableBeanFactory, AutowireCapableBeanFactory, ConfigurableBeanFactory 这三个接口 BeanFactory 主要提供getbean ,和isSingleType 这样的方法 ListableBeanFactory 这个接口是BeanFactory接口的实现之一(还有HierarchicalBeanFactory,AutowireCapableBeanFactory),这个主要是获取容器中各种相关javabean属性的方法,比如有 getBeansOfType,getBeansWithAnnotation 这种方法, AutowireCapableBeanFactory 提供自动装配扩展(用来分层将功能细化) HierarchicalBeanFactory 其中有一个方法,getParentBeanFactory 这里提供分级功能,实现了这个接口可以获取夫工厂方法 ConfigurableListableBeanFactory 这个接口同时继承了ListableBeanFactory,AutowireCapableBeanFactory,HierarchicalBeanFactory. 如果看一些上面这些类就会有一个大致上的理解了 spring 通过各种继承和接口扩展了这个类的使用方法,将相关的各种功能尽心组装(一个接口专注一个点记性细化,上层实现,典型的oop思想) ConfigurableListableBeanFactory 接口继承了上面的这些接口,所以他拥有获取所有的相关的javaBean集合的能力,并且拥有在初始化之前对属性进行操作的能力. 单例模式下实现内部变量非单例配置 之前在博客中做过测试，在spring容器中单例模式注入非单例模式属性的时候其实各个元素注入的是相同的类（使用工厂方法进行注入的时候也是如此）（&#x3D;&#x3D;返回true），想要获取不同的属性就要使用lookup标签 配置文件 spring框架在抽象类中将会自动使用动态代理实现这个抽象方法，讲指定的bean对象进行返回如果对象实现了借口 spring框架将会使用jdk代理 否则使用cglib代理 推荐使用接口 Lookup类 javabean spring message机制该ApplicationContext接口扩展了一个称为的接口MessageSource，因此提供了国际化（i18n）功能。Spring还提供了HierarchicalMessageSource可以分层解析消息的接口。这些接口一起为Spring特效消息解析提供了基础。这些接口上定义的方法包括： String getMessage(String code, Object[] args, String default, Locale loc)：用于从中检索消息的基本方法MessageSource。如果未找到指定语言环境的消息，则使用默认消息。使用MessageFormat标准库提供的功能，传入的任何参数都将成为替换值。 String getMessage(String code, Object[] args, Locale loc)：与前面的方法基本相同，但有一点不同：不能指定默认消息; 如果无法找到消息，NoSuchMessageException则会抛出a。 String getMessage(MessageSourceResolvable resolvable, Locale locale)：前面方法中使用的所有属性也都包含在一个名为的类中 MessageSourceResolvable，您可以使用该方法。 注意:当一个ApplicationContext被加载时，它会自动搜索上下文中实现MessageSource 接口的bean。这个bean的名字必须是messageSource,如果 ApplicationContext无法找到任何消息源，DelegatingMessageSource则会实例化一个空 以便能够接受对上面定义的方法的调用。 Spring提供了两个MessageSource实现，ResourceBundleMessageSource并且 StaticMessageSource。两者都是HierarchicalMessageSource为了做嵌套消息传递而实现的。这StaticMessageSource是很少使用，但提供了编程方式来添加消息到源。在ResourceBundleMessageSource被示出在下面的例子： 例子中 标签中的数据表示的是数据名称 对应的文件是 format.properties文件,举例子为format.properties中的信息 使用这个方法 输出为 spring事件 Event 其实就是实现一个event 和 一个listener的过程 event 一个事件 定义一个event事件:并不需要在配置文件中进行配置，是需要使用application手动发出 实现 ApplicationEvent接口 spring 发布一个一个event事件 在spring中任何一个可以发布事件的方法都实现了一个ApplicationEventPublisher接口,applicationContext就是其中的一个实现类 我们在自己进行开发的时候并不需要实现这个接口只要实现一个ApplicationEventPublisherAware , spring 容器就回自动的将相关的属性注入到这个这个bean中 spring监听event ApplicationListener 这个监听器需要在bean中进行配置，spring容器会自动的处理event xml配置文件 spring bean获得容器和自身相关属性spring 中bean获得spring 需要实现一下两个接口 ApplicationContextAware BeanFactoryAware spring bean获得自身的相关配置属性使用如下的接口 BeanClassLoaderAware BeanNameAware","date":"2024-12-31","categories":["java"],"tags":["spring"]},{"title":"spring5-AOP","url":"/2024/12/31/spring5-AOP/","content":"AOP 是一种新的属性配置方法这种方法不同与OOP，不是使用继承的方法来增强相关的参数而是使用横向的切面编程的方法来进行相关增强的 spring中使用xml进行配置的方法切面类 注意： around方式的方法需要第一参数为ProceedingJoinPoint 类型 ，当需要接受参数的时候需要在 exection方法中进行相关的配置 被切入类 xml配置文件 测试类 输出结果 使用注解方法进行配置增强类 切面类 切入点表达式 支持如下的几个,with(examples.chap03.Horseman) 指定类名称的所有方法,通过类名指定，同时包含所有子类target(examples.chap03.Horseman),args(java.util.String) 指定参数(如果使用的不是基本类型,那就会匹配对应的变量名称),annotation(org.springframework.transaction.annotation.Transactional) 指定标注了指定的注解的方法,这些参数必须是全限定名称 xml文件配置文件 注意上面的配置方法其实可以替换成如下的方法 注意:完整的AspectJ切入点语言支持Spring中不支持的其他切入点指示符。这些是：call, get, set, preinitialization, staticinitialization, initialization, handler, adviceexecution, withincode, cflow, cflowbelow, if, @this，和@withincode 注意: 使用注解的缺点,支持“singleton”方面的实例化模型，并且不可能组合使用XML声明的命名切入点。使用XML方法的缺点是您无法通过组合这些定义来定义切入点。 注意: spring不支持final的调用,应为final不支持覆盖 注意当在代理中使用方法的方法的时候不能使用代理自己的方法,因为解析器会使用this.xxx()来运行 而不是使用 proxy来运行,所以应该使用如下的方法进行代理 注意: 使用的表达式格式 - execution(modifiers-pattern? ret-type-pattern declaring-type-pattern?name-pattern(param-pattern) throws-pattern?)","date":"2024-12-31","categories":["java"],"tags":["spring"]},{"title":"spring5-SpEL表达式","url":"/2024/12/31/spring5-SpEL表达式/","content":"spring5-SpEL表达式SpEL是一种强大的、简洁的装配Bean的方式，它通过运行期执行的表达式将值装配到Bean的属性或构造器参数中。比如说 xml文件中动态的加入上下文信息，使用类的静态方法等相关方法载入参数 载入基本类型 使用xml配置文件中的其他配置选项 获取poet中传入的poem对象的两种方法 调用方法调用其他 名字为poet bean的方法 上面使用 ?. 运算符代替点（.）来访问toUpperCase()方法。在访问邮编方法之前，该运算符会确保左边项的值不为null。所以，如果selectorSong返回null，SpEL就不再尝试调用toUpperCase()方法。 在SpEL中，使用T()运算符会调用类作用域的方法和常量 使用逻辑算数运行算数表达式 操作一个表达式的值：eq(&#x3D;&#x3D;),lt(<),le(<&#x3D;),gt(>),ge(>&#x3D;)。逻辑表达式：and,or,not或!。条件运算符：使用三元运算符 ？： 正则表达式匹配","date":"2024-12-31","categories":["java"],"tags":["spring"]},{"title":"spring5-SpEL表达式的使用","url":"/2024/12/31/spring5-SpEL表达式的使用/","content":"SpEL 表达式的使用SpEl 表达式主要用在bean的定义是上 基于xml的配置 基于注解的配置 基于注解的配置在使用自动装配的方法和构造函数时使用@Value注解配套方法 语言参考spring 内置了一个解析器SpelExpressionParser 基本类型表达式 获取对象的属性 获取数组的属性 内联map 调用方法 运算符解析 注意null 在这里视为0 和 空 1> null 返回true -1>null 返回false 注意:每个符号运算符也可以被指定为纯粹的字母等效。这避免了所使用的符号对嵌入表达式的文档类型（例如XML文档）具有特殊含义的问题。文本等价物如下所示：lt（<），gt（>），le（⇐），ge（>&#x3D;），eq（&#x3D;&#x3D;）， ne（!&#x3D;），div（&#x2F;），mod（%），not（!） 逻辑运算符 数学运算符 属性赋值 构造函数 三元运算符和猫王运算符 集合选择 着表达式将会选择 List Menbers中 Nationality 为 Serbian的所有元素 引申:除了返回所有选定的元素之外，还可以检索第一个或最后一个值。要获得与选择相匹配的第一个条目，语法使用^[…​],要获取语法的最后匹配使用$[…​]。 在字符串中嵌入表达式模版","date":"2024-12-31","categories":["java"],"tags":["spring"]},{"title":"spring5-beanfactory系列接口的分析","url":"/2024/12/31/spring5-beanfactory系列接口的分析/","content":"","date":"2024-12-31","categories":["java"],"tags":["spring"]},{"title":"spring5-condition注解实现bean细致控制","url":"/2024/12/31/spring5-condition注解实现bean细致控制/","content":"","date":"2024-12-31","categories":["java"],"tags":["spring"]},{"title":"spring5-core核心配置文件使用","url":"/2024/12/31/spring5-core核心配置文件使用/","content":"依赖注入和控制反转IOC inversion of control 控制反转 DI Dependency Injection 依赖注入 依赖注入当然是某个对象依赖于IoC&#x2F;DI的容器，对象需要IoC&#x2F;DI的容器来提供对象需要的外部资源，IoC&#x2F;DI的容器 注入 某个对象并且注入某个对象所需要的外部资源，由IoC&#x2F;DI的容器来控制对象了，根本上是控制对象实例的创建 控制反转如果要在A里面使用C，你会怎么做呢？当然是直接去创建C的对象，也就是说，是在A类中主动去获取所需要的外部资源C，这种情况被称为正向的。那么什么是反向呢？就是A类不再主动去获取C，而是被动等待，等待IoC&#x2F;DI的容器获取一个C的实例，然后反向的注入到A类中 spring IOC bean 配置文件 注意：不使用构造注入的时候，将会使用setter注入所以必须提供setter函数 spring 自动配置内部类内部类注入方式一：添加内部类默认构造函数参数非静态的内部类默认的构造函数有一个参数，这个参数指向其外部类的实例，所以我们需要给此内部类的bean添加constructor-arg节点，并指向外部类即可，配置文件： 内部类注入方式二：将内部类修改为static这个使用不需要访问外部，所以就和外部类型等同了 注意spring 声明内部类的时候需要特殊处理,需要通过外部类$内部类名来使用 parent abstract和list map set Properties 属性使用 parent 和 abstract 只是相当于在配置文件中指定一个参数的继承方法，相当于实现一个参数的模板，而集合类型的配置，例子中使用了基本类型使用value标签进行配置其实和可以传入bean的引用。 获取bean容器使用applicationContext 获得 spring 容器 而 BeanFactory是 application的父类 二者区别 applicationContext接口,它由BeanFactory接口派生而来，因而提供BeanFactory所有的功能。ApplicationContext以一种更向面向框架的方式工作以及对上下文进行分层和实现继承，ApplicationContext包还提供了以下的功能： MessageSource, 提供国际化的消息访问 资源访问，如URL和文件 事件传播 载入多个（有继承关系）上下文 ，使得每一个上下文都专注于一个特定的层次，比如应用的web层 其他区别： BeanFactroy采用的是延迟加载形式来注入Bean的，即只有在使用到某个Bean时(调用getBean())，才对该Bean进行加载实例化，这样，我们就不能发现一些存在的Spring的配置问题。而ApplicationContext则相反，它是在容器启动时，一次性创建了所有的Bean。这样，在容器启动时，我们就可以发现Spring中存在的配置错误。 BeanFactory和ApplicationContext都支持BeanPostProcessor、BeanFactoryPostProcessor的使用，但两者之间的区别是：BeanFactory需要手动注册，而ApplicationContext则是自动注册 autowrite + autowirte-candidate lazy-init（spring只用在使用的时候才会加载bean） 通过 名称或者类型自动装载但是要注意一个问题就是唯一性（姓名的唯一性和类型的唯一性），注意，在xml中使用autowrite的时候如果使用的list集合就可以出现相同的类型的情况，spring会将所有的正确的类型注入到集合中 autowrite : 只用自动装配的方法autowrite-candidate：忽略自动装配（也就是是说spring在查找可自动装配的候选项时忽略这个选项，就是不能被自动装配） scopespring有 如下的作用域（在作用域内默认使用单例），表示这个对象在这一个区域的独立性（比如单例在不同的区域就是表现的不同），singleton ， prototype，request ，session ，application（servletcontext做用域），websocket后面的四种只用在使用springmvc或者进行一些配置并且使用的web应用的时候才有效，配置如下： 注意了：如果长的scope引用短的scope中的bean 将会导致短的scope在长的scope中无法随着短的scope生命周期的结束而结束，而是以长的scope作为标准，如果想要以完美的方法需要进行如下的配置，添加aop:scoped-proxy/ 这种方法让传入长的scope中的对象不是短的scope的bean而是这个短的scope的代理,使用的是cglib（aop），这个代理只随着session的创建而创建，销毁而销毁。注意cglib只能代理公有方法，所以不要调用非公有方法 也可以使用jdk代理 <aop:scoped-proxy proxy-target-class&#x3D;”false”&#x2F;>， 这个方法将会自动的使用的jdk代理。 一个例子：配置文件： javabean： 测试类： 注意一个结论：注意使用原型模式注入但单例模式的时候,或者使用单例模式注入原型模式的时候，一次操作中注入的对象是相同的 ，如果想要让单例模式注入原型模式的时候取得数据为原型模式就要使用lookup方法 lookup method使用lookup 方法，其实本质上就是让一个单例bean使用一个原型bean的时候，去实现这个单例bean中的一个抽象方法，让这个抽象方法返回一个单例bean 一个有抽象方法的单例bean 对应的原型bean main函数中的方法 配置文件 输出结果 可以看出BeanTwo的结果是不同的 factory-method factory-bean constructor-arg 这个工厂方法并不是我们去使用的而是让spring容器使用的工厂方法，spring容器使用这个工厂方法进行实例的生成，在spring中使用构造注入的形式调用指定的工厂法，并传入属性(可以理解成使用方法作为构造函数进行注入)，引申spring也能使用构造注入但是需要提供对应的构造函数 spring 生命周期管理，初始化和销毁实现方法 继承并且实现 InitializingBean，DisposableBean 接口 在xml文件中bean的属性指明 destroy-method&#x3D;”” init-method&#x3D;”” 对应的方法 使用注解 @PostConstruct and @PreDestroy @PreConstruct-容器在卸载这个bean的时候将会调用的方法 spring框架 通过BeanPostProcessor 类来实现生命周期的管理的。 引申优雅关闭spring容器 如果使用的web项目使用webapplication的时候并不需要特殊配置spring框架自动的将相关的各种配置都实现了如果使用的是非web项目那么需要手动进行配置，通过ConfigurableApplicationContext 类进行配置，使用registerShutdownHook方法优雅停机 引申一个:使用context简化配置操作 特殊用法 metho完全替换使用基于XML的配置元数据，您可以使用被替换的方法元素将已有的方法实现替换为已部署的bean 比如需要替换MyValueCalculator类的cmputeValue方法 MyValueCalculator类 替换的方法需要继承MethodReplacer接口并且在xml中进行配置","date":"2024-12-31","categories":["java"],"tags":["spring"]},{"title":"spring5-springMVC使用简介","url":"/2024/12/31/spring5-springMVC使用简介/","content":"spring5-springMVC使用使用一个基本demo进行相关的整理 java controller 其他一些注解 springMVC webapplication 配置文件 web.xml配置文件 springMVC过滤器 相关流程简介","date":"2024-12-31","categories":["java"],"tags":["spring"]},{"title":"spring5-springMVC异步请求支持","url":"/2024/12/31/spring5-springMVC异步请求支持/","content":"pring5-springMVC异步请求支持提供AsyncRestTemplate用于客户端非阻塞异步支持 服务端 客户端 注意:这个方法在spring5中是过期的，使用 WebClient","date":"2024-12-31","categories":["java"],"tags":["spring"]},{"title":"spring5-springMVC详解","url":"/2024/12/31/spring5-springMVC详解/","content":"说道spring的web支持首先就可以想到了spring MVC 的技术(其他的技术还有spring的webflux 以后讨论),从这片博客开始要进行相关知识点的整理. spring MVC 整体的架构设计 spring 自己的webapplication支持嵌套作用域,通过这个方法可以实现spring applicationcongtext的继承特性(继承特性,子作用域可以访问夫作用域的中的属性,但是父作用域中的属性无法访问子作用域中的属性,具体的使用看HierarchicalBeanFactory) web容器的初始化设置springMVC 支持使用xml进行配置 从spring5.0 开始 spring官方文档提倡使用接口配置,容器在初始化的时候将会自动的加载这个接口的实现类从而进行配置spring mvc 的自动化配置是通过 WebApplicationInitializer 接口实现的 但是系统提供了更加高级的接口 AbstractAnnotationConfigDispatcherServletInitializer 如果使用基于xml 的 spring配置还可以，使用这个方法进行相关的调用这个类是AbstractAnnotationConfigDispatcherServletInitializer 如果要添加filter等配置，可以使用WebApplicationInitializer实现了的相关方法比如，AbstractDispatcherServletInitializer类的如下的方法 这个方法将会为每一个filter 添加一个默认的过滤器,并且自动的添加到对应的display中 引申：AbstractDispatcherServletInitializer这个类还有一个isisAsyncSupported()方法，默认情况下返回true 表示spring mvc 框架中的将会进行异步的处理请求 spring mvc 非xml配置的实现原理在servlet3.0 的标准中，提供了一中使用spi技术实现配置的方法，通过这种方法可以实现零xml配置 引申： tomcat 此处实现spi技术的解析 spring的web包的META-INF的文件夹中有一个名为，javax.servlet.ServletContainerInitializer的文件，其中的内容org.springframework.web.SpringServletContainerInitializer，表示定义的ServletContainerInitializer和spring的实现接口SpringServletContainerInitializer，其中@HandlesTypes注解表示ServletContainerInitializerde 可以处理的类，可以将感兴趣的一些类注入到ServletContainerInitializerde的onStartup方法作为参数传入。,在onStartup 方法中，可以通过Set<Class<?>> c 获取得到。 见下面代码： spring mvc最核心类 DispatcherServlet如果要说这个类就需要看一下springmvc的流程图 ; ps：暂时不知到这个地方有什么用 在这里之前 DispathcerServlet 将会webapplicationcontext的字符串引用放入java中 解释一下其中的各种组件 HandlerMapping 这个类解决了 url地址映射到对应的处理类中，主要有两个实现RequestMappingHandlerMapping-为@RequestMapping 注解提供支持 ，SimpleUrlHandlerMapping，实现简单的url地址映射 通俗点说这个方法在设计的时候相当于一个路由，通过前端获取的url地址等到对应的handlerMapping，之后再根据这个handlermapping获取handleradapter HandlerExceptionResolver 这个是试图返回的异常处理包括相关的错误处理方法 – 通过这种方法可以实现全局的异常捕获 HandlerIntercepter Print(a …interface{}) (nPrint(a …interface{}) (n 处理相关的接口进行拦截 HandlerAdapter 使用适配器模式，将试图的映射由指定的接口处理 各种resolver 提供视图解析展示的功能 LocaleResolver, LocaleContextResolver，ThemeResolver，MultipartResolver FlashMapManager 处理flash的时候使用的 估计用不到了 spring mvc 的 Interception 拦截器在spring MVC 中声明springmvc 的方法有如下几种： 实现HandlerInterceptor接口或者实现HandlerInterceptorAdapter 抽象类 实现WebRequestInterceptor接口，或者实现了WebRequestInterceptor的类 HandlerInterceptor 接口方法(1)boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handle)方法，这个将会在请求传入之前进行相关的处理，如果返回值是true，将会自动的进行链式调用其他按照顺序执行的定时器，否则将会终止调用controll和其他拦截器。 (2) postHandle (HttpServletRequest request, HttpServletResponse response, Object handle, ModelAndView modelAndView) 方法,这个方法将会在处理器执行完后进行处理，和preHandle的执行方法，注意这个方法将不会自动拦截@requsetBody注解ResponseEntity 注意这里：（1）和（2）的第三个参数 handle 官方的解释是@controller标记的本身或者使用HandleMethod这个类，其实这里是springmvc 自己增强的方法， 引申:ResponseEntity 这个是spring中对相关htttp请求的封装,提供了封装包头,封装http协议body字段中的内容的功能 ResponseEntity 详解： 这个类其实是对 http请求的一个封装，封装了http的报头，状态码，http code 等数据，本质上是一种通信协议 如果使用的不是string类型，那么将会是一种类似rpc协议 当使用string类型的时候就和@ResponceBody+@ResponseStatus(手动设置httpcode) 相同 (3)afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handle, Exception ex) 方法，该方法也是需要当前对应的Interceptor 的preHandle 方法的返回值为true 时才会执行。该方法将在整个请求结束之后，也就是在DispatcherServlet 渲染了对应的视图之后执行。这个方法的主要作用是用于进行资源清理工作的。 除了实现HandlerInterceptor可以实现拦截器之外，使用WebRequestInterceptor 同样可以实现拦截器，这个方法封装了作用域处理 (1)preHandle(WebRequest request) 方法。注意这个方法没有返回直，一般主要用它来进行资源的准备工作，其中的webrequest参数是HttpServletRequest的加强版，可以使用setAttribute(name, value, scope)添加参数到指定的作用域中，scope参数有如下的几个值： 1. SCOPE_REQUEST ：它的值是0 ，代表只有在request 中可以访问。 2. SCOPE_SESSION ：它的值是1 ，如果环境允许的话它代表的是一个局部的隔离的session，否则就代表普通的session，并且在该session范围内可以访问。 3. SCOPE_GLOBAL_SESSION ：它的值是2 ，如果环境允许的话，它代表的是一个全局共享的session，否则就代表普通的session，并且在该session 范围内可以访问。(2)postHandle(WebRequest request, ModelMap model) 方法。该方法将在请求处理之后，也就是在Controller 方法调用之后被调用，但是会在视图返回被渲染之前被调用，request 就是传递的请求参数，model就是返回的视图(3)afterCompletion(WebRequest request, Exception ex) 方法。该方法会在整个请求处理完成，也就是在视图返回并被渲染之后执行。所以在该方法中可以进行资源的释放操作。而WebRequest 参数就可以把我们在preHandle 中准备的资源传递到这里进行释放。Exception 参数表示的是当前请求的异常对象，如果在Controller 中抛出的异常已经被Spring 的异常处理器给处理了的话，那么这个异常对象就是是null 。 springmvc 异常处理 HandlerExceptionResolver异常处理用于处理@controll 这种接口抛出的各种异常，主要有如下的几种 SimpleMappingExceptionResolver 异常类名称和错误视图名称之间的映射。用于在浏览器应用程序中呈现错误页面。 DefaultHandlerExceptionResolver 解决Spring MVC引发的异常并将它们映射到HTTP状态代码。另请参阅备用ResponseEntityExceptionHandler和REST API例外。 ResponseStatusExceptionResolver 根据@ResponseStatus注释中的值解决注释中的异常并将其映射到HTTP状态代码。 ExceptionHandlerExceptionResolver 通过调用@ExceptionHandler一个@Controller或一个 @ControllerAdvice类中的方法来解决异常。请参阅@ExceptionHandler方法。 对于异常处理来说通常的解决结果有如下的几种方法 使用ModelAndView 指向错误视图。 返回空的modelandview 如果异常被处理 如果异常没有被解决，将会使用异常调用连进行处理，如果调用到最后将会抛出到servlet中 在spring mvc 中集中进行异常处理的有三种方法 使用@ResponseStatus 注释一个异常类，当spring中抛出这个异常的时候将会自动的交由这个类处理,并且可以制定http code值，比如下面的方法将会跑出403错误 注意如果这个注解使用在 一个方法上的时候 ， 不论结果如何都将会放回制定的httpcode 异常 2.@ControllerAdvice 和 @ExceptionHandler 这两个注解同样用于异常处理 （1）@ExceptionHandler 当一个Controller中有方法加了@ExceptionHandler之后，这个Controller其他方法中没有捕获的异常就会以参数的形式传入加了@ExceptionHandler注解的那个方法中。注意这个方法要有一个参数，这个参数就是指定要处理的异常 （2）@ControllerAdvice 实现这个注解的类可以让这个类中 @ExceptionHandler标记的方法实现全觉异常监听 最后对于springmvc 如果异常没有被处理，spring提供了默认的页面展示异常，使用如下方法配置xml ： 制定默认错误页面 java ： 处理url ViewResolver 提供了view 名称到view实例之间的绑定，并且完成真正视图展示之前将相关数据进行整理的功能spring mvc 中将相关的视图变成一个网页经历的过程 将SpringMVC控制器中的返回结果封装成一个ModelAndView对象。 通过SpringMVC中的视图解析器，使用ViewResolver对控制器返回的ModelAndView对象进行解析，将逻辑视图转换成物理视图。 调用View中的render()方法对物理视图进行渲染。 几个主要的视图的介绍 AbstractCachingViewResolver: 最抽象的类提供了视图缓存的功能 UrlBasedViewResolver: 提供了更加细粒度的url控制，支持前缀后缀等功能，但是使用这个方法必须制定相关的view解析工具，默认使用的是jsp解析工具InternalResourceView（和可以使用其他的是解析工具比如FreeMarkerView等）， 并且提供了重定向和转发的功能 InternalResourceViewResolver：这个方法是 UrlBasedViewResolver 的子类，支持父类的所有功能，InternalResourceViewResolver会把返回的视图名称都解析为InternalResourceView对象，内部使用重定向的方法，将controller返回的view 包装成InternalResourceView， 并且鞋带上前缀和后缀，同时再转发出去 视图解析链：在SpringMVC中可以同时定义多个ViewResolver视图解析器，然后它们会组成一个ViewResolver链。当Controller处理器方法返回一个逻辑视图名称后，ViewResolver链将根据其中ViewResolver的优先级来进行处理。所有的ViewResolver都实现了Ordered接口，在Spring中实现了这个接口的类都是可以排序的。在ViewResolver中是通过order属性来指定顺序的，默认都是最大值。所以我们可以通过指定ViewResolver的order属性来实现ViewResolver的优先级，order属性是Integer类型，order越小，对应的ViewResolver将有越高的解析视图的权利，所以第一个进行解析的将是ViewResolver链中order值最小的那个。当一个ViewResolver在进行视图解析后返回的View对象是null的话就表示该ViewResolver不能解析该视图，这个时候如果还存在其他order值比它大的ViewResolver就会调用剩余的ViewResolver中的order值最小的那个来解析该视图，依此类推。当ViewResolver在进行视图解析后返回的是一个非空的View对象的时候，就表示该ViewResolver能够解析该视图，那么视图解析这一步就完成了，后续的ViewResolver将不会再用来解析该视图。当定义的所有ViewResolver都不能解析该视图的时候，Spring就会抛出一个异常。 spring 重定向写法 spring mvc 常用注解spring 提供了一整套注解来简化spring相关的配置 @Controll 和 @RestControllerSpring MVC提供了一种基于注释的编程模型，其中@Controller和@RestController组件使用注释来表示请求映射，请求输入，异常处理等。 其中的@RestController 是@ResponseBody和@Controller注解的一种集合。 注意：如果使用aop方法对controller 进行增强的话请使用class-based proxying代理，但是如果使用了非spring context回调接口的方法的时候，需要明确的制定相关的配置信息：tx:annotation-driven/, 改变为 <tx:annotation-driven proxy-target-class&#x3D;”true”&#x2F;>. @RequestMapping requestMapping:有简化版的各种注解@GetMapping，@PostMapping，@PutMapping，@DeleteMapping，@PatchMapping，指定了相关的method方法对应的各种请求 request 请求可以接受的请求可以通过通配符或者glob参数的方法进行匹配 spring mvc 的地址匹配方法满足一定的相关原则，可以使用通配符进行匹配 Wildcard Description ？ 匹配任何单字符 * 匹配0或者任意数量的字符 ** 匹配0或者更多的目录 注意：spring将会按照匹配的字符最长的那个进行匹配，比如&#x2F;**&#x2F;.jsp 和&#x2F;app&#x2F;dir&#x2F;.jsp，在这个过程中，将会匹配后者。 注意：spring mvc 的匹配原则是按照后缀匹配的原则，比如一个url地址 &#x2F;name这个地址，表示的就是&#x2F;name.*，一定程度上实现了文件扩展名引用 如果使用了文件扩展名称这种东西如果想要配置请查看如下两个接口： 在request映射的方法中可以接收的数据接受的函数体中可以使用的注解和参数 WebRequest, NativeWebRequest WebRequest是Spring Web MVC提供的统一请求访问接口，不仅仅可以访问请求相关数据（如参数区数据、请求头数据，但访问不到Cookie区数据），还可以访问会话和上下文中的数据；NativeWebRequest继承了WebRequest，并提供访问本地Servlet API的方法。 javax.servlet.ServletRequest, javax.servlet.ServletResponse，javax.servlet.http.HttpSession javax 提供的具体接口注意其中的httpsession ，会话访问不是线程安全的。如果允许多个请求同时访问会话，请考虑将RequestMappingHandlerAdapter的“synchronizeOnSession”标志设置为“true”。 HttpMethod 这个值中有传入的方式，比如get还是post java.io.InputStream, java.io.Reader，java.io.OutputStream, java.io.Writer 请求的原始请求数据流，和原始返回数据 @PathVariable和@MatrixVariable 使用{}表示的url请求对应的相关参数, @PathVariable 没什么好说的关键是@MatrixVariable ， 这个注解将会自动的匹配url 地址中 ;uuu&#x3D;123;iii&#x3D;333 这种参数，并且一定的程度下并不需要{jj}中指定的名称来匹配，当发生参数冲突的时候可以使用pathVar参数指定名称，目前有bug不记录了 @PathVariable,@MatrixVariable,@RequestParam，@RequestBody，@RequestHeader，@CookieValue，@RequestPart,@ModelAttribute,@SessionAttribute,@RequestAttribute @PathVariable ,spring mvc提供了一套支持reastfulapi的方法，这套方法可以使用{}+通配符的方式，将url中的数据传递到method 对应的参数中 注意：{}里面的值值也可以使用正则表达式进行相关的配置，格式：{varName:regex} @MatrixVariable，这个注解将会自动的匹配url地址中;uuu&#x3D;123;iii&#x3D;333这种参数，并且一定的程度下并不需要{jj}中指定的名称来匹配，当发生参数冲突的时候可以使用pathVar参数指定名称，目前有bug不记录了 @RequestParam接口将会自动的将传入到指定的地址中 如果使用这个注解标识Map<String, String> or MultiValueMap<String, String>，将会将所有的属性注入进来A） 常用来处理简单类型的绑定，通过Request.getParameter() 获取的String可直接转换为简单类型的情况（ String–> 简单类型的转换操作由ConversionService配置的转换器来完成）；因为使用request.getParameter()方式获取参数，所以可以处理get 方式中queryString的值，也可以处理post方式中 body data的值；B）用来处理Content-Type: 为 application&#x2F;x-www-form-urlencoded编码的内容，提交方式GET、POST；C) 该注解有两个属性： value、required； value用来指定要传入值的id名称，required用来指示参数是否必须绑定； @RequestBody 该注解常用来处理Content-Type: 不是application&#x2F;x-www-form-urlencoded编码的内容，例如application&#x2F;json, application&#x2F;xml等；它是通过使用HandlerAdapter 配置的HttpMessageConverters来解析post data body，然后绑定到相应的bean上的。 使用：HttpMessageConverter接口，需要开启<mvc:annotation-driven &#x2F;>。 AnnotationMethodHandlerAdapter将会初始化7个转换器，可以通过调用AnnotationMethodHandlerAdapter的getMessageConverts()方法来获取转换器的一个集合 List，这7个转化器如下： PS:Spring默认的json协议解析由Jackson完成。 这种方式的时候默认使用的就是json解析。这个JackSon需要进行maven配置 问题 如何扩展 ps：如果使用xml 方式，需要使用注解 注意 这里如果要使用注意spring mvc 环境的配置 使用xml 进行配置 使用java 进行配置 @RequestPart 和文件上传相关，难度有点大，和http协议相关的暂时不考虑 @RequestHeader 将http协议中相关的头注入到指定的数据中 @ModelAttribute @sessionAttribute @RequestAttribute（处理的是表单数据类型） 在使用model view 场景下，有如下的一应用 全局model配置，在获得请求&#x2F;helloWorld 后，populateModel方法在helloWorld方法之前先被调用，它把请求参数（&#x2F;helloWorld?abc&#x3D;text）加入到一个名为attributeName的model属性中，在它执行后 helloWorld被调用，返回视图名helloWorld和model已由@ModelAttribute方法生产好了。 指派model配置 返回 helloworld.do视图，有一个model 参数是attributeName和值 hi 绑定application&#x2F;x-www-form-urlencoded 提交的请求中 的值到对象中 支持user.xxx,user2.ddd 嵌套对应法 这里注意一个比较特殊的就是@RequestBody这个绑定的是json对象 @SessionAttribute 和@SessionAttributes 这两个对对象是配合ModeAttribute使用的 齐总SessionAttribute只有从Session作用域中取数据的作用而@SessionAttributes 就是将ModeAttribute中制定key的数据设置到Session作用域中来 @RequestAttribute 这个方法只能取出 request作用域中的数据 @CookieValue 和之前的相同，就是将cookie中相关的数据拿出来 HttpEntity HttpEntity或多或少与使用@RequestBody相同，但基于公开请求标头和主体的容器对象。 注意这个方法多是用于post请求用来针对ajax序列化的json对象解析，其中有一个getbody方法 @InitBinder spring自带的数据处理模块(所有的表单处理，相反的参数有Model) 由@InitBinder表示的方法，可以对WebDataBinder对象进行初始化。WebDataBinder是DataBinder的子类，用于完成由表单到JavaBean属性的绑定。@InitBinder方法不能有返回值，它必须盛名为void。@InitBinder方法的参数通常是WebDataBinder，@InitBinder可以对WebDataBinder进行初始化。 注意这个注解只是针对这个controller中的方法起作用，无法针对所有的controller 注意WebDataBinder这个对象，这个对象拥有一个方法registerCustomEditor,这个方法将会自动的配置属性映射器，将相关的属性映射到指定的位置，属性映射器可以使用如下的方法进行自定义实现PropertyEditor或者重写PropertyEditorSupport对象中的方法，注意这种方法只能实现string到对象的转换 setValue中就是转化后的对象，setAsText传入的就是传入的url字符串 同时WebDataBinder这个对象，这个对象拥有一个addCustomFormatter 可以直接使用formatter进行参数转化本质上是相同的 addCustomFormatter本质上还是和registerCustomEditor是一样的见源代码 @RequestMapping 方法返回值中的参数 @ResponseBody HttpEntity, ResponseEntity HttpEntity HttpEntity或多或少与使用@RequestBody相同，但基于公开请求标头和主体的容器对象。 ResponseEntity 这个类其实是对 http请求的一个封装，封装了http的报头，状态码，http code 等数据，本质上是一种通信协议 如果使用的不是string类型，那么将会是一种类似rpc协议 当使用string类型的时候就和@ResponceBody+@ResponseStatus 相同 HttpHeaders 返回一个封装的httpheaders的头，这个类有一个set方法，制定方法的头和内容，如果想深入的使用，需要精通http协议 string 最简单的一个方法，spring将会使用这个字符串找到对应的view java.util.Map, org.springframework.ui.Model，@ModelAttribute spring mvc modelandview体系的东西 ResponseBodyEmitter, SseEmitter， StreamingResponseBody Reactive types — Reactor, RxJava, or others via ReactiveAdapterRegistry @ResponseStatus(HttpStatus.CREATED) 制定返回值的头部信息, 比如制定401 402 这种http code","date":"2024-12-31","categories":["java"],"tags":["spring"]},{"title":"spring5-springMVC返回值和请求值预先处理","url":"/2024/12/31/spring5-springMVC返回值和请求值预先处理/","content":"一般情况下，我们有的时候可能需要对web框架的返回值进行一定的处理，但是这种情况下有一定的局限性比如下面这种使用拦截器的情况 我们继承了HandlerUbterceotor 接口并且实现了postHandle 方法，这个方法将会在controller处理完之后将返回值放入handler中，通过这种方法我们可以对handler中的属性进行修改。 但是这样是有局限性的，首先我们无法封装handler 比如将返回的值换成另一个类，其次这个对象是object 我们需要强制转化才能使用，这样就有了一定的局限性。 正因为这样在spring4 添加了两个新的接口 RequestBodyAdvice和ResponseBodyAdvice 其中supports方法是为了判断是否可以执行处理逻辑，剩下的方法就封装恶劣在参数生成之前之后等操作步骤和拦截器相同，就不在多说了","date":"2024-12-31","categories":["java"],"tags":["spring"]},{"title":"spring5-spring事务操作支持","url":"/2024/12/31/spring5-spring事务操作支持/","content":"spring事务概述 事物：数据库事务(Database Transaction) ，是指作为单个逻辑工作单元执行的一系列操作，要么完全地执行，要么完全地不执行。一个逻辑工作单元要成为事务，必须满足所谓的ACID（原子性、一致性、隔离性和持久性）属性 原子性（Atomic）（Atomicity)：事务必须是原子工作单元；对于其数据修改，要么全都执行，要么全都不执行。 隔离性（Insulation）(Isolation)：由并发事务所作的修改必须与任何其它并发事务所作的修改隔离。事务查看数据时数据所处的状态，要么是另一并发事务修改它之前的状态，要么是另一事务修改它之后的状态，事务不会查看中间状态的数据。 一致性（Consistent）(Consistency)：事务在完成时，必须使所有的数据都保持一致状态。在相关数据库中，所有规则都必须应用于事务的修改，以保持所有数据的完整性。不能只修改了一半 持久性（Duration）(Durability）：事务完成之后，它对于系统的影响是永久性的。该修改即使出现致命的系统故障也将一直保持。 spring中事务的传播行为 PROPAGATION_REQUIRED–支持当前事务，如果当前没有事务，就新建一个事务。这是最常见的选择。 PROPAGATION_SUPPORTS–支持当前事务，如果当前没有事务，就以非事务方式执行。 PROPAGATION_MANDATORY–支持当前事务，如果当前没有事务，就抛出异常。 PROPAGATION_REQUIRES_NEW–新建事务，如果当前存在事务，把当前事务挂起。 PROPAGATION_NOT_SUPPORTED–以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。 PROPAGATION_NEVER–以非事务方式执行，如果当前存在事务，则抛出异常。 spring中事物的隔离级别 Serializable：最严格的级别，事务串行执行，资源消耗最大； repeatable READ：保证了一个事务不会修改已经由另一个事务读取但未提交（回滚）的数据。避免了“脏读取”和“不可重复读取”的情况，但是带来了更多的性能损失。 READ COMMITTED:大多数主流数据库的默认事务等级，保证了一个事务不会读到另一个并行事务已修改但未提交的数据，避免了“脏读取”。该级别适用于大多数系统。 Read Uncommitted：保证了读取过程中不会读取到非法数据。 并发中可能发生的3中不讨人喜欢的事情 Dirty reads–读脏数据。也就是说，比如事务A的未提交（还依然缓存）的数据被事务B读走，如果事务A失败回滚，会导致事务B所读取的的数据是错误的。 non-repeatable reads–数据不可重复读。比如事务A中两处读取数据-total-的值。在第一读的时候，total是100，然后事务B就把total的数据改成200，事务A再读一次，结果就发现，total竟然就变成200了，造成事务A数据混乱。 phantom reads–幻象读数据，这个和non-repeatable reads相似，也是同一个事务中多次读不一致的问题。但是non-repeatable reads的不一致是因为他所要取的数据集被改变了（比如total的数据），但是phantom reads所要读的数据的不一致却不是他所要读的数据集改变，而是他的条件数据集改变。比如Select account.id where account.name&#x3D;”ppgogo*”,第一次读去了6个符合条件的id，第二次读取的时候，由于事务b把一个帐号的名字由”dd”改成”ppgogo”，结果取出来了7个数据。 spring中其中事务的传播行为-在什么时候添加事务 spring中事务的使用方法使用阿里数据库链接池进行配置，这是下面样例使用的配置文件 在spring框架中，事务的实现原理：首先实现一个事务的管理类 DataSourceTransactionManager 然后使用spring内置的数据库封装类进行相关的操作：JdbcTemplate，之后使用aop将事务管理直接注入到需要实现事务的操作类中，使用切面增强，当发生异常的时候，spring将会自动的将之前封装的JDBCTempate进行回滚操作 使用基于配置文件的配置方法 xml配置文件 java的数据库操作类 java 测试方法 使用spring注解的方法进行配置 xml配置文件 java数据库操作类 测试类","date":"2024-12-31","categories":["java"],"tags":["spring"]},{"title":"spring5-webflux(1)简介","url":"/2024/12/31/spring5-webflux-1-简介/","content":"","date":"2024-12-31","categories":["java"],"tags":["spring"]},{"title":"spring5-导入资源文件","url":"/2024/12/31/spring5-导入资源文件/","content":"spring5-导入资源文件一般情况下一些配置属性都是使用配置文件进行导入的，在一定的程度上进行了解藕，这里整理一下java 和spring框架在获取资源上的一些配配置 java进行资源的配置 坑点：Thread.currentThread().getContextClassLoader() 可以获得项目编译后的执行根目录相当于 maven项目 的resource， 这一点非常重要 使用spring的bean.xml配置文件导入相关的配置 spring 增强配置文件处理使用注解进行相关的各种配置 使用 java 的@value 和 PropertySource 注解结合使用 springboot-导入资源文件增强spring boot 通过注解的方法将各种属性通过自动化的形式加载进javabean中，简化操作。 配置文件 @ConfigurationProperties注解将会对 自动注入相关配置文件中的属性省去 前缀自动的注入到相关的配置文件中 升级版注解 @EnableConfigurationProperties(MyconfigProperties.class) 一般结合 @configuration注解使用 —-将指定的类似上面javabean的bean 自动完成属性注入功能","date":"2024-12-31","categories":["java"],"tags":["spring"]},{"title":"spring5-异步调用和定时器","url":"/2024/12/31/spring5-异步调用和定时器/","content":"spring5-异步调用和定时器 Eable*类型的注解是和spring configurtion注解搭配使用的目的是减少配置和代码数量 @EnableAsync 支持异步操作 配置类 @EnableAsync javabean 类 @Async 执行主函数 结果： 结论：函数变成一部执行非阻塞型 @EnableScheduling 允许进行定时操作 配置类 javabean类 注意：除了cron标记的其他元素如果没有使用iniiaDelay参数将会执行两边（调用一边，没有进行延迟一遍） ，cron表达式只支持六个参数 cron 表达式详解 符号 Seconds Minutes Hours DayofMonth Month DayofWeek Year ， Y Y Y Y Y Y Y – Y Y Y Y Y Y Y * Y Y Y Y Y Y Y &#x2F; Y Y Y Y Y Y Y ? N N N Y N Y N L N N N Y N Y N W N N N Y N N N C N N N Y N Y N # N N N N N Y N ：表示匹配该域的任意值，假如在Minutes域使用*, 即表示每分钟都会触发事件。 ? ：只能用在DayofMonth和DayofWeek两个域。它也匹配域的任意值，但实际不会。因为3. DayofMonth和DayofWeek会相互影响。例如想在每月的20日触发调度，不管20日到底是星期几，则只能使用如下写法： 13 13 15 20 * ?, 其中最后一位只能用？，而不能使用，如果使用表示不管星期几都会触发，实际上并不是这样。 – :表示范围，例如在Minutes域使用5-20，表示从5分到20分钟每分钟触发一次 &#x2F; ：表示起始时间开始触发，然后每隔固定时间触发一次，例如在Minutes域使用5&#x2F;20,则意味着5分钟触发一次，而25，45等分别触发一次. , ：表示列出枚举值值。例如：在Minutes域使用5,20，则意味着在5和20分每分钟触发一次。 L ：表示最后，只能出现在DayofWeek和DayofMonth域，如果在DayofWeek域使用5L,意味着在最后的一个星期四触发。 W ：表示有效工作日(周一到周五),只能出现在DayofMonth域，系统将在离指定日期的最近的有效工作日触发事件。例如：在 DayofMonth使用5W，如果5日是星期六，则将在最近的工作日：星期五，即4日触发。如果5日是星期天，则在6日(周一)触发；如果5日在星期一到星期五中的一天，则就在5日触发。另外一点，W的最近寻找不会跨过月份 LW ：这两个字符可以连用，表示在某个月最后一个工作日，即最后一个星期五。 # ：用于确定每个月第几个星期几，只能出现在DayofMonth域。例如在4#2，表示某月的第二个星期三。","date":"2024-12-31","categories":["java"],"tags":["spring"]},{"title":"spring5-扩展(一)sope(生命周期)扩展","url":"/2024/12/31/spring5-扩展-一-sope-生命周期-扩展/","content":"","date":"2024-12-31","categories":["java"],"tags":["spring"]},{"title":"spring5-扩展(二)application(集装箱扩展)","url":"/2024/12/31/spring5-扩展-二-application-集装箱扩展-/","content":"","date":"2024-12-31","categories":["java"],"tags":["spring"]},{"title":"spring5-扩展深入分析spring的装箱扩展","url":"/2024/12/31/spring5-扩展深入分析spring的装箱扩展/","content":"BeanPostProcessor","date":"2024-12-31","categories":["java"],"tags":["spring"]},{"title":"spring5-数据库支持","url":"/2024/12/31/spring5-数据库支持/","content":"JdbcTemplate spring 数据库操作基础类这个类是spring进行jdbc操作的核心，通过这个方法将节省数据库操作，和避免一些低级错误，比如资源没哟释放等等 select 操作 update delete insert操作 使用数据操作语句进行操作的通用方法这个方法可以执行任何语句 jdbcTemple 初始化传入一个数据库链接的相关信息和是否使用延迟加载的方法 NamedParameterJdbcTemplate这个接口是一个特殊的存在，底层使用JDBCTemplate同时将之前使用适配符的？进行了替换 这个接口的一个高级应用就是使用 SqlParameterSource 进行分装 ， spring 官方文档中主要提供了如下的三种方法进行相关的操作 DriverManagerDataSource spring 内置数据库的实现 spring 进行批处理使用JDBCTemplete进行批处理 使用namedParameterJdbcTemplate 进行批处理 结语： 这里的这些特性其实就是在使用相关的技术的时候感兴趣才看的，真正在生产环境下使用 mybatis这种持久层框架比较多","date":"2024-12-31","categories":["java"],"tags":["spring"]},{"title":"spring5-条件注解@Conditional","url":"/2024/12/31/spring5-条件注解-Conditional/","content":"spring5-条件注解@Conditional@Conditional将会按照条件自动的创建相关的javabean类型 继承Condition接口创建条件判断类 一个bean类型 配置类 配置类将会通过Condition返回是否是true进行相关判断如果返回的结果是true将会自动的载入相关的配置","date":"2024-12-31","categories":["java"],"tags":["spring"]},{"title":"spring5-资源使用","url":"/2024/12/31/spring5-资源使用/","content":"Bean操作和BeanWrapperBeanWrapper 实现了对bean更加细致的操作这种方法将会让bean的各种操作更加容易和方便,并且提供了高级使用方法 设置和获取基本和嵌套的属性设置和获取属性是通过使用setPropertyValue(s)和 getPropertyValue(s)两个重载变量都有的方法完成的。 s字符串支持的属性见下 表达 说明 name 指示与name方法getName()或isName() 和相对应的属性setName(..) account.name 指示对应于例如方法或属性name的属性的嵌套属性accountgetAccount().setName()getAccount().getName() account[2] 指示索引属性的第三个元素account。索引属性可以是类型的array，也可以是list其他自然顺序的集合 account[COMPANYNAME] 指示由Map属性的键COMPANYNAME索引的地图条目的值account 下面的代码片断展示了如何检索和操作的一些实例化属性的一些例子Companies和Employees：","date":"2024-12-31","categories":["java"],"tags":["spring"]},{"title":"springboot2.0-深入springBoot-2.0知识点整理","url":"/2024/12/31/springboot2-0-深入springBoot-2-0知识点整理/","content":"spring 个性化配置自定义横幅比如在classpath中写入一个 banner.txt文件springboot将自动的将这个文件打印到开始运行的地方 banner.txt 还可以针对java的MANIFEST.MF文件进行个性化配置比如： ${application.formatted-version}，${application.version} 打印在MANIFEST.MF文件中Implementation-Version: 1.0字段的1.0版本号，如果是前者将会带上v前缀 v1.0 ${spring-boot.formatted-version}，${spring-boot.version} 打印spring的版本号，同样前者有前缀 ${application.title} 打印在MANIFEST.MF文件中Implementation-Title: MyApp字段中的MyApp sp：如果要以编程方式生成横幅，则可以使用SpringApplication.setBanner（…）方法。使用org.springframework.boot.Banner接口并实现您自己的printBanner（）方法。还可以使用spring.main.banner-mode属性来确定横幅是否必须在System.out（控制台）上打印，发送到配置的记录器（日志），还是根本不生成（关闭） yaml 文件中使用的是如下的配置 spring boot 上层异常处理模块FailureAnalyzers这个模块用来封装一些spring 内置的异常问题，用来显示解决方案，暂时不做深入的研究 ###spring boot 核心注解 spring boot 是spring 零配置的升级，通过各种注解增强了spring容器 @SpringBootApplication这个其实是一个聚合接口，这个接口聚合了@SpringBootConfiguration（和Configuration注解相同），@EnableAutoConfiguration，@ComponentScan注解，实现了自动化配置等功能，同样可以实现自己的方法来实现自动化配置 @Configurationspring boot 鼓励使用java配置类的配置方法，如果一定要使用xml的配置，也同样建议使用@Configuration注解并加上下面的注解 @ImportResource 这个注解注解在java类上面，通过importResource的value或者locations指定，文件的路径，reader是一个解析器，默认情况下，阅读器将适应指定的资源路径：“.groovy”文件将使用GroovyBeanDefinitionReader进行处理; 而所有其他资源将使用XmlBeanDefinitionReader进行处理。 @Enable* 系列注解@Enable* 系列注解是springboot实现自动化配置的核心方法，这种方法将自动的将相关的需要配置的方法进行配置 官方文档中介绍的，springboot 的自动化配置是非侵入的方式，自己实现的方法将会覆盖系统实现的方法。如果想要知道springboot应用了什么自动化配置，请使用debug模式运行springboot，并且打印出springboot的配置调用日志 @EnableAutoConfiguration 2. @ComponentScan并不是springboot特有的接口，应为springboot 需要spring零配置相关的功能，所以记录在这里，如果没有参数将会通过这个方法作为跟路径向下搜索自动化配置的相关信息 @Import({ MyConfig.class, MyAnotherConfig.class })这个注解可以自定义配置需要的配置类，从而实现自定义类似@springbootapplication注解","date":"2024-12-31","categories":["java"],"tags":["spring"]},{"title":"springboot2.0-深入springBoot-@Enablexxx注解和@import注解解惑","url":"/2024/12/31/springboot2-0-深入springBoot--Enablexxx注解和-import注解解惑/","content":"深入springBoot-@Enable***注解和@import注解解惑spring boot的自动配置功能相当强悍，其强悍的更本在于提供了Eable*** 这种自动化配置，之前的博客，简要分析如何自定义starter，这里对他进行一下补充 首先我们观察@SpringBoot注解这个注解是一个组合注解，她提供自动化配置的注解是@EableAutoConfiguration注解 类似EableAutoConfiguration这样的@Eable***注解，都会使用@Import注解导入一写自动化配置bean，讲一些自动化配置类进行自动导入 @Import注解导入配置方式的三种类型 第一类：直接导入配置类 直接导入配置类SchedulingConfiguration，这个类注解了@Configuration，且注册了一个scheduledAnnotationProcessor的Bean 第二类：依据条件选择配置类 AsyncConfigurationSelector通过条件来选择需要导入的配置类， AsyncConfigurationSelector的根接口为ImportSelector，这个接口需要重写selectImports方法，在此方法内进行事先条件判断。 AdviceMode adviceMode 就是 调用注解中的AdivceMode 若adviceMode为PORXY，则返回ProxyAsyncConfiguration这个配置类。 若activeMode为ASPECTJ，则返回AspectJAsyncConfiguration配置类。 第三类：动态注册Bean AspectJAutoProxyRegistrar 实现了ImportBeanDefinitionRegistrar接口，ImportBeanDefinitionRegistrar的作用是在运行时自动添加Bean到已有的配置类，通过重写方法： 其中，AnnotationMetadata参数用来获得当前配置类上的注解；BeanDefinittionRegistry参数用来注册Bean。","date":"2024-12-31","categories":["java"],"tags":["spring"]},{"title":"springboot2.0-深入springBoot-Mybatis整合和分析","url":"/2024/12/31/springboot2-0-深入springBoot-Mybatis整合和分析/","content":"深入springBoot-Mybatis整合和分析之前做过springBoot框架的自定义starter配置，这次通过配置springboot和Mybatis，深入源码在解析一下 spring和mybatis 整合所需要的相关的各种类 SqlSessionFactoryBean SqlSessionFactory sqlSessionTemplate MapperFactoryBean MapperScannerConfigurer 现在查看一些spring-mybatis的starer相关的config 总结：发现相关的关键类都被自动加载了 查看mybatis Properties 参数配置类 总结：传入的参数就使用xml文件需要传入的参数，只不过在使用sporingboot的时候使用soring.yml配置文件的明明方式发生改变 整合实例 运行类 service dao 和 bean 类 pom.xml mybatis config和mappper yaml文件 引申，也可以使用spring的原生bean进行相关的配置 只需要在注解上加上@ImportResource ，并在其中引入自己需要的xml文件就行 bean.xml 数据库配置文件","date":"2024-12-31","categories":["java"],"tags":["spring"]},{"title":"springboot2.0-深入springBoot-入门","url":"/2024/12/31/springboot2-0-深入springBoot-入门/","content":"深入springBoot-入门引申：Spring Boot是由Pivotal团队提供的全新框架，其设计目的是用来简化新Spring应用的初始搭建以及开发过程。该框架使用了特定的方式来进行配置，从而使开发人员不再需要定义样板化的配置。用我的话来理解，就是spring boot其实不是什么新的框架，它默认配置了很多框架的使用方式，就像maven整合了所有的jar包，spring boot整合了所有的框架（不知道这样比喻是否合适）。 核心注解@SpringBootApplication @SpringBootApplication是一个组合注解，整合了@EnableAutoConfiguration，@ComponentScan，@Configuration，其中@EnableAutoConfiguration会扫描包进行相关的javabean的配置等 样例 springboot的配置文件（application.properties or applicatin.yaml）在springboot中可以指定 @PropertySource 注解进行手动导入spring boot配置的文件（配置文件地址数组） 此外@PropertySource注解还能和@Value 和 @ConfigureProperties 注解配合使用，为元素添加属性，，（springboot在启动的时候将会自动的添加application.properties和application.yaml配置文件） 注意：和@value连用，只用SPElL表达式进行属性的自动装载 注意：和@ConfigurationProperties连用 注意要提供get和set方法 注解作用是自动的导入 springboot的配置文件（spring.properties或者spring.yml）中的相关属性 spring boot 载入 bean.xml配置文件sring boot 源自于 spring 自然也能使用spring原生的相关配置文件 使用ImportResource（{“classpath:xxx.xml”}） 自动的导入相关的各种配置文安 spring boot 非web项目启动spring boot 启动方式是使用SpringApplication.run(SpringbootmybatisApplication.class, args); 进行启动 当使用自己的逻辑的时候，需要实现CommandLineRunner 接口并在run方法中进行相关逻辑的编写 spring boot 自动化配置标签spring boot 便捷卡快度的根本原因是因为spring boot 提供自动化的Eable标签， 和动态处理@configuration配置类配置的@condition相关注解 enable***类 @EnableConfigurationProperties：关联使用@ConfigurationProperties注解的类，然这个类可以进行初始化并且导入容器中，一般用在自定义starer @EnableAutoConfiguration：希望通过之前添加的jar依赖来实现 自动的配置你的Spring Application. 比如, 如果 HSQLDB 在你的 classpath中, 而且你没有配置任何数据库相关的bean,这时我们会自动配置一个内存数据库. 你只能加一个 @EnableAutoConfiguration 注解. 通常建议加在主 @Configuration 类上. 注意了查看源代码可以知道，这个注解组合类 @import注解，import一个EnableAutoConfigurationImportSelector.class类，这个类实现了import接口中的import selectImports 方法，这个方法可以返回一个 class.getClassName() 数组，用来将相关的bean导入进class中 @EnableAspectJAutoProxy： 启动spring自动aop 相当一xml文件中的aop:aspectj-autoproxy 让@Aspectj注解生效 @EnableScheduling和@EnableAsync 见博客spring5框架知识整理-多线程和定时器 注意：EnableAsync 注解的configrution类可以实现一个Asyncconfig接口并且实现 接口实现类，返回一个数据连接池Exeturor @EnableTransactionManagement开起自动化事务配置<tx:annotation-driven transaction-manager&#x3D;”myTransactionMamager”&#x2F;> @Transactional 标签提供支持 condition类 结合 configuuration注解尽心组合判断相关的配置是否有效 ConditionalOnBean 当存在指定的cbean的时候生效 ConditionalOnClass 当存在指定的class 文件是生效 ConditionalOnMissBean 当不存在指定的 bean的时候生效 ConditionalOnMissClass 当不存在指定的class 时生效 ConditionalOnExpression 当使用的SpEl表达是true的时候才能进行使用 ConditionalOnJava 指定java版本进行配置 ConditionalOnProperty 当springboot 存在指定的相关配置文件中的属性的时候进行相关的配置 ConditionalOnresource 当指定的资源存在的时候进行相关的配置 @ConditionalOnSingleCandidate 当前的类在容器中只用一个的时候执行， springboot的例子是加载事务控制器的时候进行相关的操作 condition例子和分析见深入springBoot-自动配置原理和手动实现","date":"2024-12-31","categories":["java"],"tags":["spring"]},{"title":"springboot2.0-深入springBoot-自动配置原理和手动实现","url":"/2024/12/31/springboot2-0-深入springBoot-自动配置原理和手动实现/","content":"深入springBoot-自动配置原理和手动实现这里深入的分析一下为什么spring boot 可以进行 简单化配置, 如何进行手动配置 spring如何进行相关的配置springBoot的自动配置一般配置的源码包是放置在springboot-Autoconfig.jar包下面的 在springboot进行运行的时候，@SpringBoot注解提供了自动导入配置的@EableAutoConfiguration 注解 查看一下这个注解的源代码 分析源代码我们可以知道，注解中使用了Import注解将EnableAutoConfigurationImportSelector.class这个类进行运行，这个类将会自动的检测META&#x2F;spring.factories文件，这个文件中声明了相关的各种配置 上面文件中的各配置类中都会发现在springboot中声明的各种配置注解 （在autoconfigure.config包下面）如@ConditionalOnProperty @ConditionalOnMissingBean 分析一下最后一个注解 发现使用OnWebApplicationCondition.class 这个类 ，找一下这个类 这个类就是实现了condition接口的根本类型 总结：如果要实现自定配置，并将这个配置好的类之前使用condition各种判断方法及你想嗯判断，然后放入spring.factories文件中就行（注意配置类就是一个使用@configuration注解进行标记配置类-也就是说，springBoot框架使用这个配置类进行自动花配置，并将配置类中的相关Bean文件提取出来放入公共命名空间） 手动实现一个starter实现一个starter关键的一部就是要实现config类 参数导入类：使用这个类就是为了在上面的生成bean的环节中动态的进行导入相关的参数 生成的javabean ， 在springboot中实现动态的身成相关的各种配置参数注入类型,注意这个使用了@ConfiguurationProperties 注解的这个类 ，这个类不单单提供了属性的自动注入功能还提供了，对外面暴露配置文件信息的功能 接下来就可以在使用maven打包成jar包，进行发布，注意坑点，必须打成jar包，其次要关闭spring boot的maven插件，因为这个插件会强制将 必有要存在main方法否则将会导致相关的mvn install失败 其他类尽心引用：通过上面的分析我们已经知道类，spring boot 进行相关的配置的时候其实是通过一个spring.factories 配置文件进行相关的配置的，所以，当我们进行自定一配置的时候需在resouce文件夹下新建METE-INF&#x2F;spring.factories 文件，导入自己尽心的相关配置 其他class类","date":"2024-12-31","categories":["java"],"tags":["spring"]},{"title":"springboot2.0-深入springboot-导入资源文件增强","url":"/2024/12/31/springboot2-0-深入springboot-导入资源文件增强/","content":"深入springboot-导入资源文件增强spring boot 通过注解的方法将各种属性通过自动化的形式加载进javabean中，简化操作。 配置文件 @ConfigurationProperties注解将会对 自动注入相关配置文件中的属性省去 前缀自动的注入到相关的配置文件中 升级版注解 @EnableConfigurationProperties(MyconfigProperties.class) 一般结合 @configuration注解使用 —-将指定的类似上面javabean的bean 自动完成属性注入功能","date":"2024-12-31","categories":["java"],"tags":["spring"]},{"title":"undertow-servlet支持","url":"/2024/12/31/undertow-servlet支持/","content":"undertow 是通过了servlet 4.0 认证的服务器在本质上是支持servlet标准的","date":"2024-12-31","categories":["java"],"tags":["undertow"]},{"title":"undertow-xnio","url":"/2024/12/31/undertow-xnio/","content":"underttow 是redhat 开发的高性能服务器，具有jetty的易扩展和netty 高性能，之前阅读过部分undertow的源代码，发现undertow地成的异步框架使用的是xnio(并不是netty) 这里记录一下undertow 最底层xnio的使用方法 XNIO有两个重要的概念： Channel，是传输管道的抽象概念，在NIO的Channel上进行的扩展加强，使用ChannelListener API进行事件通知。在创建Channel时，就赋予IO线程，用于执行所有的ChannelListener回调方法。 区分IO线程和工作线程，创建一个工作线程池可以用来执行阻塞任务。一般情况下，非阻塞的Handler由IO线程执行，而阻塞任务比如Servlet则被调度到工作线程池执行。这样就很好的区分了阻塞和非阻塞的两种情形。 (这一点和netty很类似) 引申：我们知道NIO的基本要求是不阻塞当前线程的执行，对于非阻塞请求的结果，可以用两种方式获得：一种是对于请求很快返回一个引用（如JDK中Future，XNIO中称为IoFuture，其中很多方法是类似的），过一段时间再查询结果；还有一种是当结果就绪时，调用事先注册的回调方法来通知（如NIO2的CompletionHandler，XNIO的ChannelListener） 重要概念： 在xnio中，有几个词出现频率很高，Source表示信息源头，Sink是信息目的地，Conduit是源头到目的地管道的抽象。 一个例子查看一下 2018: ps - xnioworker 方法中的 connectStream 和 createStreamServer 已经被标记为过时, 建议采用","date":"2024-12-31","categories":["java"],"tags":["undertow"]},{"title":"undertow-架构解析","url":"/2024/12/31/undertow-架构解析/","content":"undertow 是在国外评测网站techempower 中评测排行头10高性能框架,同时也是spring boot 可以使用的web容器之一. 这里记录一下undertow的学习和使用的记录 基础列子 简单服务器构建和手动服务起构建 简单服务器构建 其实通过源代码可知，简单服务器构建其实相当于使用了默认值的复杂服务器构建 手动组装服务器 这里其实是自己手动了设置了undertow(xnio)体系中各种组件, 具体的各个步骤的意义如下 创建一个XNIO工作者。此worker管理服务器的IO和Worker线程。 创建XNIO SSL实例（可选，仅在使用HTTPS时才需要） 创建相关Undertow侦听器类的实例 使用XNIO打开服务器套接字并设置其accept侦听器 undertow的架构设计首先说明一下undertwo的架构非常简单,大致拆分成如下的几个部分 xnio层 这个是undertow 底层使用的nio框架,比netty更加的轻量级 管理IO和工作线程 在undertow 体系统中可以手动设置ThreadPool 而不适用java自己线程池 channel API 和 Listeners undertow 是基于 java 的nio, channel api 本质上其实是 nio通道的一种封装.listeners在undertow中最为channel api的监听表示,channel api将listener感兴趣的请求派发到listener中进行处理 Undertow附带5个不同的监听器：HTTP &#x2F; 1.1,HTTPS,AJP,HTTP &#x2F; 2 处理程序 handle 这个方法也就是我们需要实现的方法, 表示接受到请求后进行处理时的处理方法 undertow 一些参数控制 xnio 主干参数 这个方法的参数主要是在 listener参数 这个参数是用来配置listener的各种各种配置信息的参数主要在 undertow 的生命周期的交换结束请求生命周期当客户端连接到服务器时，Undertow会创建一个io.undertow.server.HttpServerConnection。当客户端发送请求时，它由Undertow解析器解析，然后将结果io.undertow.server.HttpServerExchange传递给根处理程序。当根处理程序完成时，可能会发生以下四种情况之一： 交换可以完成 如果请求和响应通道都已完全读&#x2F;写，则认为交换完成。对于没有内容的请求（例如GET和HEAD），请求方自动被视为完全读取。当处理程序写出完整响应并关闭并完全刷新响应通道时，读取端被视为完成。如果交易已经完成，那么在交易完成后不会采取任何行动。 根处理程序正常返回而不完成交换 在这种情况下，交换将通过呼叫完成HttpServerExchange.endExchange()。语义endExchange() 将在后面讨论。 根处理程序返回一个Exception 在这种情况下，500将设置响应代码，并且将使用结束交换HttpServerExchange.endExchange()。 根处理程序可以在HttpServerExchange.dispatch()调用之后或启动异步IO之后返回 在这种情况下，已调度的任务将提交给调度执行程序，或者如果已在请求或响应通道上启动了异步IO，则将启动此任务。在这种情况下，交换将不会完成，完成交换完成交换取决于您的异步任务。 到目前为止，最常见的用途HttpServerExchange.dispatch()是将执行从不允许阻塞的IO线程移动到工作线程，这允许阻塞操作。这种模式通常看起来像： 调度到工作线程 因为在调用堆栈返回之前实际上并不调度exchange，所以可以确保一次交换中的一个线程永远不会活动。交换不是线程安全的，但是它可以在多个线程之间传递，只要两个线程不会立即尝试修改它，并且在第一个和第二个之间的操作（例如线程池调度）之前发生线程访问。 结束交换如上所述，一旦请求和响应通道都已关闭并刷新，则认为交换已完成。 有两种方法可以通过完全读取请求通道 调用shutdownWrites()响应通道然后刷新它 通过调用来结束交换HttpServerExchange.endExchange()。 当endExchange()调用时，Undertow将检查是否已生成内容，如果已生成，则它将简单地消耗请求通道，并关闭并刷新响应通道。如果没有，并且在交换机上注册了任何默认响应侦听器，那么Undertow将为每个人提供生成默认响应的机会。此机制是生成默认错误页面的方式。 endExchange() 会在内部调用 shutdownWrties方法 undertow 阻塞流支持HttpServerExchange(undertow对一次请求的抽象) 调用startBlocking() 返回一个BlockingHttpServerExchange 通过这个类可以正常调用HttpServerExchange.getInputStream()和 HttpServerExchange.getOutputStream()向他们写入数据 undertow 错误处理方法处理异常的最简单方法是在外部处理程序中捕获它们。例如： 响应监听器undertow因为各个请求都是使用的异步操作,所有在监听响应的时候需要使用异步回调的方法来进行监听操作 各种内置hander见 undertow 官方handle支持 undertow handle undertow servlet 支持暂时略过以后有时间的时候再看这个不重要 只写一个例子吧","date":"2024-12-31","categories":["java"],"tags":["undertow"]},{"title":"vert.x-(1)vert.x的核心设计和使用原则","url":"/2024/12/31/vert-x--1-vert-x的核心设计和使用原则/","content":"vert.x 的核心功能 编写 TCP 客户端和服务端 编写支持 WebSocket 的 HTTP 客户端和服务端 事件总线(Event Bus) 共享数据 —— 本地的Map和分布式集群Map 周期性、延迟性动作 部署和撤销 Verticle 实例 数据报套接字 DNS客户端 文件系统访问 高可用性 集群 vert.x 使用的三原则1. 流式简单的说就是.x().y().z() 这种写法 2. 回调最核心的特性，我们没有使用函数而是使用定义函数，由框架或者编译器来决定什么时候运行 3. 非阻塞这个是最重要的，因为vert.x 底层使用的单线程轮询的操作，如果将线程阻塞掉的话，换回导致所有的逻辑都在阻塞中 vert.x 本质上是多线程板的eventLoop 模式又叫Multi-Reactor 模式 vert.x 运行阻塞代码第一种方法，使用vertx.executeBlocking 第二种方法使用WorkerExecutor 这种方法相当于生成一个新的线程池让阻塞的代码在线程中运行","date":"2024-12-31","categories":["java"],"tags":["vertx"]},{"title":"vert.x-(2)一个简单的web服务器","url":"/2024/12/31/vert-x--2-一个简单的web服务器/","content":"vert.x 的核心就是异步非阻塞，大量使用了流式调用，相比较netty还是非常容易使用的 之前用过undertow 现在发现vert.x 和 undertow 非常相像 , 这里对比一下二者编写一个基本服务器的时候的代码 vert.x undertow 流程其实非常想像 初始状态->配置参数->指定handle->运行 接下来运行这段代码然后输入指定的url地址就可以实现查看相应的结果了","date":"2024-12-31","categories":["java"],"tags":["vertx"]},{"title":"vert.x-(3)-vert.x核心思操作","url":"/2024/12/31/vert-x--3--vert-x核心思操作/","content":"vert.x 其实在本质上并不单单是一个网络框架，更多的是封装了一层的函数式或者异步回掉的框架 vert.x 在使用的时候要求使用者尽量不使用阻塞的方法，支持非阻塞的方法，这些其实就是vert.x最初的设计哲学 一个基本列子，展示vert.x 的非阻塞操作 这个例子是一个定时执行任务的例子，后面的lamble表达式就是回调函数，之前表示执行的时间间隔，这个函数将会在1000毫秒后执行 vert.x 运行阻塞代码vert.x 提供两种方法来运行阻塞的代码 1. 使用executeBlocking 方法来实现阻塞调用 注意默认情况下，如果 executeBlocking 在同一个上下文环境中（如：同一个 Verticle 实例）被调用了多次，那么这些不同的 executeBlocking 代码块会 顺序执行（一个接一个）。 若您不需要关心您调用 executeBlocking 的顺序，可以将 ordered 参数(函数中的第二个参数)的值设为 false。这样任何 executeBlocking 都会在 Worker Pool 中并行执行。 使用 Worker Verticle 实现阻塞代码一个 Worker Verticle 始终会使用 Worker Pool 中的某个线程来执行。 默认的阻塞式代码会在 Vert.x 的 Worker Pool 中执行，通过 setWorkerPoolSize 配置。 可以为不同的用途创建不同的池： Worker Executor 在不需要的时候必须被关闭： 当使用同一个名字创建了许多 worker 时，它们将共享同一个 pool。当所有的 worker executor 调用了 close 方法被关闭过后，对应的 worker pool 会被销毁。 如果 Worker Executor 在 Verticle 中创建，那么 Verticle 实例销毁的同时 Vert.x 将会自动关闭这个 Worker Executor。 Worker Executor 可以在创建的时候配置：","date":"2024-12-31","categories":["java"],"tags":["vertx"]},{"title":"vert.x-(3)延迟函数和周期函数","url":"/2024/12/31/vert-x--3-延迟函数和周期函数/","content":"vert.x 提供了延迟函数和周期函数两种功能，本质上相当于两个定时器 在 Vert.x 中,想要延迟之后执行或定期执行操作很常见。在 Standard Verticle(之后会讨论) 中您不能直接让线程休眠以引入延迟,因为它会阻塞 Event Loop 线程 一次性计时器 - 只执行一次这个方法在使用lamble的时候传入的参数是这个定时器的id，之后vertx可以使用这个id来终止定时器的调用 多次调用定时器 - 这个方法vert.x 将会周期性的调用lamble中的handle 两种计时器的取消操作 这个方法其中的trmerId就是在声明计时器的时候产生的id编码","date":"2024-12-31","categories":["java"],"tags":["vertx"]},{"title":"vert.x-(4)常用类或方法","url":"/2024/12/31/vert-x--4-常用类或方法/","content":"Vertx类WorkerExecutor类Future类 和 CompositeFuture 类vert.x 支持并发合并操作，实现的方法就是使用上面的两个类 一个例子 Futuer 描述的是这个异步执行的结果状态，通过这种方法可以将下层的异步监听方法在上层反馈到 Funtuer的compose方法用于顺序组合Future 实现多个futuer 链式调用 这里例子中，有三个操作被串起来了： 一个文件被创建（fut1） 一些东西被写入到文件（fut2） 文件被移走（startFuture） 这个方法的参数有两种的形式 只有一个函数 : 两个函数 : CompsiteFuture 方法监听传入的Future类的状态，可以传入一个list列表或者最多留个Future参数 CompsiteFuture 提供的整理方法 any : 只要有一个成功，传入下一个handle中的状态就是成功 all : 必须全部成功，传入下一个handle中的状态才能是成功 join : 所有的future 都执行完成了没有一场无论结果是否失败，传入到下一个handle中的状态就是成功 vert.x VerticleVertticle是vert.x提供的一个简单便捷的、可扩展的、类似 Actor Model 的部署和并发模型机制 Verticle 是由 Vert.x 部署和运行的代码块。默认情况一个 Vert.x 实例维护了N（默认情况下N &#x3D; CPU核数 x 2）个 Event Loop 线程。Verticle 实例可使用任意 Vert.x 支持的编程语言编写，而且一个简单的应用程序也可以包含多种语言编写的 Verticle。 可以将 Verticle 想成 Actor Model 中的 Actor。 一个应用程序通常是由在同一个 Vert.x 实例中同时运行的许多 Verticle 实例组合而成。不同的 Verticle 实例通过向 Event Bus 上发送消息来相互通信。","date":"2024-12-31","categories":["java"],"tags":["vertx"]},{"title":"vert.x-体系(1)-基本使用方法","url":"/2024/12/31/vert-x-体系-1--基本使用方法/","content":"vert.x核心功能 编写 TCP 客户端和服务端 编写支持 WebSocket 的 HTTP 客户端和服务端 事件总线(Event Bus) 共享数据 —— 本地的Map和分布式集群Map 周期性、延迟性动作 部署和撤销 Verticle 实例 数据报套接字 DNS客户端 文件系统访问 高可用性 集群 其实vertx的最核心的东西就是异步和非阻塞 ps:除了极少数例外（即某些文件系统操作以’Sync’结尾），Vert.x中的所有API都不会阻塞调用线程。 vert.x使用方法 vert.x的使用其实非常简单,使用工厂方法既可以创建一个vert实例 ps: 我们可以使用VertxOptions方法创建一个vert配置的参数. vert.x的一些基本的使用方法 ps:除了极少数例外（即某些文件系统操作以’Sync’结尾），Vert.x中的所有API都不会阻塞调用线程。","date":"2024-12-31","categories":["java"],"tags":["vertx"]},{"title":"vert.x-体系(2)-eventloop","url":"/2024/12/31/vert-x-体系-2--eventloop/","content":"简单的说明一下vert.x eventLoop的相关信息Vert.x使用了Reactor模型，通过Event driven的方式，将Events分发到handlers中进行处理，任何event都不会阻塞Eventloop线程，这样Eventloop线程可以一直保持高速分发Events的速度。 假设在同一时间有很多HTTP请求到达服务器，Eventloop会将每一个http request分发到handler中处理，比如有10K个Requests,假设服务器想在1s内处理这些requests，那么平均算下来Eventloop必须能在0.1ms内分发完每个event，一旦Eventloop被阻塞，整个服务器吞吐量都会受到极大影响。 所以Eventloop绝对不应该被阻塞住，所有交给Eventloop去处理的event都应该是non-blocking(I&#x2F;O)的方式或者CPU执行时间较短的，这样才能确保每个event得到及时的分发。 而Vert.x不仅仅实现了Reactor模型，还实现了Multi-Reactor模型，也就是说，每个Vertx instance都会有多个Eventloop，默认的设置是Eventloop数量对应CPU核心数量乘以2，在VertxOptions类中可以看到 可以通过setEventLoopPoolSize()方法改变Vertx实例拥有Eventloop的数量。 vert使用的三大原则1. 流式简单的说就是.x().y().z() 这种写法 2. 回调最核心的特性，我们没有使用函数而是使用定义函数，由框架或者编译器来决定什么时候运行 3. 非阻塞这个是最重要的，因为vert.x 底层使用的单线程轮询的操作，如果将线程阻塞掉的话，换回导致所有的逻辑都在阻塞中 vert.x 本质上是多线程板的eventLoop 模式又叫Multi-Reactor 模式","date":"2024-12-31","categories":["java"],"tags":["vertx"]},{"title":"vert.x-体系(3)-Verticle","url":"/2024/12/31/vert-x-体系-3--Verticle/","content":"vert.x提供的Verticle这种方便部署","date":"2024-12-31","categories":["java"],"tags":["vertx"]},{"title":"vert.x-使用(1)-创建一个TCP服务器","url":"/2024/12/31/vert-x-使用-1--创建一个TCP服务器/","content":"vert.x 创建一个TCP服务器是超级简单的创建一个server端服务器 从这个方法中可以看出来vert.x自身的一切皆异步的特性,其实看上面的注释应该就可以理解了 创建一个客户端 vert.x tcp 其他特性1. socket 检索 本地地址和远程地址vert.x 提供了两种方法获取本地(我的地址)和远程地址(去链接的地址) 2. socket 文件处理vert.x 针对文件的处理方法同样非常简单使用sendfile方法就可以进行文件数据的发送了 注意: 这个文件的地址必须是这个项目的相对路径地址 3. vert.x 的tcp使用的eventLoop,是在一个核心上运行的,如果要使用更好的性能,自动的扩展到多核心上去,可以采用一下的方法 果使用Verticle，则可以使用-instances命令行上的选项简单地部署服务器Verticle的更多实例： 或者以编程方式部署Verticle 执行此操作后，您会发现echo服务器在功能上与以前完全相同，但您的服务器上的所有核心都可以使用，并且可以处理更多工作。 vert.x tcp 的SSL&#x2F;TLS支持可以将TCP客户端和服务器配置为使用传输层安全性 - 早期版本的TLS称为SSL 在服务器上启用SSL&#x2F;TLS,并指定服务器的密钥&#x2F;证书 使用JDK附带的keytool实用程序管理Java密钥库 使用密钥文件初始化 读取密钥存储区作为缓冲区并直接提供 PKCS＃12格式的密钥&#x2F;证书 使用密钥文件初始化 读取密钥存储区作为缓冲区并直接提供 使用.pem文件分别提供服务器私钥和证书 使用密钥文件初始化 还支持缓冲区配置 ps:暂时就看这么多吧,改天开个专题专门研究一下这个TSL&#x2F;SSL","date":"2024-12-31","categories":["java"],"tags":["vertx"]},{"title":"vert.x-使用(2)-创建一个HTTP服务器","url":"/2024/12/31/vert-x-使用-2--创建一个HTTP服务器/","content":"vert.x 简单请求处理 总结一下,这里说的简单请求处理,其实就是不包括表单提交和文件上传的处理,总结一下 vert.x 提供了几种回调的用法 handle : 最基本的请求处理方法,不保证数据完整 endHandlle : 最终状态的处理器,这个时候数据完整 bodyHandle : 专门处理简单类型大body的请求,相当于handle和endhandle配合使用的情况其他的回调 loadhandle : 表单文件上下传递御用handle exceptionHandler : 异常御用handle vert.x 提供了非常方便的参数处理方法,提供了一个特殊的Map–MultiMap,通过这个方法可以很方便的获取header,params,表单(下面会说明名)中的数据,和普通map的不同点是,MultiMap支持相同的key的情况 vert.x 简单响应发送处理 状态码设置 vert.x的 request对象可以设置setStatusCode来表示对应的状态码 写入相关的头部信息 vert.x 提供的很方便的消息头部调用 vert.x 分块http响应支持 vert.x 的表单处理方法vert.x如果要开启表单支持需要配置一些req的相关参数(针对使用了application&#x2F;x-www-form-urlencoded或multipart&#x2F;form-data协议的表单) 我们之前讨论过MultiMap是啥,这里不多说了,他将会自动的解析表单中的内容 vert.x 的文件处理方法 返回文件流 vert.x core并没有提供路径解析的方法,这一点我们需要自己手动模拟,这一段代码实现了输入文件名称返回文件流的信息 接受文件上传请求 vert.x提供了一个时间loadhandle 专门处理文件上传操作 vert.x 对defaule或者gzip压缩体的支持vert.x提供一个开关来控制是否解压缩gzip方法 vert.x http client客户端处理vert.x 提供了非常方便的http请求处理函数 vert.x创建新的httpClient客户端 vert.x 通过使用这种方法,可以更加快速和便利的初始化客户端,并为他初始化指定的host post url等参数 vert.x发送简单无body请求 vert.x 为了方便发送简单的http请求,提供了一组方法来处理 vert.x 针对三种请求做了特殊化的处理 GET HEAD OPTIONS , 这三种方法都不会带有body参数,而是直接发出请求 方法所有参数如下,其中host port url等可以省略 发送一般请求 vert.x 提供了request方法用来发送一般的请求 这个就很明显了,vert.x 提供了request方法,我们可以指定httpmethod host port url 等等参数 注意最后的end和writer方法,这些方法和server中的类似,将会在body中添加相关的信息数据,并且必须使用了end才会真正的发送数据 http客户端拆分写法 注意点: 当写入请求时，第一次调用write将导致请求标头被写入线路 实际写入是异步的，可能在调用返回后的某个时间才会发生 具有请求主体的非分块HTTP请求需要提供Content-Length标头。 因此，如果您没有使用分块HTTP，则必须Content-Length在写入请求之前设置标头，否则将为时已晚。 如果您正在调用end带有字符串或缓冲区的方法之一，则Vert.x将Content-Length在写入请求主体之前自动计算并设置标头。 如果您使用HTTP分块，Content-Length则不需要标头，因此您无需预先计算大小。 vert.x 标准头写入 第一种方法,使用head的Multipmap实现 request 直接写入header 分块的HTTP请求这允许HTTP请求主体以块的形式写入，并且通常在大型请求主体流式传输到服务器时使用，服务器的大小事先不知道。 您使用HTTP将HTTP请求置于分块模式setChunked。 在chunked模式下，每次调用write都会导致新的块被写入线路。在分块模式下，无需预先设置Content-Length请求。 http超时client 或者 Client的Option可以设置Timeout超时事件 客户端以文件内容发送请求信息vert.x 可以将本地文件的内容作为输出,发送出去 客户端异常处理vert.x 提供了一个方法可以处理客户端的异常问题 客户端处理响应 引申一下:vert.x其实本质上还是能在client中直接编写回调函数的,提供一定的便利性,比如这样 vert.x client的cookie获取 vert.x 重定向和自定义重定向 vert.x默认重定向使用方法 自定义重定向 该政策处理原始HttpClientResponse收到并返回null 或者a Future。 当null返回，原来的响应被处理 返回Future时，请求将在成功完成后发送 返回future时，将在失败时调用请求中设置的异常处理程序","date":"2024-12-31","categories":["java"],"tags":["vertx"]},{"title":"vert.x-使用(3)-jdbc数据库支持","url":"/2024/12/31/vert-x-使用-3--jdbc数据库支持/","content":"","date":"2024-12-31","categories":["java"],"tags":["vertx"]},{"title":"vertx","url":"/2024/12/31/vertx/","content":"Vert.x 快速入门和进阶使用Vert.x 快速入门vertx简介 vertx 的现状和背后支持的组织 vertx 是 eclipse基金会的一个顶级项目,目前社区非常活跃,是在国外非常火的一个框架 vertx的核心思想 Reactive : 事件驱动式编程模型 reactive组织宣言 vertx生态系统完善 和spring 进行对比 项目 Spring Vertx 核心框架 spring-core vertx-core Web开发 spring-webmvc vertx-web jdbc框架 spring-jdbc vertx-jdbc-client redis spring-data-redis vertx-redis-client 微服务 spring-cloud vertx-hazelcast vertx核心功能点1. 使用reactive模型快速构建简单web应用vertx 构建简单 web应用分为三步 创建并配置Vertx对象实例 创建并配置HttpServer对象实例 使用回调方式编写业务逻辑代码 完整示例 2. 内置微服务&#x2F;分布式支持EventBus eventBus简介 eventBus 官方对的定义是vertx的”神经系统”,用来模糊掉系统相互调用时候的强耦合\\硬编码的部分. eventBus 将各个逻辑模块化,并且支持不同语言编写的模块相互通信 event bus支持发布&#x2F;订阅模式，点对点模式，和请求&#x2F;响应模式。 EventBus 使用方法EventBus 的使用方法非常简单,只需要在上面实现的简单服务器加上几行就可以实现了 EventBus 扩展到分布式模式Vertx 内置了一个接口ClusterManager,通过这个接口可以自己实现 分布式逻辑 Vertx 官方提供了一个默认的ClusterManager实现 - Hazelcast Hazelcast 简介 Hazelcast 是由Hazelcast公司开发和维护的开源产品，可以为基于jvm环境运行的各种应用提供分布式集群和分布式缓存服务。类似golang的etcd,不过可以嵌入到java中运行可以做”服务发现治理”,”分布式存储”的底层依赖. Vertx 嵌入 Hazelcast 实现分布式非常简单,只需要在EventBus上面增加几行代码就能实现 server client 3. 插件化代码组织方式vertx支持基于Verticle接口的方式组织代码,沿用一种插件化编程的思想,最大限度的提高功能或者代码逻辑组织过程中的灵活度 插件化思想的历史 插件化思想最早是来源于android开发,早期的android开发因为android jvm的问题存在64K方法数魔咒-android的Dalvik vm的可执行文件规范限制了单个.dex文件最多引用的方法数是65536个。 为了解决这个问题,有人想想出了利用java热部署的能力开发插件化功能,支持插件化的app可以在运行时加载和运行class文件，这样便可以将app中一些不常用的功能模块做成插件，减小了安装包的大小和函数数量 , 并且实现了app功能的动态扩展 vertx 引入插件化开发同样非常简单 共需要两部 构造好Verticle接口实现类 Vertx加载这个实现类 4. 基于jvm的多语言支持vertx的多语言支持其实依赖于 Vertx的插件化部署和jvm的多语言支持 例子 groovy Verticle 部署 vertx 基础部分总结vertx 提供了四大特性 高性能 - 底层netty &#x2F; 异步化编程 EventBus内置分布式 Hazelcast Verticle插件化 高度扩展性 多语言支持 依赖jvm vertx进阶 - 实用vertx 实现一个简单的在线函数运行平台(Faas平台)一个Faas需要的东西 支持函数运行 分布式调用(函数及服务) 多语言支持 (多种语言编写函数) Vertx 和Faas 需求的对比 函数运行 - vertx Verticle 可以实现 分布式调用 - vertx Verticle 可以内置的使用EventBus实现分布式 多语言支持 - vertx 天然支持多语言 实现 - >轻量级函数运行时引擎","date":"2024-12-31","categories":["java"],"tags":["vertx"]},{"title":"kafka_consumer","url":"/2024/12/31/kafka_consumer/","content":"kafka consumerkafka 消费者与消费组消费者（Consumer）负责订阅Kafka中的主题（Topic），并且从订阅的主题上拉取消息。与其他一些消息中间件不同的是：在Kafka的消费理念中还有一层消费组（Consumer Group）的概念，每个消费者都有一个对应的消费组。当消息发布到主题后，只会被投递给订阅它的每个消费组中的一个消费者 kafka这块的模型大致是这种结构： 主题 - 细分成分区 -> 消费者组订阅主题 -> 组中的消费者分别订阅 分区 对于消息中间件而言，一般有两种消息投递模式：点对点（P2P，Point-to-Point）模式和发布&#x2F;订阅（Pub&#x2F;Sub）模式。 点对点模式是基于队列的，消息生产者发送消息到队列，消息消费者从队列中接收消息。 发布订阅模式定义了如何向一个内容节点发布和订阅消息，这个内容节点称为主题（Topic），主题可以认为是消息传递的中介，消息发布者将消息发布到某个主题，而消息订阅者从主题中订阅消息。 消费者并非逻辑上的概念，它是实际的应用实例，它可以是一个线程，也可以是一个进程。同一个消费组内的消费者既可以部署在同一台机器上，也可以部署在不同的机器上。 每一个消费组都会有一个固定的名称，消费者在进行消费前需要指定其所属消费组的名称，这个可以通过消费者客户端参数group.id来配置，默认值为空字符串。 主题使得消息的订阅者和发布者互相保持独立，不需要进行接触即可保证消息的传递，发布&#x2F;订阅模式在消息的一对多广播时采用。Kafka 同时支持两种消息投递模式，而这正是得益于消费者与消费组模型的契合： 如果所有的消费者都隶属于同一个消费组，那么所有的消息都会被均衡地投递给每一个消费者，即每条消息只会被一个消费者处理，这就相当于点对点模式的应用。 如果所有的消费者都隶属于不同的消费组，那么所有的消息都会被广播给所有的消费者，即每条消息会被所有的消费者处理，这就相当于发布&#x2F;订阅模式的应用。 一个正常的消费逻辑需要具备以下几个步骤（1）配置消费者客户端参数及创建相应的消费者实例。（2）订阅主题。（3）拉取消息并消费。（4）提交消费位移。（5）关闭消费者实例。 针对之前的模型 ， 我们在初始化kafka对应的配置的时候就需要配置这个消费者对应的消费组（和其他订阅者组相同或者不同能决定这个消息是广播还是点对点） kafka三种不同订阅状态集合订阅的方式subscribe（Collection）正则表达式订阅的方式subscribe（Pattern）指定分区的订阅方式assign（Collection）分表代表了三种不同的订阅状态：AUTO_TOPICS、AUTO_PATTERN和USER_ASSIGNED（如果没有订阅，那么订阅状态为NONE）。 USER_ASSIGNED（如果没有订阅，那么订阅状态为NONE）。然而这三种状态是互斥的，在一个消费者中只能使用其中的一种，否则会报出IllegalStateException异常 通过 subscribe（）方法订阅主题具有消费者自动再均衡的功能，在多个消费者的情况下可以根据分区分配策略来自动分配各个消费者与分区的关系。当消费组内的消费者增加或减少时，分区分配关系会自动调整，以实现消费负载均衡及故障自动转移。而通过assign（）方法订阅分区时，是不具备消费者自动均衡的功能的，其实这一点从assign（）方法的参数中就可以看出端倪，两种类型的subscribe（）都有ConsumerRebalanceListener类型参数的方法，而assign（）方法却没有 集合和正则对应：KafkaConsumer.subscribe（）方法分区订阅对应：KafkaConsumer.assgin（）方法 ， 这个需要制定这个集合定于那个消息的那个位置 kafka客户端反序列化同生产者 ， 要一一对应 kafka 消息消费Kafka中的消费是基于拉模式的。消息的消费一般有两种模式：推模式和拉模式。推模式是服务端主动将消息推送给消费者，而拉模式是消费者主动向服务端发起请求来拉取消息。 Kafka中的消息消费是一个不断轮询的过程，消费者所要做的就是重复地调用poll（）方法，而poll（）方法返回的是所订阅的主题（分区）上的一组消息 kafka poll过程到目前为止，可以简单地认为poll（）方法只是拉取一下消息而已，但就其内部逻辑而言并不简单，它涉及消费位移、消费者协调器、组协调器、消费者的选举、分区分配的分发、再均衡的逻辑、心跳等内容 下面一个一个介绍 消费位移对offset做了一些区分：对于消息在分区中的位置，我们将offset称为“偏移量”；对于消费者消费到的位置，将 offset 称为“位移”，有时候也会更明确地称之为“消费位移” 在每次调用poll（）方法时，它返回的是还没有被消费过的消息集（当然这个前提是消息已经存储在Kafka 中了，并且暂不考虑异常情况的发生），要做到这一点，就需要记录上一次消费时的消费位移。并且这个消费位移必须做持久化保存，而不是单单保存在内存中，否则消费者重启之后就无法知晓之前的消费位移。 在旧消费者客户端中，消费位移是存储在ZooKeeper中的。而在新消费者客户端中，消费位移存储在Kafka内部的主题__consumer_offsets中。这里把将消费位移存储起来（持久化）的动作称为“提交”，消费者在消费完消息之后需要执行消费位移的提交。 因为kafka的订阅机制，在一个主题下的一个分区只能被订阅这个主题的任意消费者组中的一个消费者消费 和之前的HW和LEO 类似 ， 这里提交的偏移量并不是当前消费玩的位置，而是下一个位置 在 Kafka 中默认的消费位移的提交方式是自动提交，这个是定期的，消费者每隔5秒会将拉取到的每个分区中最大的消息位移进行提交。自动位移提交的动作是在poll（）方法的逻辑里完成的，在每次真正向服务端发起拉取请求之前会检查是否可以进行位移提交，如果可以，那么就会提交上一次轮询的位移 消息异常的两种情况 1. 重复消费：消费完但是结果并没有提交，重新poll导致重新消费 2. 丢失：消费中出现异常，然后poll了，导致异常之后的消息没有被消费 自动位移提交的方式在正常情况下不会发生消息丢失或重复消费的现象，但是在编程的世界里异常无可避免，与此同时，自动位移提交也无法做到精确的位移管理。在Kafka中还提供了手动位移提交的方式，这样可以使得开发人员对消费位移的管理控制更加灵活。应于 KafkaConsumer 中的 commitSync（）和commitAsync（）两种类型的方法。 kafka手动控制提交commitSync（） 于采用 commitSync（）的无参方法而言，它提交消费位移的频率和拉取批次消息、处理批次消息的频率是一样的，如果想寻求更细粒度的、更精准的提交，那么就需要使用commitSync（）的另一个含参方法 这个手动参数有两个值 ， 一个描述分片的位置 ， 一个描述偏移量 我们在使用的时候可以使用消息类ConsumerRecord的api ， 比如 ConsumerRecords 类的 partitions（）方法和records（TopicPartition）方法获取某个分片中的所有值，或者直接使用offset方法来获取偏移量，topic和patition方法获取主题和分区 注意一个点 , kafka这个偏移值在客户端有一份，在服务端也有一份的，客户端如果没有重启的化是以客户端为依据的，如果有重启将会以服务端为依据 kafka consumer 控制和关闭KafkaConsumer中使用pause（）和resume（）方法来分别实现暂停某些分区在拉取操作时返回数据给客户端和恢复某些分区向客户端返回数据的操作 KafkaConsumer还提供了一个无参的paused（）方法来返回被暂停的分区集合，此方法的具体定义如下： KafkaConsumer的wakeup（）方法，wakeup（）方法是 KafkaConsumer 中唯一可以从其他线程里安全调用的方法（KafkaConsumer 是非线程安全的，可以通过3.2.10节了解更多细节），调用wakeup（）方法后可以退出poll（）的逻辑，并抛出 WakeupException 的异常，我们也不需要处理WakeupException 的异常，它只是一种跳出循环的方式 跳出循环以后一定要显式地执行关闭动作以释放运行过程中占用的各种系统资源，包括内存资源、Socket连接等。KafkaConsumer提供了close（）方法来实现关闭，close（）方法有三种重载方法，分别如下 第二种方法是通过 timeout 参数来设定关闭方法的最长执行时间，有些内部的关闭逻辑会耗费一定的时间，比如设置了自动提交消费位移，这里还会做一次位移提交的动作；而第一种方法没有 timeout 参数，这并不意味着会无限制地等待，它内部设定了最长等待时间（30秒）；第三种方法已被标记为@Deprecated，可以不考虑。 kafka 指定位移消费消费位移的提交，正是有了消费位移的持久化，才使消费者在关闭、崩溃或者在遇到再均衡的时候，可以让接替的消费者能够根据存储的消费位移继续进行消费 想一下，当一个新的消费组建立的时候，它根本没有可以查找的消费位移。或者消费组内的一个新消费者订阅了一个新的主题，它也没有可以查找的消费位移。当__consumer_offsets主题中有关这个消费组的位移信息过期而被删除后，它也没有可以查找的消费位移。 在 Kafka 中每当消费者查找不到所记录的消费位移时，就会根据消费者客户端参数auto.offset.reset的配置来决定从何处开始进行消费，这个参数的默认值为“latest”，表示从分区末尾开始消费消息。参考图3-9，按照默认的配置，消费者会从9开始进行消费（9是下一条要写入消息的位置），更加确切地说是从9开始拉取消息。如果将auto.offset.reset参数配置为“earliest”，那么消费者会从起始处，也就是0开始消费。 auto.offset.reset参数还有一个可配置的值—“none”，配置为此值就意味着出现查到不到消费位移的时候，既不从最新的消息位置处开始消费，也不从最早的消息位置处开始消费，此时会报出NoOffsetForPartitionException异常 到目前为止，我们知道消息的拉取是根据poll（）方法中的逻辑来处理的，这个poll（）方法中的逻辑对于普通的开发人员而言是一个黑盒，无法精确地掌控其消费的起始位置。提供的auto.offset.reset 参数也只能在找不到消费位移或位移越界的情况下粗粒度地从开头或末尾开始消费。有些时候，我们需要一种更细粒度的掌控，可以让我们从特定的位移处开始拉取消息，而 KafkaConsumer 中的 seek（）方法正好提供了这个功能，让我们得以追前消费或回溯消费。seek（）方法的具体定义如下 seek（）方法中的参数partition表示分区，而offset参数用来指定从分区的哪个位置开始消费。seek（）方法只能重置消费者分配到的分区的消费位置，而分区的分配是在 poll（）方法的调用过程中实现的。也就是说，在执行seek（）方法之前需要先执行一次poll（）方法，等到分配到分区之后才可以重置消费位置。 &#x2F;&#x2F;剩下的如何设置略 > 暂时没有遇到场景 kafka再均衡","date":"2024-12-31","categories":["kafka"]},{"title":"kafka_producer和元数据","url":"/2024/12/31/kafka_producer和元数据/","content":"kafka 基本概念 Producer：生产者，也就是发送消息的一方。生产者负责创建消息，然后将其投递到Kafka中 Consumer：消费者，也就是接收消息的一方。消费者连接到Kafka上并接收消息，进而进行相应的业务逻辑处理。 Broker：服务代理节点。对于Kafka而言，Broker可以简单地看作一个独立的Kafka服务节点或Kafka服务实例。大多数情况下也可以将Broker看作一台Kafka服务器，前提是这台服务器上只部署了一个Kafka实例。一个或多个Broker组成了一个Kafka集群。一般而言，我们更习惯使用首字母小写的broker来表示服务代理节点。 Topic: 服务主题 ， Kafka中的消息以主题为单位进行归类，生产者负责将消息发送到特定的主题（发送到Kafka集群中的每一条消息都要指定一个主题），而消费者负责订阅主题并进行消费。 Partition：一个topic中的主题可能会有多个分区 ， 并且不同的分区可能并不会在同一个broker中，同一主题下的不同分区包含的消息是不同的，分区在存储层面可以看作一个可追加的日志（Log）文件，消息在被追加到分区日志文件的时候都会分配一个特定的偏移量（offset）。offset是消息在分区中的唯一标识，Kafka通过它来保证消息在分区内的顺序性，不过offset并不跨越分区，也就是说，Kafka保证的是分区有序而不是主题有序。 Replica和leader&follow副本：kafka针对分区有多副本机制实现高可用，并且保证副本存放在不同的broker中 注意点： kafka消费者使用pull模式拉取 ？长轮询？ kafka基本架构系统架构 名词概念 AR（Assigned Replicas） ： kafka副本系统统称 ISR（In-Sync Replicas）： 与leader副本保持同步的副本 OSR（Out-of-Sync Replicas） leader副本负责维护和跟踪ISR和OSR集合的follower副本的滞后状态，当follower副本落后太多或失效时，leader副本会把它从ISR集合中剔除。如果OSR集合中有follower副本“追上”了leader副本，那么leader副本会把它从OSR集合转移至ISR集合 默认情况下，当leader副本发生故障时，只有在ISR集合中的副本才有资格被选举为新的leader，而在OSR集合中的副本则没有任何机会（不过这个原则也可以通过修改相应的参数配置来改变） LEO ： LEO是Log End Offset的缩写，它标识当前日志文件中下一条待写入消息的offset HW ： HW是High Watermark的缩写，俗称高水位，它标识了一个特定的消息偏移量（offset），消费者只能拉取到这个offset之前的消息。 注意： LEO和HW 都描述的是下一个能插入或者能读入的位置，而不是当前位置 LEO位置的确定方法是每一个副本当前能插入的下一个问题，HW的位置是leader通过同步当前所有副本中最小的LEO来决定HW kafka producerKafkaProducer创建发送消息对象ProducerRecord ? 暂时省略 发送消息三种主要模式发后即忘（fire-and-forget）、同步（sync）及异步（async） fire-and0forget：发后即忘，它只管往Kafka中发送消息而并不关心消息是否正确到达 sync：使用send方法 阻塞等待Kafka的响应 ,返回一个Future类，直到消息发送成功，或者发生异常。如果发生异常，那么就需要捕获异常并交由外层逻辑处理 async:一般是在send（）方法里指定一个Callback的回调函数，Kafka在返回响应时调用该函数来实现异步的发送确认 注意： 使用producer之后要调用close方法来关闭请求 kafka producer异常KafkaProducer中一般会发生两种类型的异常：可重试的异常和不可重试的异常。常见的可重试异常有：NetworkException、LeaderNotAvailableException、UnknownTopicOrPartitionException、NotEnoughReplicasException、NotCoordinatorException 等。比如NetworkException 表示网络异常，这个有可能是由于网络瞬时故障而导致的异常，可以通过重试解决；又比如LeaderNotAvailableException表示分区的leader副本不可用，这个异常通常发生在leader副本下线而新的 leader 副本选举完成之前，重试之后可以重新恢复。 不可重试的异常，比如RecordTooLargeException异常，暗示了所发送的消息太大，KafkaProducer对此不会进行任何重试，直接抛出异常。 对于可重试的异常，如果配置了 retries 参数，那么只要在规定的重试次数内自行恢复了，就不会抛出异常。retries参数的默认值为0，配置方式参考如下： 示例中配置了10次重试。如果重试了10次之后还没有恢复，那么仍会抛出异常，进而发送的外层逻辑就要处理这些异常了 producer发往 broker的过程消息在通过send（）方法发往broker的过程中，有可能需要经过拦截器（Interceptor）、序列化器（Serializer）和分区器（Partitioner）的一系列作用之后才能被真正地发往 broker 分区器: Partitioner-用于确定它发往的分区，如果消息ProducerRecord中指定了partition字段，那么就不需要分区器的作用，因为partition代表的就是所要发往的分区号 Kafka中提供的默认分区器是org.apache.kafka.clients.producer.internals.DefaultPartitioner，它实现了org.apache.kafka.clients.producer.Partitioner接口，这个接口中定义了2个方法 其中partition（）方法用来计算分区号，返回值为int类型。partition（）方法中的参数分别表示主题、键、序列化后的键、值、序列化后的值，以及集群的元数据信息，通过这些信息可以实现功能丰富的分区器。close（）方法在关闭分区器的时候用来回收一些资源。 在默认分区器 DefaultPartitioner 的实现中，close（）是空方法，而在 partition（）方法中定义了主要的分区分配逻辑。如果 key 不为 null，那么默认的分区器会对 key 进行哈希（采用MurmurHash2算法，具备高运算性能及低碰撞率），最终根据得到的哈希值来计算分区号，拥有相同key的消息会被写入同一个分区。如果key为null，那么消息将会以轮询的方式发往主题内的各个可用分区。 注意：如果 key 不为 null，那么计算得到的分区号会是所有分区中的任意一个；如果 key为null，那么计算得到的分区号仅为可用分区中的任意一个，注意两者之间的差别。 在不改变主题分区数量的情况下，key与分区之间的映射可以保持不变。不过，一旦主题中增加了分区，那么就难以保证key与分区之间的映射关系了。 自定义分区器实现 - 和default方法类似，只要自己定义Partitioner重写一下里面的接口实现即可 这个自定义分区器的实现比较简单，读者也可以根据自身业务的需求来灵活实现分配分区的计算方式，比如一般大型电商都有多个仓库，可以将仓库的名称或ID作为key来灵活地记录商品信息。 序列化器：Serializer 用于指定消息的序列化方法 kafka支持自定义的序列化和反序列化，只需要实现对应的接口就可以了org.apache.kafka.common.serialization.Serializer 注意：生产者和消费者对应的序列化协议必须要一致 生产者拦截器 生产者拦截器既可以用来在消息发送前做一些准备工作，比如按照某个规则过滤不符合要求的消息、修改消息的内容等，也可以用来在发送回调逻辑前做一些定制化的需求，比如统计类工作。 生产者拦截器的使用也很方便，主要是自定义实现 org.apache.kafka.clients.producer.ProducerInterceptor接口 onSend（）方法 ：KafkaProducer在将消息序列化和计算分区之前会调用生产者拦截器的onSend（）方法来对消息进行相应的定制化操作。一般来说最好不要修改消息 ProducerRecord 的 topic、key 和partition 等信息，如果要修改，则需确保对其有准确的判断，否则会与预想的效果出现偏差。比如修改key不仅会影响分区的计算，同样会影响broker端日志压缩（Log Compaction）的功能。 onAcknowledgement（）： KafkaProducer 会在消息被应答（Acknowledgement）之前或消息发送失败时调用生产者拦截器的onAcknowledgement（）方法，优先于用户设定的 Callback 之前执行。这个方法运行在Producer 的 I&#x2F;O 线程中，所以这个方法中实现的代码逻辑越简单越好，否则会影响消息的发送速度。 close（）方法 ：主要用于在关闭拦截器时执行一些资源的清理工作。在这 3 个方法中抛出的异常都会被捕获并记录到日志中，但并不会再向上传递。 注意： KafkaProducer中不仅可以指定一个拦截器，还可以指定多个拦截器以形成拦截链 kafka producer架构图 整个生产者客户端由两个线程协调运行，这两个线程分别为主线程和Sender线程（发送线程）。 在主线程中由KafkaProducer创建消息，然后通过可能的拦截器、序列化器和分区器的作用之后缓存到消息累加器（RecordAccumulator，也称为消息收集器）中。 RecordAccumulator 主要用来缓存消息以便 Sender 线程可以批量发送，进而减少网络传输的资源消耗以提升性能。RecordAccumulator 缓存的大小可以通过生产者客户端参数buffer.memory 配置，默认值为 33554432B，即 32MB。如果生产者发送消息的速度超过发送到服务器的速度，则会导致生产者空间不足，这个时候KafkaProducer的send（）方法调用要么被阻塞，要么抛出异常，这个取决于参数max.block.ms的配置，此参数的默认值为60000，即60秒。 在 RecordAccumulator 的内部为每个分区都维护了一个双端队列，队列中的内容就是ProducerBatch，即 Deque＜ProducerBatch＞。写入追加到尾部，读取从头部读取ProducerBatch中可以包含一至多个 ProducerRecord。通俗地说，ProducerRecord 是生产者中创建的消息，而ProducerBatch是指一个消息批次，ProducerRecord会被包含在ProducerBatch中，这样可以使字节的使用更加紧凑。与此同时，将较小的ProducerRecord拼凑成一个较大的ProducerBatch，也可以减少网络请求的次数以提升整体的吞吐量 消息在网络上都是以字节（Byte）的形式传输的，在发送之前需要创建一块内存区域来保存对应的消息。在Kafka生产者客户端中，通过java.io.ByteBuffer实现消息内存的创建和释放。不过频繁的创建和释放是比较耗费资源的，在RecordAccumulator的内部还有一个BufferPool，它主要用来实现ByteBuffer的复用，以实现缓存的高效利用。不过BufferPool只针对特定大小的ByteBuffer进行管理，而其他大小的ByteBuffer不会缓存进BufferPool中，这个特定的大小由batch.size参数来指定，默认值为16384B，即16KB。 我们可以适当地调大batch.size参数以便多缓存一些消息。ProducerBatch的大小和batch.size参数也有着密切的关系。当一条消息（ProducerRecord）流入RecordAccumulator时，会先寻找与消息分区所对应的双端队列（如果没有则新建），再从这个双端队列的尾部获取一个 ProducerBatch（如果没有则新建），查看 ProducerBatch 中是否还可以写入这个 ProducerRecord，如果可以则写入，如果不可以则需要创建一个新的ProducerBatch。在新建ProducerBatch时评估这条消息的大小是否超过batch.size参数的大小，如果不超过，那么就以 batch.size参数的大小来创建 ProducerBatch，这样在使用完这段内存区域之后，可以通过BufferPool 的管理来进行复用；如果超过，那么就以评估的大小来创建ProducerBatch，这段内存区域不会被复用。 Sender 线程负责从RecordAccumulator中获取消息并将其发送到Kafka中 Sender 从 RecordAccumulator 中获取缓存的消息之后，会进一步将原本＜分区，Deque＜ProducerBatch＞＞的保存形式转变成＜Node，List＜ ProducerBatch＞的形式，其中Node表示Kafka集群的broker节点。对于网络连接来说，生产者客户端是与具体的broker节点建立的连接，也就是向具体的 broker 节点发送消息，而并不关心消息属于哪一个分区；而对于 KafkaProducer的应用逻辑而言，我们只关注向哪个分区中发送哪些消息，所以在这里需要做一个应用逻辑层面到网络I&#x2F;O层面的转换。 在转换成＜Node，List＜ProducerBatch＞＞的形式之后，Sender 还会进一步封装成＜Node，Request＞的形式，这样就可以将Request请求发往各个Node了，这里的Request是指Kafka的各种协议请求，对于消息发送而言就是指具体的ProduceRequest Sender中的InFlightRequests集合 请求在从Sender线程发往Kafka之前还会保存到InFlightRequests中，InFlightRequests保存对象的具体形式为 Map＜NodeId，Deque＜Request＞＞，它的主要作用是缓存了已经发出去但还没有收到响应的请求（NodeId 是一个 String 类型，表示节点的 id 编号）。 与此同时，InFlightRequests还提供了许多管理类的方法，并且通过配置参数还可以限制每个连接（也就是客户端与Node之间的连接）最多缓存的请求数。这个配置参数为max.in.flight.requests.per.connection，默认值为 5，即每个连接最多只能缓存 5 个未响应的请求，超过该数值之后就不能再向这个连接发送更多的请求了，除非有缓存的请求收到了响应（Response）。通过比较Deque＜Request＞的size与这个参数的大小来判断对应的Node中是否已经堆积了很多未响应的消息，如果真是如此，那么说明这个 Node 节点负载较大或网络连接有问题，再继续向其发送请求会增大请求超时的可能。 InFlightRequests还可以获得leastLoadedNode，即所有Node中负载最小的那一个。这里的负载最小是通过每个Node在InFlightRequests中还未确认的请求决定的，未确认的请求越多则认为负载越大 选择leastLoadedNode发送请求可以使它能够尽快发出，避免因网络拥塞等异常而影响整体的进度。leastLoadedNode的概念可以用于多个应用场合，比如元数据请求、消费者组播协议的交互。 kafka 元数据KafkaProducer要将此消息追加到指定主题的某个分区所对应的leader副本之前，首先需要知道主题的分区数量，然后经过计算得出（或者直接指定）目标分区，之后KafkaProducer需要知道目标分区的leader副本所在的broker 节点的地址、端口等信息才能建立连接，最终才能将消息发送到 Kafka，在这一过程中所需要的信息都属于元数据信息。 元数据是指Kafka集群的元数据，这些元数据具体记录了集群中有哪些主题，这些主题有哪些分区，每个分区的leader副本分配在哪个节点上，follower副本分配在哪些节点上，哪些副本在AR、ISR等集合中，集群中有哪些节点，控制器节点又是哪一个等信息。 当客户端中没有需要使用的元数据信息时，比如没有指定的主题信息，或者超过metadata.max.age.ms 时间没有更新元数据都会引起元数据的更新操作。客户端参数metadata.max.age.ms的默认值为300000，即5分钟。元数据的更新操作是在客户端内部进行的，对客户端的外部使用者不可见。当需要更新元数据时，会先挑选出leastLoadedNode，然后向这个Node发送MetadataRequest请求来获取具体的元数据信息。这个更新操作是由Sender线程发起的，在创建完MetadataRequest之后同样会存入InFlightRequests，之后的步骤就和发送消息时的类似。元数据虽然由Sender线程负责更新，但是主线程也需要读取这些信息，这里的数据同步通过synchronized和final关键字来保障。","date":"2024-12-31","categories":["kafka"]},{"title":"kafka架构设计和实现关键点","url":"/2024/12/31/kafka架构设计和实现关键点/","content":"kafka 基本概念 Producer：生产者，也就是发送消息的一方。生产者负责创建消息，然后将其投递到Kafka中 Consumer：消费者，也就是接收消息的一方。消费者连接到Kafka上并接收消息，进而进行相应的业务逻辑处理。 Broker：服务代理节点。对于Kafka而言，Broker可以简单地看作一个独立的Kafka服务节点或Kafka服务实例。大多数情况下也可以将Broker看作一台Kafka服务器，前提是这台服务器上只部署了一个Kafka实例。一个或多个Broker组成了一个Kafka集群。一般而言，我们更习惯使用首字母小写的broker来表示服务代理节点。 Topic: 服务主题 ， Kafka中的消息以主题为单位进行归类，生产者负责将消息发送到特定的主题（发送到Kafka集群中的每一条消息都要指定一个主题），而消费者负责订阅主题并进行消费。 Partition：一个topic中的主题可能会有多个分区 ， 并且不同的分区可能并不会在同一个broker中，同一主题下的不同分区包含的消息是不同的，分区在存储层面可以看作一个可追加的日志（Log）文件，消息在被追加到分区日志文件的时候都会分配一个特定的偏移量（offset）。offset是消息在分区中的唯一标识，Kafka通过它来保证消息在分区内的顺序性，不过offset并不跨越分区，也就是说，Kafka保证的是分区有序而不是主题有序。 Replica和leader&follow副本：kafka针对分区有多副本机制实现高可用，并且保证副本存放在不同的broker中 注意点： kafka消费者使用pull模式拉取 ？长轮询？ kafka基本架构系统架构 名词概念 AR（Assigned Replicas） ： kafka副本系统统称 ISR（In-Sync Replicas）： 与leader副本保持同步的副本 OSR（Out-of-Sync Replicas） leader副本负责维护和跟踪ISR和OSR集合的follower副本的滞后状态，当follower副本落后太多或失效时，leader副本会把它从ISR集合中剔除。如果OSR集合中有follower副本“追上”了leader副本，那么leader副本会把它从OSR集合转移至ISR集合 默认情况下，当leader副本发生故障时，只有在ISR集合中的副本才有资格被选举为新的leader，而在OSR集合中的副本则没有任何机会（不过这个原则也可以通过修改相应的参数配置来改变） LEO ： LEO是Log End Offset的缩写，它标识当前日志文件中下一条待写入消息的offset HW ： HW是High Watermark的缩写，俗称高水位，它标识了一个特定的消息偏移量（offset），消费者只能拉取到这个offset之前的消息。 注意： LEO和HW 都描述的是下一个能插入或者能读入的位置，而不是当前位置 LEO位置的确定方法是每一个副本当前能插入的下一个问题，HW的位置是leader通过同步当前所有副本中最小的LEO来决定HW kafka producerKafkaProducer创建发送消息对象ProducerRecord ? 暂时省略 发送消息三种主要模式发后即忘（fire-and-forget）、同步（sync）及异步（async） fire-and0forget：发后即忘，它只管往Kafka中发送消息而并不关心消息是否正确到达 sync：使用send方法 阻塞等待Kafka的响应 ,返回一个Future类，直到消息发送成功，或者发生异常。如果发生异常，那么就需要捕获异常并交由外层逻辑处理 async:一般是在send（）方法里指定一个Callback的回调函数，Kafka在返回响应时调用该函数来实现异步的发送确认 注意： 使用producer之后要调用close方法来关闭请求 kafka producer异常KafkaProducer中一般会发生两种类型的异常：可重试的异常和不可重试的异常。常见的可重试异常有：NetworkException、LeaderNotAvailableException、UnknownTopicOrPartitionException、NotEnoughReplicasException、NotCoordinatorException 等。比如NetworkException 表示网络异常，这个有可能是由于网络瞬时故障而导致的异常，可以通过重试解决；又比如LeaderNotAvailableException表示分区的leader副本不可用，这个异常通常发生在leader副本下线而新的 leader 副本选举完成之前，重试之后可以重新恢复。 不可重试的异常，比如RecordTooLargeException异常，暗示了所发送的消息太大，KafkaProducer对此不会进行任何重试，直接抛出异常。 对于可重试的异常，如果配置了 retries 参数，那么只要在规定的重试次数内自行恢复了，就不会抛出异常。retries参数的默认值为0，配置方式参考如下： 示例中配置了10次重试。如果重试了10次之后还没有恢复，那么仍会抛出异常，进而发送的外层逻辑就要处理这些异常了 producer发往 broker的过程消息在通过send（）方法发往broker的过程中，有可能需要经过拦截器（Interceptor）、序列化器（Serializer）和分区器（Partitioner）的一系列作用之后才能被真正地发往 broker 分区器: Partitioner-用于确定它发往的分区，如果消息ProducerRecord中指定了partition字段，那么就不需要分区器的作用，因为partition代表的就是所要发往的分区号 Kafka中提供的默认分区器是org.apache.kafka.clients.producer.internals.DefaultPartitioner，它实现了org.apache.kafka.clients.producer.Partitioner接口，这个接口中定义了2个方法 其中partition（）方法用来计算分区号，返回值为int类型。partition（）方法中的参数分别表示主题、键、序列化后的键、值、序列化后的值，以及集群的元数据信息，通过这些信息可以实现功能丰富的分区器。close（）方法在关闭分区器的时候用来回收一些资源。 在默认分区器 DefaultPartitioner 的实现中，close（）是空方法，而在 partition（）方法中定义了主要的分区分配逻辑。如果 key 不为 null，那么默认的分区器会对 key 进行哈希（采用MurmurHash2算法，具备高运算性能及低碰撞率），最终根据得到的哈希值来计算分区号，拥有相同key的消息会被写入同一个分区。如果key为null，那么消息将会以轮询的方式发往主题内的各个可用分区。 注意：如果 key 不为 null，那么计算得到的分区号会是所有分区中的任意一个；如果 key为null，那么计算得到的分区号仅为可用分区中的任意一个，注意两者之间的差别。 在不改变主题分区数量的情况下，key与分区之间的映射可以保持不变。不过，一旦主题中增加了分区，那么就难以保证key与分区之间的映射关系了。 自定义分区器实现 - 和default方法类似，只要自己定义Partitioner重写一下里面的接口实现即可 这个自定义分区器的实现比较简单，读者也可以根据自身业务的需求来灵活实现分配分区的计算方式，比如一般大型电商都有多个仓库，可以将仓库的名称或ID作为key来灵活地记录商品信息。 序列化器：Serializer 用于指定消息的序列化方法 kafka支持自定义的序列化和反序列化，只需要实现对应的接口就可以了org.apache.kafka.common.serialization.Serializer 注意：生产者和消费者对应的序列化协议必须要一致 生产者拦截器 生产者拦截器既可以用来在消息发送前做一些准备工作，比如按照某个规则过滤不符合要求的消息、修改消息的内容等，也可以用来在发送回调逻辑前做一些定制化的需求，比如统计类工作。 生产者拦截器的使用也很方便，主要是自定义实现 org.apache.kafka.clients.producer.ProducerInterceptor接口 onSend（）方法 ：KafkaProducer在将消息序列化和计算分区之前会调用生产者拦截器的onSend（）方法来对消息进行相应的定制化操作。一般来说最好不要修改消息 ProducerRecord 的 topic、key 和partition 等信息，如果要修改，则需确保对其有准确的判断，否则会与预想的效果出现偏差。比如修改key不仅会影响分区的计算，同样会影响broker端日志压缩（Log Compaction）的功能。 onAcknowledgement（）： KafkaProducer 会在消息被应答（Acknowledgement）之前或消息发送失败时调用生产者拦截器的onAcknowledgement（）方法，优先于用户设定的 Callback 之前执行。这个方法运行在Producer 的 I&#x2F;O 线程中，所以这个方法中实现的代码逻辑越简单越好，否则会影响消息的发送速度。 close（）方法 ：主要用于在关闭拦截器时执行一些资源的清理工作。在这 3 个方法中抛出的异常都会被捕获并记录到日志中，但并不会再向上传递。 注意： KafkaProducer中不仅可以指定一个拦截器，还可以指定多个拦截器以形成拦截链 kafka producer架构图 整个生产者客户端由两个线程协调运行，这两个线程分别为主线程和Sender线程（发送线程）。 在主线程中由KafkaProducer创建消息，然后通过可能的拦截器、序列化器和分区器的作用之后缓存到消息累加器（RecordAccumulator，也称为消息收集器）中。 RecordAccumulator 主要用来缓存消息以便 Sender 线程可以批量发送，进而减少网络传输的资源消耗以提升性能。RecordAccumulator 缓存的大小可以通过生产者客户端参数buffer.memory 配置，默认值为 33554432B，即 32MB。如果生产者发送消息的速度超过发送到服务器的速度，则会导致生产者空间不足，这个时候KafkaProducer的send（）方法调用要么被阻塞，要么抛出异常，这个取决于参数max.block.ms的配置，此参数的默认值为60000，即60秒。 在 RecordAccumulator 的内部为每个分区都维护了一个双端队列，队列中的内容就是ProducerBatch，即 Deque＜ProducerBatch＞。写入追加到尾部，读取从头部读取ProducerBatch中可以包含一至多个 ProducerRecord。通俗地说，ProducerRecord 是生产者中创建的消息，而ProducerBatch是指一个消息批次，ProducerRecord会被包含在ProducerBatch中，这样可以使字节的使用更加紧凑。与此同时，将较小的ProducerRecord拼凑成一个较大的ProducerBatch，也可以减少网络请求的次数以提升整体的吞吐量 消息在网络上都是以字节（Byte）的形式传输的，在发送之前需要创建一块内存区域来保存对应的消息。在Kafka生产者客户端中，通过java.io.ByteBuffer实现消息内存的创建和释放。不过频繁的创建和释放是比较耗费资源的，在RecordAccumulator的内部还有一个BufferPool，它主要用来实现ByteBuffer的复用，以实现缓存的高效利用。不过BufferPool只针对特定大小的ByteBuffer进行管理，而其他大小的ByteBuffer不会缓存进BufferPool中，这个特定的大小由batch.size参数来指定，默认值为16384B，即16KB。 我们可以适当地调大batch.size参数以便多缓存一些消息。ProducerBatch的大小和batch.size参数也有着密切的关系。当一条消息（ProducerRecord）流入RecordAccumulator时，会先寻找与消息分区所对应的双端队列（如果没有则新建），再从这个双端队列的尾部获取一个 ProducerBatch（如果没有则新建），查看 ProducerBatch 中是否还可以写入这个 ProducerRecord，如果可以则写入，如果不可以则需要创建一个新的ProducerBatch。在新建ProducerBatch时评估这条消息的大小是否超过batch.size参数的大小，如果不超过，那么就以 batch.size参数的大小来创建 ProducerBatch，这样在使用完这段内存区域之后，可以通过BufferPool 的管理来进行复用；如果超过，那么就以评估的大小来创建ProducerBatch，这段内存区域不会被复用。 Sender 线程负责从RecordAccumulator中获取消息并将其发送到Kafka中 Sender 从 RecordAccumulator 中获取缓存的消息之后，会进一步将原本＜分区，Deque＜ProducerBatch＞＞的保存形式转变成＜Node，List＜ ProducerBatch＞的形式，其中Node表示Kafka集群的broker节点。对于网络连接来说，生产者客户端是与具体的broker节点建立的连接，也就是向具体的 broker 节点发送消息，而并不关心消息属于哪一个分区；而对于 KafkaProducer的应用逻辑而言，我们只关注向哪个分区中发送哪些消息，所以在这里需要做一个应用逻辑层面到网络I&#x2F;O层面的转换。 在转换成＜Node，List＜ProducerBatch＞＞的形式之后，Sender 还会进一步封装成＜Node，Request＞的形式，这样就可以将Request请求发往各个Node了，这里的Request是指Kafka的各种协议请求，对于消息发送而言就是指具体的ProduceRequest Sender中的InFlightRequests集合 请求在从Sender线程发往Kafka之前还会保存到InFlightRequests中，InFlightRequests保存对象的具体形式为 Map＜NodeId，Deque＜Request＞＞，它的主要作用是缓存了已经发出去但还没有收到响应的请求（NodeId 是一个 String 类型，表示节点的 id 编号）。 与此同时，InFlightRequests还提供了许多管理类的方法，并且通过配置参数还可以限制每个连接（也就是客户端与Node之间的连接）最多缓存的请求数。这个配置参数为max.in.flight.requests.per.connection，默认值为 5，即每个连接最多只能缓存 5 个未响应的请求，超过该数值之后就不能再向这个连接发送更多的请求了，除非有缓存的请求收到了响应（Response）。通过比较Deque＜Request＞的size与这个参数的大小来判断对应的Node中是否已经堆积了很多未响应的消息，如果真是如此，那么说明这个 Node 节点负载较大或网络连接有问题，再继续向其发送请求会增大请求超时的可能。 InFlightRequests还可以获得leastLoadedNode，即所有Node中负载最小的那一个。这里的负载最小是通过每个Node在InFlightRequests中还未确认的请求决定的，未确认的请求越多则认为负载越大 选择leastLoadedNode发送请求可以使它能够尽快发出，避免因网络拥塞等异常而影响整体的进度。leastLoadedNode的概念可以用于多个应用场合，比如元数据请求、消费者组播协议的交互。 kafka 元数据KafkaProducer要将此消息追加到指定主题的某个分区所对应的leader副本之前，首先需要知道主题的分区数量，然后经过计算得出（或者直接指定）目标分区，之后KafkaProducer需要知道目标分区的leader副本所在的broker 节点的地址、端口等信息才能建立连接，最终才能将消息发送到 Kafka，在这一过程中所需要的信息都属于元数据信息。 元数据是指Kafka集群的元数据，这些元数据具体记录了集群中有哪些主题，这些主题有哪些分区，每个分区的leader副本分配在哪个节点上，follower副本分配在哪些节点上，哪些副本在AR、ISR等集合中，集群中有哪些节点，控制器节点又是哪一个等信息。 当客户端中没有需要使用的元数据信息时，比如没有指定的主题信息，或者超过metadata.max.age.ms 时间没有更新元数据都会引起元数据的更新操作。客户端参数metadata.max.age.ms的默认值为300000，即5分钟。元数据的更新操作是在客户端内部进行的，对客户端的外部使用者不可见。当需要更新元数据时，会先挑选出leastLoadedNode，然后向这个Node发送MetadataRequest请求来获取具体的元数据信息。这个更新操作是由Sender线程发起的，在创建完MetadataRequest之后同样会存入InFlightRequests，之后的步骤就和发送消息时的类似。元数据虽然由Sender线程负责更新，但是主线程也需要读取这些信息，这里的数据同步通过synchronized和final关键字来保障。","date":"2024-12-31","categories":["kafka"]},{"title":"linux-linuxmint等ubuntu系统全局字体不统一，楷体无法更换问题","url":"/2024/12/31/linux-linuxmint等ubuntu系统全局字体不统一-楷体无法更换问题/","content":"linux mint等ubuntu系统全局字体不统一，楷体无法更换问题此页面是关于此楷体无法更换问题的，应该是类乌班图系统都会遇到的问题: https://bugs.launchpad.net/ubuntukylin/+bug/1227034 新安装的Linux mint 字体更新了以后，设置的全局字体不统一，部分字体偏小，部分字体甚至是比较小的楷体，模糊不清，十分影响观感和使用体验。 终端使用命令fc-match ‘sans-serif’和fc-match ‘serif’和fc-match ‘monospace’均输出类似于ukai.ttc: “AR PL UKai CN” “Book”如果删除了ukai即楷体则输出uming.ttc: “AR PL UMing CN” “Light”。种种迹象表面是&#x2F;etc&#x2F;font&#x2F;conf.d中的某文件改写错误造成.新安装的linuxMint应该均会出现这样的楷体错误。 一个简单的解决办法是终端输入命令： 安装完成后字体会改变为 另外建议新安装的Mint18全中文系统，使用文本编辑器比如kate执行 或者 编辑其中的一个文件即可。添加如下内容，以配置更完全详细的中文本地支持. 然后执行 sudo locale-gen","date":"2024-12-31","categories":["linux"]},{"title":"linux-shadowsock梯子","url":"/2024/12/31/linux-shadowsock梯子/","content":"ss-server -s 149.28.211.158 -p 11098 -k woaiwo1314ya.. -m aes-128-cfb -t 3000 ex","date":"2024-12-31","categories":["linux"]},{"title":"linux-ubuntu环境下php+nginx配置踩坑","url":"/2024/12/31/linux-ubuntu环境下php-nginx配置踩坑/","content":"nginx安装方法—进入官网 查看官方教程就好了http://nginx.org/en/linux_packages.html#stable php安装 使用ppa方法进行相关的安装 接下来按照相关的规定进行安装就好哦nginx 配置 打开&#x2F;etc&#x2F;nginx&#x2F; 目录下的配置文件修改如下 修改php-fpm 相关配置如下 添加如下字段 开启php调试模式 修改fpm cli等php.ini中的如下字段","date":"2024-12-31","categories":["linux"]},{"title":"rocketmq-原理记录-通信-消息发送-消息接收","url":"/2024/12/31/rocketmq-原理记录-通信-消息发送-消息接收/","content":"一.rocketmq模块架构 rocketMQ中的几个重要的角色 (1)NameServer:在MQ集群中做的是做命名服务，更新和路由发现 broker服务；(2)Broker-Master:broker 消息主机服务器；(3)Broker-Slave:broker 消息从机服务器；(4)Producer:消息生产者；(5)Consumer:消息消费者； 二. rocketmq 通信的过程和方法RocketMQ集群的一部分通信如下: (1)Broker启动后需要完成一次将自己注册至NameServer的操作；随后每隔30s时间定期向NameServer上报Topic路由信息； (2)消息生产者Producer作为客户端发送消息时候，需要根据Msg的Topic从本地缓存的TopicPublishInfoTable获取路由信息。如果没有则更新路由信息会从NameServer上重新拉取； (3)消息生产者Producer根据(2)中获取的路由信息选择一个队列(MessageQueue)进行消息发送；Broker作为消息的接收者收消息并落盘存储； 从上面(1)~(3)中可以看出在消息生产者, Broker和NameServer之间都会发生通信(这里只说了MQ的部分通信)，因此如何设计一个良好的网络通信模块在MQ中至关重要，它将决定RocketMQ集群整体的消息传输能力与最终的性能。 rocketmq-remoting 模块是 RocketMQ消息队列中负责网络通信的模块，它几乎被其他所有需要网络通信的模块(诸如rocketmq-client、rocketmq-server、rocketmq-namesrv)所依赖和引用。为了实现客户端与服务器之间高效的数据请求与接收，RocketMQ消息队列自定义了通信协议并在Netty的基础之上扩展了通信模块。 三. 消息的协议设计与编码解码在Client和Server之间完成一次消息发送时，需要对发送的消息进行一个协议约定，因此就有必要自定义RocketMQ的消息协议。同时，为了高效地在网络中传输消息和对收到的消息读取，就需要对消息进行编解码。 在RocketMQ中，RemotingCommand这个类在消息传输过程中对所有数据内容的封装，不但包含了所有的数据结构，还包含了编码解码操作。 RemotingCommand类的部分成员变量如下: Header字段 类型 Request说明 Response说明 code int 请求操作码，应答方根据不同的请求码进行不同的业务处理 应答响应码。0表示成功，非0则表示各种错误 language LanguageCode 请求方实现的语言 应答方实现的语言 version int 请求方程序的版本 应答方程序的版本 opaque int 相当于reqeustId，在同一个连接上的不同请求标识码，与响应消息中的相对应 应答不做修改直接返回 flag int 区分是普通RPC还是onewayRPC得标志 区分是普通RPC还是onewayRPC得标志 remark String 传输自定义文本信息 传输自定义文本信息 extFields HashMap<String,String> 请求自定义扩展信息 响应自定义扩展信息 这里展示下Broker向NameServer发送一次心跳注册的报文: 下面来看下RocketMQ通信协议的格式 可见传输内容主要可以分为以下4部分: (1)消息长度:总长度，四个字节存储，占用一个int类型； (2)序列化类型&消息头长度:同样占用一个int类型，第一个字节表示序列化类型，后面三个字节表示消息头长度； (3)消息头数据:经过序列化后的消息头数据； (4)消息主体数据:消息主体的二进制字节数据内容； 消息的编码和解码分别在RemotingCommand类的encode和decode方法中完成. 四. 消息通信方式和通信流程在RocketMQ消息队列中支持通信的方式主要有以下三种: (1)同步(sync) (2)异步(async) (3)单向(oneway) 其中“同步”通信模式相对简单，一般用在发送心跳包场景下，无需关注其Response。本文将主要介绍RocketMQ的异步通信流程(限于篇幅，读者可以按照同样的模式进行分析同步通信流程) 五. rocketMq消息发送 对于上图中几个角色的说明: (1)NameServer:RocketMQ集群的命名服务器(也可以说是注册中心)，它本身是无状态的(实际情况下可能存在每个NameServer实例上的数据有短暂的不一致现象，但是通过定时更新，在大部分情况下都是一致的)，用于管理集群的元数据( 例如，KV配置、Topic、Broker的注册信息)。 (2)Broker(Master):RocketMQ消息代理服务器主节点，起到串联Producer的消息发送和Consumer的消息消费，和将消息的落盘存储的作用； (3)Broker(Slave):RocketMQ消息代理服务器备份节点，主要是通过同步&#x2F;异步的方式将主节点的消息同步过来进行备份，为RocketMQ集群的高可用性提供保障； (4)Producer(消息生产者):在这里为普通消息的生产者，主要基于RocketMQ-Client模块将消息发送至RocketMQ的主节点。 对于上面图中几条通信链路的关系: (1)Producer与NamerServer:每一个Producer会与NameServer集群中的一个实例建立TCP连接，从这个NameServer实例上拉取Topic路由信息； (2)Producer和Broker:Producer会和它要发送的topic相关联的Master的Broker代理服务器建立TCP连接，用于发送消息以及定时的心跳信息； (3)Broker和NamerServer:Broker(Master or Slave)均会和每一个NameServer实例来建立TCP连接。Broker在启动的时候会注册自己配置的Topic信息到NameServer集群的每一台机器中。即每一个NameServer均有该broker的Topic路由配置信息。其中，Master与Master之间无连接，Master与Slave之间有连接； 六. rocketMQ 消息消费MQ中Pull和Push的两种消费方式对于任何一款消息中间件而言，消费者客户端一般有两种方式从消息中间件获取消息并消费：（1）Push方式：由消息中间件（MQ消息服务器代理）主动地将消息推送给消费者；采用Push方式，可以尽可能实时地将消息发送给消费者进行消费。但是，在消费者的处理消息的能力较弱的时候(比如，消费者端的业务系统处理一条消息的流程比较复杂，其中的调用链路比较多导致消费时间比较久。概括起来地说就是“慢消费问题”)，而MQ不断地向消费者Push消息，消费者端的缓冲区可能会溢出，导致异常；（2）Pull方式：由消费者客户端主动向消息中间件（MQ消息服务器代理）拉取消息；采用Pull方式，如何设置Pull消息的频率需要重点去考虑，举个例子来说，可能1分钟内连续来了1000条消息，然后2小时内没有新消息产生（概括起来说就是“消息延迟与忙等待”）。如果每次Pull的时间间隔比较久，会增加消息的延迟，即消息到达消费者的时间加长，MQ中消息的堆积量变大；若每次Pull的时间间隔较短，但是在一段时间内MQ中并没有任何消息可以消费，那么会产生很多无效的Pull请求的RPC开销，影响MQ整体的网络性能；","date":"2024-12-31","categories":["mq"]},{"title":"rocketmq架构设计","url":"/2024/12/31/rocketmq架构设计/","content":"","date":"2024-12-31","categories":["mq"]},{"title":"mysql-jdbc-数据库连接池druid配置详解","url":"/2024/12/31/mysql-jdbc-数据库连接池druid配置详解/","content":"数据库连接池druid配置详解 druid的配置项详解 配置 缺省值 说明 name 无 配置这个属性的意义在于，如果存在多个数据源，监控的时候可以通过名字来区分开来。如果没有配置，将会生成一个名字， 格式是：”DataSource-” + System.identityHashCode(this) dbcUrl 无 连接数据库的url，不同数据库不一样。例如：mysql : jdbc:mysql:&#x2F;&#x2F;10.20.153.104:3306&#x2F;druid2 oracle : jdbc:oracle:thin:@10.20.149.85:1521:ocnauto username 无 连接数据库的用户名 password 无 连接数据库的密码。如果你不希望密码直接写在配置文件中，可以使用ConfigFilter。详细看这里https://github.com/alibaba/druid/wiki/%E4%BD%BF%E7%94%A8ConfigFilter driverClassName 根据url自动识别 这一项可配可不配，如果不配置druid会根据url自动识别dbType，然后选择相应的driverClassName initialSize 0 初始化时建立物理连接的个数。初始化发生在显示调用init方法，或者第一次getConnection时 maxActive 8 最大连接池数量 maxIdle 8 已经不再使用，配置了也没效果 minIdle 无 最小连接池数量 maxWait 无 获取连接时最大等待时间，单位毫秒。配置了maxWait之后，缺省启用公平锁,并发效率会有所下降，如果需要可以通过配置useUnfairLock属性为true使用非公平锁。 poolPreparedStatements false 是否缓存preparedStatement，也就是PSCache。PSCache对支持游标的数据库性能提升巨大，比如说oracle。在mysql5.5以下的版本中没有PSCache功能，建议关闭掉。作者在5.5版本中使用PSCache，通过监控界面发现PSCache有缓存命中率记录，该应该是支持PSCache。 maxOpenPreparedStatements -1 要启用PSCache，必须配置大于0，当大于0时，poolPreparedStatements自动触发修改为true。在Druid中，不会存在Oracle下PSCache占用内存过多的问题， 可以把这个数值配置大一些，比如说100 validationQuery 用来检测连接是否有效的sql，要求是一个查询语句。 如果validationQuery为null，testOnBorrowtestOnReturn、testWhileIdle都不会其作用。 testOnBorrow true 申请连接时执行validationQuery检测连接是否有效，做了这个配置会降低性能。 testOnReturn false 归还连接时执行validationQuery检测连接是否有效，做了这个配置会降低性能 testWhileIdle false 建议配置为true，不影响性能，并且保证安全性。申请连接的时候检测，如果空闲时间大于timeBetweenEvictionRunsMillis，执行validationQuery检测连接是否有效。 timeBetweenEvictionRunsMillis 无 有两个含义： 1) Destroy线程会检测连接的间隔时间 2) testWhileIdle的判断依据，详细看testWhileIdle属性的说明 numTestsPerEvictionRun 无 不再使用，一个DruidDataSource只支持一个EvictionRun minEvictableIdleTimeMillis 无 一个连接在池中最小的生存时间，单位是毫秒 connectionInitSqls 无 物理连接初始化的时候执行的sql exceptionSorter 根据dbType自动识别 当数据库抛出一些不可恢复的异常时，抛弃连接 filters 无 属性类型是字符串，通过别名的方式配置扩展插件，常用的插件有：监控统计用的filter:stat,日志用的filter:log4j,防御sql注入的filter:wall proxyFilters 无 类型是List<com.alibaba.druid.filter.Filter>， 如果同时配置了filters和proxyFilters， 是组合关系，并非替换关系 配置示例在spring配置文件中加入如下的配置，进行数据库链接池的信息配置 dbconfig.properties 配置文件的配置 web 监控页面控制， 启动spring 之后 再运行 http://ip:port/projectName/druid/index.html.即可访问监控信息","date":"2024-12-31","categories":["mysql"]},{"title":"mysql-jdbc-驱动使用详解(1)基本使用方法","url":"/2024/12/31/mysql-jdbc-驱动使用详解-1-基本使用方法/","content":"java-jdbc驱动使用详解（一）基本使用方法大多数时候我都都是使用mybatis 或者数据库链接池这样的框架，渐渐对底层jdbc驱动缺少研究这里一下整理 驱动驱动加载 jvm在运行的时候，必须手动加载jdbc的驱动程序，因为jvm不会自动导入驱动，导致DriverManager无法使用 Connection 链接 建立和数据库的链接（java程序一般使用数据链接池，这里只是做一些基本的介绍） 链接方法：直接设置或者使用Property 创建方法 链接控制 链接状态检测 数据库交互 statementjdbc中有三种交互方法 Statement 使用通用访问数据库。当在运行时使用静态SQL语句。 Statement接口不能接受的参数。 PreparedStatement 当计划多次使用SQL语句。 那么可以PreparedStatement接口接收在运行时输入参数。 CallableStatement 当要访问数据库中的存储过程中使用。 CallableStatement对象的接口还可以接受运行时输入参数 statement对象，一般不使用，一般做心跳检测的时候使用 PreparedStatement — statement升级版可以使用占位符 和进行批处理操作 jdbc结果集操作 声明结果集的类型 在statement的生成方法可以进行很多的配置参数 Rstyoe RsConcurrency都是ResultSet中指定的类型 ResultSet的类型 RSType 可能的RSType如下，如果不指定ResultSet类型，将自动获得一个是TYPE_FORWARD_ONLY。 Type 描述: ResultSet.TYPE_FORWARD_ONLY 游标只能向前移动的结果集。 ResultSet.TYPE_SCROLL_INSENSITIVE 游标可以向前和向后滚动，结果集不是别人向创建结果集后发生的数据库更改敏感。 ResultSet.TYPE_SCROLL_SENSITIVE. 游标可以向前和向后滚动，结果集是别人向创建结果集后发生的数据库更改敏感。 并发性的ResultSe RSConcurrencyt 可能的RSConcurrency如下，如果不指定任何并发类型，将自动获得一个为CONCUR_READ_ONLY。 并发 描述: ResultSet.CONCUR_READ_ONLY 创建结果集只读。这是默认的 ResultSet.CONCUR_UPDATABLE 创建一个可更新的结果集。 结果集相关方法: 使用可滚动结果集相关属性 需要指定 Resultset相关配置 并发性（可修改结果集相关配置） 结果集通用方法","date":"2024-12-31","categories":["mysql"]},{"title":"mysql-jdbc-驱动使用详解(2)批处理性能优化","url":"/2024/12/31/mysql-jdbc-驱动使用详解-2-批处理性能优化/","content":"java-jdbc驱动使用详解（二）批处理性能优化 在使用mysql的时候或多或少会遇见大量的批操作的情况这里整理一下经验 这里我们对三种情况进行测试 性能优化的方法，在url请求中加上这些参数，useServerPrepStmts&#x3D;true&cachePrepStmts&#x3D;true&rewriteBatchedStatements&#x3D;true 组号 是否开起服务端预处理和预处理缓存 是否使用事务提交方法 是否使用批处理方法 1w（毫秒） 100w（毫秒） 第一组 N N N 添加 : 39619，修改 : 41440，删除 : 39734 大于一小时 第二组 Y N N 添加 : 38741，修改 : 41146，删除 : 39538 大于一小时 第三组 N Y N 添加 : 1145，修改 : 1321，删除 : 926 添加 : 80560，修改 : 122566，删除 : 92001 第四组 Y Y N 添加 : 674，修改 : 1114，删除 : 859 添加 : 76134，修改 : 116780，删除 : 88954 第五组 N N Y 添加 : 72，修改 : 38930，删除 : 37931 大于一小时 第六组 Y N Y 添加 : 112，修改 : 38409，删除 : 37677 大于一小时 第七组 N Y Y 添加 : 54，修改 : 770，删除 : 510 添加 : 8925，修改 : 79631，删除 : 46655 第八组 Y Y Y 添加 : 88，修改 : 729，删除 : 444 添加 : 9401，修改 : 78899，删除 : 47316 测试代码","date":"2024-12-31","categories":["mysql"]},{"title":"mysql-jdbc-驱动使用详解(3)单机分库分表下的性能测试","url":"/2024/12/31/mysql-jdbc-驱动使用详解-3-单机分库分表下的性能测试/","content":"java-jdbc驱动使用详解（三）单机分库分表下的性能测试上一篇进行过测试，批处理和事务提交结合，可以显著的提升数据库增删改操作的性能，但是有一个问题——在多表和多数据库情况下使用多线程编程性能会提升多少。为了解决这个问题进行了如下的测试 测试代码 测试样例和结果 组号 是否是同一个数据库 是否使用多个链接 是否使用批处理方法 1w（毫秒） 100w（毫秒） 第一组 Y N N 添加 :42306，修改 :43521，删除 :44073 大于一小时 第二组 N Y N 添加 : 34137，修改 : 34215，删除 : 33698 大于一小时 第三组 Y Y N 添加 : 34511，修改 : 34060，删除 : 33970 大于一小时 第四组 Y N Y 添加 : 81，修改 : 724，删除 : 454 添加 : 9504，修改 : 78451，删除 : 46229 第五组 N Y Y 添加 : 70，修改 : 424，删除 : 276 添加 : 8953，修改 : 69865，删除 : 40738 第六组 Y Y Y 添加 :200，修改 : 595，删除 : 289 添加 : 6119，修改 : 78899，删除 : 47316 总结（结合专题二和专题三）单库多链接在大数据量（100w）下性能优势明显，但是多库分表在新能区间上更优，分表相比教不分表性能更好","date":"2024-12-31","categories":["mysql"]},{"title":"mysql-jdbc-驱动使用详解(4)流式读取","url":"/2024/12/31/mysql-jdbc-驱动使用详解-4-流式读取/","content":"java-jdbc驱动使用详解（四）流式读取mysql驱动默认的行为是需要把整个结果全部读取到内存中才开始允许应用读取结果，执行ResultSet的next方法是阻塞的，期望的方式是有数据就让人他返回，所以这里开发mysql的流式读取 mysql判断是否开启流式读取结果的方法，有三个条件forward-only，read-only，fatch size是Integer.MIN_VALUE","date":"2024-12-31","categories":["mysql"]},{"title":"mysql-修改用户密码","url":"/2024/12/31/mysql-修改用户密码/","content":"mysql修改用户密码 注意有的时候还是无法使用密码登入mysql操作系统中，这个时候还需要处理user中的plugin参数，让这个参数变成mysql_native_password mysql 8.0 使用alter user ‘root‘@’localhost’ identified by ‘设置的新密码’ 修改密码 生效和刷新","date":"2024-12-31","categories":["mysql"]},{"title":"mysql-数据库创建新的用户","url":"/2024/12/31/mysql-数据库创建新的用户/","content":"创建用户:命令: 说明:username - 你将创建的用户名, host - 指定该用户在哪个主机上可以登陆,如果是本地用户可用localhost, 如果想让该用户可以从任意远程主机登陆,可以使用通配符%. password - 该用户的登陆密码,密码可以为空,如果为空则该用户可以不需要密码登陆服务器. 例子: 授权:命令: 说明: privileges - 用户的操作权限,如SELECT , INSERT , UPDATE 等(详细列表见该文最后面).如果要授予所的权限则使用ALL.;databasename - 数据库名,tablename-表名,如果要授予该用户对所有数据库和表的相应操作权限则可用表示, 如.*. 例子: 注意:用以上命令授权的用户不能给其它用户授权,如果想让该用户可以授权,用以下命令: 权限信息用user、db、host、tables_priv和columns_priv表被存储在MySQL数据库中(即在名为mysql的数据库中)。 权限 列 Context select Select_priv 表 insert Insert_priv 表 update Update_priv 表 delete Delete_priv 表 index Index_priv 表 alter Alter_priv 表 create Create_priv 数据库、表或索引 drop Drop_priv 数据库或表 grant Grant_priv 数据库或表 references References_priv 数据库或表 reload Reload_priv 服务器管理\\ shutdown Shutdown_priv 服务器管理 process Process_priv 服务器管理 file File_priv 在服务器上的文件存取 设置与更改用户密码命令: 例子: 撤销用户权限命令: 说明: privilege, databasename, tablename - 同授权部分.例子: REVOKE SELECT ON . FROM ‘pig‘@’%’;注意: 假如你在给用户‘pig‘@’%’授权的时候是这样的(或类似的):GRANT SELECT ON test.user TO ‘pig‘@’%’, 则在使用REVOKE SELECT ON . FROM ‘pig‘@’%’;命令并不能撤销该用户对test数据库中user表的SELECT 操作.相反,如果授权使用的是GRANT SELECT ON . TO ‘pig‘@’%’;则REVOKE SELECT ON test.user FROM ‘pig‘@’%’;命令也不能撤销该用户对test数据库中user表的Select 权限. 具体信息可以用命令SHOW GRANTS FOR ‘pig‘@’%’; 查看. 删除用户命令: 查看用户的授权 以上所述是小编给大家介绍的mysql5.7创建用户授权删除用户撤销授权，希望对大家有所帮助，如果大家有任何疑问请给我留言，小编会及时回复大家的。在此也非常感谢大家对脚本之家网站的支持！","date":"2024-12-31","categories":["mysql"]},{"title":"mysql-数据库常见问题总结","url":"/2024/12/31/mysql-数据库常见问题总结/","content":"mysql数据库常见问题总结主键 超键 候选键 外键 主 键：数据库表中对储存数据对象予以唯一和完整标识的数据列或属性的组合。一个数据列只能有一个主键，且主键的取值不能缺失，即不能为空值（Null）。 超 键：在关系中能唯一标识元组的属性集称为关系模式的超键。一个属性可以为作为一个超键，多个属性组合在一起也可以作为一个超键。超键包含候选键和主键。 候选键：是最小超键，即没有冗余元素的超键。 外 键：在一个表中存在的另一个表的主键称此表的外键。 数据库事务的四个特性及含义数据库事务transanction正确执行的四个基本要素。ACID,原子性(Atomicity)、一致性(Correspondence)、隔离性(Isolation)、持久性(Durability)。 原子性:整个事务中的所有操作，要么全部完成，要么全部不完成，不可能停滞在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。 一致性:在事务开始之前和事务结束以后，数据库的完整性约束没有被破坏。 隔离性:隔离状态执行事务，使它们好像是系统在给定时间内执行的唯一操作。如果有两个事务，运行在相同的时间内，执行 相同的功能，事务的隔离性将确保每一事务在系统中认为只有该事务在使用系统。这种属性有时称为串行化，为了防止事务操作间的混淆，必须串行化或序列化请 求，使得在同一时间仅有一个请求用于同一数据。 持久性:在事务完成以后，该事务所对数据库所作的更改便持久的保存在数据库之中，并不会被回滚。 视图的作用，视图可以更改么？视图是虚拟的表，与包含数据的表不一样，视图只包含使用时动态检索数据的查询；不包含任何列或数据。使用视图可以简化复杂的sql操作，隐藏具体的细节，保护数据；视图创建后，可以使用与表相同的方式利用它们。 视图不能被索引，也不能有关联的触发器或默认值，如果视图本身内有order by 则对视图再次order by将被覆盖。 创建视图 sql 语句 对于某些视图比如未使用联结子查询分组聚集函数Distinct Union等，是可以对其更新的，对视图的更新将对基表进行更新；但是视图主要用于简化检索，保护数据，并不用于更新，而且大部分视图都不可以更新。 索引的工作原理及其种类数据库索引，是数据库管理系统中一个排序的数据结构，以协助快速查询、更新数据库表中数据。索引的实现通常使用B树及其变种B+树。在数据之外，数据库系统还维护着满足特定查找算法的数据结构，这些数据结构以某种方式引用（指向）数据，这样就可以在这些数据结构上实现高级查找算法。这种数据结构，就是索引。为表设置索引要付出代价的：一是增加了数据库的存储空间，二是在插入和修改数据时要花费较多的时间(因为索引也要随之变动)。 图展示了一种可能的索引方式。左边是数据表，一共有两列七条记录，最左边的是数据记录的物理地址（注意逻辑上相邻的记录在磁盘上也并不是一定物理相邻的）。为了加快Col2的查找，可以维护一个右边所示的二叉查找树，每个节点分别包含索引键值和一个指向对应数据记录物理地址的指针，这样就可以运用二叉查找在O(log2n)的复杂度内获取到相应数据。 索引的优势、劣势、和最佳应用场景 索引是建立在数据库表中的某些列的上面。在创建索引的时候，应该考虑在哪些列上可以创建索引，在哪些列上不能创建索引。 一般来说，应该在这些列上创建索引： 在经常需要搜索的列上，可以加快搜索的速度； 在作为主键的列上，强制该列的唯一性和组织表中数据的排列结构； 在经常用在连接的列上，这些列主要是一些外键，可以加快连接的速度； 在经常需要根据范围进行搜索的列上创建索引，因为索引已经排序，其指定的范围是连续的； 在经常需要排序的列上创建索引，因为索引已经排序，这样查询可以利用索引的排序，加快排序查询时间；在经常使用在WHERE子句中的列上面创建索引，加快条件的判断速度。 对于有些列不应该创建索引。一般来说，不应该创建索引的的这些列具有下列特点: 对于那些在查询中很少使用或者参考的列不应该创建索引。这是因为，既然这些列很少使用到，因此有索引或者无索引，并不能提高查询速度。相反，由于增加了索引，反而降低了系统的维护速度和增大了空间需求。 对于那些只有很少数据值的列也不应该增加索引。这是因为，由于这些列的取值很少，例如人事表的性别列，在查询的结果中，结果集的数据行占了表中数据行的很大比例，即需要在表中搜索的数据行的比例很大。增加索引，并不能明显加快检索速度。 对于那些定义为text, image和bit数据类型的列不应该增加索引。这是因为，这些列的数据量要么相当大，要么取值很少。 当修改性能远远大于检索性能时，不应该创建索引。这是因为，修改性能和检索性能是互相矛盾的。当增加索引时，会提高检索性能，但是会降低修改性能。当减少索引时，会提高修改性能，降低检索性能。因此，当修改性能远远大于检索性能时，不应该创建索引。 索引的优势、劣势 优势： 创建索引可以大大提高系统的性能。 通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性。可以大大加快数据的检索速度，这也是创建索引的最主要的原因。 可以加速表和表之间的连接，特别是在实现数据的参考完整性方面特别有意义 在使用分组和排序子句进行数据检索时，同样可以显著减少查询中分组和排序的时间。 通过使用索引，可以在查询的过程中，使用优化隐藏器，提高系统的性能。 劣势： 创建索引和维护索引要耗费时间，这种时间随着数据量的增加而增加。 索引需要占物理空间，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如果要建立聚簇索引，那么需要的空间就会更大。 当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，这样就降低了数据的维护速度。 根据数据库的功能，可以在数据库设计器中创建三种索引：唯一索引、主键索引和聚集索引。 唯一索引：唯一索引是不允许其中任何两行具有相同索引值的索引。 主键索引：数据库表经常有一列或列组合，其值唯一标识表中的每一行。该列称为表的主键。 在数据库关系图中为表定义主键将自动创建主键索引，- 主键索引是唯一索引的特定类型。该索引要求主键中的每个值都唯一。当在查询中使用主键索引时，它还允许对数据的快速访问。 聚集索引 在聚集索引中，表中行的物理顺序与键值的逻辑（索引）顺序相同。一个表只能包含一个聚集索引。 链接查询种类使用外连接外连接包括左向外联接、右向外联接或完整外部联接 左连接：left join 或 left outer join 左向外联接的结果集包括 LEFT OUTER 子句中指定的左表的所有行，而不仅仅是联接列所匹配的行。如果左表的某行在右表中没有匹配行，则在相关联的结果集行中右表的所有选择列表列均为空值(null)。 右连接：right join 或 right outer join 右向外联接是左向外联接的反向联接。将返回右表的所有行。如果右表的某行在左表中没有匹配行，则将为左表返回空值。 完整外部联接:full join 或 full outer join 完整外部联接返回左表和右表中的所有行。当某行在另一个表中没有匹配行时，则另一个表的选择列表列包含空值。如果表之间有匹配行，则整个结果集行包含基表的数据值。 使用内链接内联接是用比较运算符比较要联接列的值的联接 内连接：join 或 inner join 注意这个方法和普通的from 多表查询没什么区别，内链接可以省略on和使用where 作为约束 交叉连接(完全)概念：没有 WHERE（或者on）子句的交叉联接将产生联接所涉及的表的笛卡尔积。第一个表的行数乘以第二个表的行数等于笛卡尔积结果集的大小。（table1和table2交叉连接产生3*3&#x3D;9条记录） 数据库范式定义 第一范式（1NF） 在任何一个关系数据库中，第一范式（1NF）是对关系模式的基本要求，不满足第一范式（1NF）的数据库就不是关系数据库。 所谓第一范式（1NF）是指数据库表的每一列都是不可分割的基本数据项，同一列中不能有多个值，即实体中的某个属性不能有多个值或者不能有重复的属性。如果出现重复的属性，就可能需要定义一个新的实体，新的实体由重复的属性构成，新实体与原实体之间为一对多关系。在第一范式（1NF）中表的每一行只包含一个实例的信息。简而言之，第一范式就是无重复的列。 第二范式（2NF） 第二范式（2NF）是在第一范式（1NF）的基础上建立起来的，即满足第二范式（2NF）必须先满足第一范式（1NF）。第二范式（2NF）要求数据库表中的每个实例或行必须可以被惟一地区分。为实现区分通常需要为表加上一个列，以存储各个实例的惟一标识。这个惟一属性列被称为主关键字或主键、主码。 第二范式（2NF）要求实体的属性完全依赖于主关键字。所谓完全依赖是指不能存在仅依赖主关键字一部分的属性，如果存在，那么这个属性和主关键字的这一部分应该分离出来形成一个新的实体，新实体与原实体之间是一对多的关系。为实现区分通常需要为表加上一个列，以存储各个实例的惟一标识。简而言之，第二范式就是非主属性非部分依赖于主关键字。 第三范式（3NF） 满足第三范式（3NF）必须先满足第二范式（2NF）。简而言之，第三范式（3NF）要求一个数据库表中不包含已在其它表中已包含的非主关键字信息。例如，存在一个部门信息表，其中每个部门有部门编号（dept_id）、部门名称、部门简介等信息。那么在员工信息表中列出部门编号后就不能再将部门名称、部门简介等与部门有关的信息再加入员工信息表中。如果不存在部门信息表，则根据第三范式（3NF）也应该构建它，否则就会有大量的数据冗余。简而言之，第三范式就是属性不依赖于其它非主属性。（我的理解是消除冗余）","date":"2024-12-31","categories":["mysql"]},{"title":"mysql-数据库开启远程登入权限","url":"/2024/12/31/mysql-数据库开启远程登入权限/","content":"mysql数据库开启远程登入权限注释掉在&#x2F;etc&#x2F;mysql&#x2F;mysql.conf.d&#x2F;mysqld.cnf（使用apt安装后的配置文件）里面的bind-address &#x3D; 127.0.0.1s 登入的mysql数据库，给需要的用户添加权限","date":"2024-12-31","categories":["mysql"]},{"title":"mysql-高效分页查询方法","url":"/2024/12/31/mysql-高效分页查询方法/","content":"分页查询在互联网应用中非常常见,但是我们真的把mysql分页查询的性能优化到极限了吗 所以这里总结一下mysql分页查询优化相关的问题","date":"2024-12-31","categories":["mysql"]},{"title":"mysql主从复制模型(master-slave)-replication模式","url":"/2024/12/31/mysql主从复制模型-master-slave--replication模式/","content":"Mysql 主从复制 replicationmysql集群方案有replication和PXC replication和PXC两种方案写入数据同步比较： replication采用异步复制，无法保证数据的一致。 PXC采用同步复制，事务在集群的所有节点要么同时提交，要么不提交，PXC使用的是percona，percona是mysql改进版，性能挺升很大 MySQL Replication主从复制（也称 AB 复制）允许将来自一个MySQL数据库服务器（主服务器）的数据复制到一个或多个MySQL数据库服务器（从服务器) 复制是异步的 从站不需要永久连接以接收来自主站的更新。 根据配置，您可以复制数据库中的所有数据库，所选数据库甚至选定的表。 MySQL主从复制的优点包括： 横向扩展解决方案 - 在多个从站之间分配负载以提高性能。在此环境中，所有写入和更新都必须在主服务器上进行。但是，读取可以在一个或多个从设备上进行。该模型可以提高写入性能（因为主设备专用于更新），同时显着提高了越来越多的从设备的读取速度。 数据安全性 - 因为数据被复制到从站，并且从站可以暂停复制过程，所以可以在从站上运行备份服务而不会破坏相应的主数据。 分析 - 可以在主服务器上创建实时数据，而信息分析可以在从服务器上进行，而不会影响主服务器的性能。 远程数据分发 - 您可以使用复制为远程站点创建数据的本地副本，而无需永久访问主服务器。 mysql Replication原理 前提是作为主服务器角色的数据库服务器必须开启二进制日志 过程: 主服务器上面的任何修改都会通过自己的 I&#x2F;O tread(I&#x2F;O 线程)保存在二进制日志 Binary log 里面。 从服务器上面也启动一个 I&#x2F;O thread，通过配置好的用户名和密码, 连接到主服务器上面请求读取二进制日志，然后把读取到的二进制日志写到本地的一个Realy log（中继日志）里面。 从服务器上面同时开启一个 SQL thread 定时检查 Realy log(这个文件也是二进制的)，如果发现有更新立即把更新的内容在本机的数据库上面执行一遍. 注意:每个从服务器都会收到主服务器二进制日志的全部内容的副本。从服务器设备负责决定应该执行二进制日志中的哪些语句。除非另行指定，否则主从二进制日志中的所有事件都在从站上执行。如果需要，您可以将从服务器配置为仅处理一些特定数据库或表的事件。 重要: 您无法将主服务器配置为仅记录特定事件。 一主多从配制方法如果一主多从的话，这时主库既要负责写又要负责为几个从库提供二进制日志。此时可以稍做调整，将二进制日志只给某一从，这一从再开启二进制日志并将自己的二进制日志再发给其它从。或者是干脆这个从不记录只负责将二进制日志转发给其它从，这样架构起来性能可能要好得多，而且数据之间的延时应该也稍微要好一些。工作原理图如下： 关于二进制日志mysqld将数字扩展名附加到二进制日志基本名称以生成二进制日志文件名。每次服务器创建新日志文件时，该数字都会增加，从而创建一系列有序的文件。每次启动或刷新日志时，服务器都会在系列中创建一个新文件。服务器还会在当前日志大小达到max_binlog_size参数设置的大小后自动创建新的二进制日志文件 。二进制日志文件可能会比max_binlog_size使用大型事务时更大， 因为事务是以一个部分写入文件，而不是在文件之间分割。 为了跟踪已使用的二进制日志文件， mysqld还创建了一个二进制日志索引文件，其中包含所有使用的二进制日志文件的名称。默认情况下，它具有与二进制日志文件相同的基本名称，并带有扩展名’.index’。在mysqld运行时，您不应手动编辑此文件。 术语二进制日志文件通常表示包含数据库事件的单个编号文件。 术语 二进制日志 表示含编号的二进制日志文件集加上索引文件。 SUPER 权限的用户可以使用SET sql_log_bin&#x3D;0语句禁用其当前环境下自己的语句的二进制日志记录 mysql主从复制配置Master-Server 配置修改 my.cnf配置 Master 以使用基于二进制日志文件位置的复制，必须启用二进制日志记录并建立唯一的服务器ID,否则则无法进行主从复制。 停止MySQL服务。 开启binlog ，每台设置不同的 server-id 启动MySQL服务 登录MySQL 创建用户每个从库使用MySQL用户名和密码连接到主库，因此主库上必须有用户帐户，从库可以连接。任何帐户都可以用于此操作，只要它已被授予 REPLICATION SLAVE权限。可以选择为每个从库创建不同的帐户，或者每个从库使用相同帐户连接到主库 虽然不必专门为复制创建帐户，但应注意，复制用到的用户名和密码会以纯文本格式存储在主信息存储库文件或表中 。因此，需要创建一个单独的帐户，该帐户只具有复制过程的权限，以尽可能减少对其他帐户的危害。 登录MySQL Slave-Server 配置修改 my.cnf停止MySQL服务。 如果要设置多个从库，则每个从库的server-id与主库和其他从库设置不同的唯一值。 启动MySQL服务 登录MySQL 配置主库通信 查看 Master-Server ， binlog File 文件名称和 Position值位置 并且记下来 要设置从库与主库进行通信，进行复制，使用必要的连接信息配置从库在从库上执行以下语句 将选项值替换为与系统相关的实际值 参数格式，请勿执行 MASTER_LOG_POS&#x3D;0 写成0 也是可以的 放在一行执行方便 启动从服务器复制线程 查看复制状态 检查主从复制通信状态 必须都是 Yes 如果不是原因主要有以下 4 个方面： 1、网络不通 2、密码不对 3、MASTER_LOG_POS 不对 ps 4、mysql 的 auto.cnf server-uuid 一样（可能你是复制的mysql） 测试主从复制启动MySQL服务 登录MySQL 在 Master-Server 创建测试库 在 Slave-Server 查看是否同步过来 一些命令 查看主服务器的运行状态 查看从服务器主机列表 获取binlog文件列表 只查看第一个binlog文件的内容 查看指定binlog文件的内容 启动从库复制线程 停止从库复制线程 复制实现细节分析MySQL主从复制功能使用三个线程实现，一个在主服务器上，两个在从服务器上 Binlog转储线程。当从服务器与主服务器连接时，主服务器会创建一个线程将二进制日志内容发送到从服务器。该线程可以使用 语句 SHOW PROCESSLIST(下面有示例介绍) 在服务器 sql 控制台输出中标识为Binlog Dump线程。 二进制日志转储线程获取服务器上二进制日志上的锁，用于读取要发送到从服务器的每个事件。一旦事件被读取，即使在将事件发送到从服务器之前，锁会被释放。 从服务器I&#x2F;O线程。当在从服务器sql 控制台发出 START SLAVE语句时，从服务器将创建一个I&#x2F;O线程，该线程连接到主服务器，并要求它发送记录在主服务器上的二进制更新日志。 从机I&#x2F;O线程读取主服务器Binlog Dump线程发送的更新 （参考上面 Binlog转储线程 介绍），并将它们复制到自己的本地文件二进制日志中。 该线程的状态显示详情 Slave_IO_running 在输出端 使用 命令SHOW SLAVE STATUS 使用\\G语句终结符,而不是分号,是为了，易读的垂直布局 这个命令在上面 查看从服务器状态 用到过 从服务器SQL线程。从服务器创建一条SQL线程来读取由主服务器I&#x2F;O线程写入的二级制日志，并执行其中包含的事件。 在前面的描述中，每个主&#x2F;从连接有三个线程。主服务器为每个当前连接的从服务器创建一个二进制日志转储线程，每个从服务器都有自己的I&#x2F;O和SQL线程。从服务器使用两个线程将读取更新与主服务器更新事件，并将其执行为独立任务。因此，如果语句执行缓慢，则读取语句的任务不会减慢。 例如，如果从服务器开始几分钟没有运行，或者即使SQL线程远远落后，它的I&#x2F;O线程也可以从主服务器建立连接时，快速获取所有二进制日志内容。 如果从服务器在SQL线程执行所有获取的语句之前停止，则I&#x2F;O线程至少获取已经读取到的内容，以便将语句的安全副本存储在自己的二级制日志文件中，准备下次执行主从服务器建立连接，继续同步。 使用命令 SHOW PROCESSLIST\\G 可以查看有关复制的信息 命令 SHOW FULL PROCESSLISTG 在 Master 主服务器 执行的数据示例 Id: 22是Binlog Dump服务连接的从站的复制线程Host: node2:39114 是从服务，主机名 级及端口State: 信息表示所有更新都已同步发送到从服务器，并且主服务器正在等待更多更新发生。如果Binlog Dump在主服务器上看不到 线程，意味着主从复制没有配置成功; 也就是说，没有从服务器连接主服务器。 命令 SHOW PROCESSLISTG 在 Slave 从服务器 ，查看两个线程的更新状态 Id: 6是与主服务器通信的I&#x2F;O线程Id: 7是正在处理存储在中继日志中的更新的SQL线程 在 运行 SHOW PROCESSLIST 命令时，两个线程都空闲，等待进一步更新 如果在主服务器上在设置的超时，时间内 Binlog Dump线程没有活动，则主服务器会和从服务器断开连接。超时取决于的 服务器系统变量 值 net_write_timeout(在中止写入之前等待块写入连接的秒数，默认10秒)和 net_retry_count;(如果通信端口上的读取或写入中断，请在重试次数，默认10次) 设置 服务器系统变量 该SHOW SLAVE STATUS语句提供了有关从服务器上复制处理的附加信息。请参见 第16.1.7.1节“检查复制状态”。 常见主从复制问题官方文档常见问题 描述msyql replication 机制的实现原理，如何在不停掉mysql主库的情况下，恢复数据不一致的slave的数据库节点？MySQL的复制（replication）是一个异步的复制，从一个MySQL instace（称之为Master）复制到另一个MySQL instance（称之Slave）。实现整个复制操作主要由三个进程完成的，其中两个进程在Slave（Sql进程和IO进程），另外一个进程在Master（IO进程）上。 引用新浪某位大牛的话：mysql复制就是一句话：基于binlog的单线程异步复制过程。MySQL Replication复制的基本过程如下：1、Slave上面的IO进程连接上Master，并请求从指定日志文件的指定位置（或者从最开始的日志）之后的日志内容； 代码如下: 2、Master接收到来自Slave的IO进程的请求后，通过负责复制的IO进程根据请求信息读取制定日志指定位置之后的日志信息，返回给Slave的IO进程。返回信息中除了日志所包含的信息之外，还包括本次返回的信息已经到Master端的bin-log文件的名称以及bin-log的位置； 3、Slave的IO进程接收到信息后，将接收到的日志内容依次添加到Slave端的relay-log文件的最末端，并将读取到的Master端的bin-log的文件名和位置记录到master-info文件中，以便在下一次读取的时候能够清楚的高速Master“我需要从某个bin-log的哪个位置开始往后的日志内容，请发给我”； 4、Slave的Sql进程检测到relay-log中新增加了内容后，会马上解析relay-log的内容成为在Master端真实执行时候的那些可执行的内容，并在自身执行 操作过程：（1）登陆主服务器，查看主服务器的状态mysql>show master status；找到现阶段master的数据偏移量的值。 （2）登陆从服务器，执行同步操作。mysql>stop slave;mysql > change master to 直接定位到这个值得位置； 这里也就相当于给slave指明了相应的位置。mysql > start slave; （3）从服务器上查看状态mysql > show slave status","date":"2024-12-31","categories":["mysql"]},{"title":"mysql数据库拆分-冷热库","url":"/2024/12/31/mysql数据库拆分-冷热库/","content":"概念冷热库其实是mysql数据库进行拆分的一种方法,核心思想是将热点数据和历史数据分离 拆分原则我们一般有两种方式来区分冷热数据： 按数据创建时间：一般情况我们会使用数据的创建时间来区分数据冷热，按常理说越久时间前创建的数据访问越少，这对于交易类数据很明显，很可能一周内某笔交易会被频繁查询，一个月内也许只是偶尔查询，三个月后可能在极偶然的情况下才会被用到，有可能绝大多数数据在几个月后再也不会被访问到。 按访问热度：有些数据访问频度并非按时间，比如某个热门文章也许发表一年之后还会有大量的人访问，新发表的文章反而很少有人浏览。或者某个事件会引发某些很少阅读的文章或者信息在某一个时间段突然大量的被访问到，这样冷数据也会变成热数据。这个时候再按时间区分就不科学了，需要按业务情况、数据规律来区分冷热。","date":"2024-12-31","categories":["mysql"]},{"title":"nginx-location映射地址","url":"/2024/12/31/nginx-location映射地址/","content":"nginx是一个非常棒的web代理服务器， 这里记录一下nginx 使用localtion字段，来进行访问控制的处理方法 location匹配顺序 “&#x3D;”前缀指令匹配，如果匹配成功，则停止其他匹配 普通字符串指令匹配，顺序是从长到短，匹配成功的location如果使用^~，则停止其他匹配（正则匹配） 正则表达式指令匹配，按照配置文件里的顺序，成功就停止其他匹配 如果第三步中有匹配成功，则使用该结果，否则使用第二步结果 注意点 匹配的顺序是先匹配普通字符串，然后再匹配正则表达式。另外普通字符串匹配顺序是根据配置中字符长度从长到短，也就是说使用普通字符串配置的location顺序是无关紧要的，反正最后nginx会根据配置的长短来进行匹配，但是需要注意的是正则表达式按照配置文件里的顺序测试。找到第一个比配的正则表达式将停止搜索。 一般情况下，匹配成功了普通字符串location后还会进行正则表达式location匹配。有两种方法改变这种行为，其一就是使用“&#x3D;”前缀，这时执行的是严格匹配，并且匹配成功后立即停止其他匹配，同时处理这个请求；另外一种就是使用“^~”前缀，如果把这个前缀用于一个常规字符串那么告诉nginx 如果路径匹配那么不测试正则表达式。 匹配模式及顺序 location &#x3D; &#x2F;uri &#x3D;开头表示精确匹配，只有完全匹配上才能生效。 location ^~ &#x2F;uri ^~ 开头对URL路径进行前缀匹配，并且在正则之前。 location ~ pattern ~开头表示区分大小写的正则匹配。 location ~* pattern ~*开头表示不区分大小写的正则匹配。 location &#x2F;uri 不带任何修饰符，也表示前缀匹配，但是在正则匹配之后。 location &#x2F; 通用匹配，任何未匹配到其它location的请求都会匹配到，相当于switch中的default。 location中的root 和aliasnginx的location中的root和alias 若按照上述配置的话，则访问&#x2F;img&#x2F;目录里面的文件时，ningx会自动去&#x2F;var&#x2F;www&#x2F;image&#x2F;目录找文件 若按照这种配置的话，则访问&#x2F;img&#x2F;目录下的文件时，nginx会去&#x2F;var&#x2F;www&#x2F;image&#x2F;img&#x2F;目录下找文件 举一个例子如果我想把127.0.0.1&#x2F;item 目录映射到 &#x2F;user&#x2F;share&#x2F;html路径下，我只需要这么配置皆可以了","date":"2024-12-31","categories":["nginx"]},{"title":"postgresql-学习记录(1)","url":"/2024/12/31/postgresql-学习记录-1-/","content":"postgresql学习记录（一）一.初探postgresqlPostgreSQL是一个功能强大的开源对象关系数据库管理系统(ORDBMS)。 用于安全地存储数据; 支持最佳做法，并允许在处理请求时检索它们。PostgreSQL(也称为Post-gress-Q-L)由PostgreSQL全球开发集团(全球志愿者团队)开发。 它不受任何公司或其他私人实体控制。 它是开源的，其源代码是免费提供的。 二.踩坑postgresql的安装单纯的安装postgresql是非常容易的打开官网按照教程就可以很容易的安装上去 在 &#x2F;etc&#x2F;apt&#x2F;sources.list.d&#x2F;pgdg.list 文件中添加如下的语句注意要选择适合自己系统的版本,具体要求看postgresql的官方网站 运行命令开始安装 三.为postgresql添加新的用户并且开启远程登入的权限postgresql和mysql有一些差别,要想开启远程登入权限需要修改两个文件 输入 vi postgresql.conf 修改pistgresql.conf文件 编辑其中的listen_addresses 字段 将原来的listen_addresses &#x3D; ‘localhost’修改为listen_addresses &#x3D; ‘*’ 修改整个文件变成如上的样子:说明一下,local 表示本地登入的权限使用peer认证(本机使用unix socket认证:注意在这种模式下,linux系统必须切换到相应的用户中才行,这样就不需要使用密码进行登入了),host外网使用的是md5认证(增加安全权限)具体为什么要看一下其他的东西:http://hi-kys.me/?p=361 针对安全性为postgresql添加一个新的用户在上面的过程中我们已将使用了peer认证方法声明了所有的用户,也就是说只要linux系统中拥有和postgresql相同名称的用户,那么将linux切换到这个用户中将可以不使用密码进行登入数据的操作第一次使用postgresql首先切换linux用户 直接登入到数据库中 以上两步可以使用一个命令解决 创建新的用户并设置密码 或者使用这个命令 引申:user和role的区别就是role没有登入权限而user拥有登入权限创建一个数据库并关联到当前的用户上 付给这个新用户全部的权限(这里只是做开发不考虑进行权限管理) 使用\\du命令查看一下 \\q退出 测试一下使用命令 或者使用图形工具进行连接登入成功证明配置正确","date":"2024-12-31","categories":["postgresql"]},{"title":"postgresql-学习记录(2)-postgresql用户权限解析","url":"/2024/12/31/postgresql-学习记录-2--postgresql用户权限解析/","content":"postgresql学习记录(二)-postgresql用户权限解析PostgreSQL支持的认证方法非常多，除了自身的密码认证以外，还支持很多其他认证服务。 详见 https://www.postgresql.org/docs/9.6/static/auth-methods.html这里主要介绍一下ident和peer相关的坑ident 认证，客户端和数据库建立TCP会话后（假设会话的连接信息是client_ip:12345 <-> db_ip:5432），数据库通过ident协议询问客户端所在IP地址的ident server (默认是113监听端口)，询问内容：使用client_ip:12345端口连接db_ip:5432的操作系统用户是谁？ 如图: 在获取到客户端的OS用户名之后，PostgreSQL会通过pg_hba.conf中配置的map名与pg_ident.conf中配置的映射关系，以及客户端提供的数据库用户名，判断是否允许登陆数据库。如图 这里的peer相关的东西解释了为什么在上一篇中说使用linux的同名用户可以登入postgresql数据库","date":"2024-12-31","categories":["postgresql"]},{"title":"redis-分布式锁-redis实现方法","url":"/2024/12/31/redis-分布式锁-redis实现方法/","content":"在分布式系统中，有的时候会发生多个节点争夺资源，或者一个任务只能由一个节点触发（比如分布式定时任务系统），在这个情况下我们就要用到分布式锁了，这里使用redis 实现分布式锁 分布式锁一般有三种实现方式： 数据库乐观锁； 基于Redis的分布式锁； 基于ZooKeeper的分布式锁。 这里参考网上的相关的资料，记录如何正确地实现Redis分布式锁。 互斥性。在任意时刻，只有一个客户端能持有锁。 不会发生死锁。即使有一个客户端在持有锁的期间崩溃而没有主动解锁，也能保证后续其他客户端能加锁。 具有容错性。只要大部分的Redis节点正常运行，客户端就可以加锁和解锁。 解铃还须系铃人。加锁和解锁必须是同一个客户端，客户端自己不能把别人加的锁给解了。 代码实现-正确锁代码 代码分析 这里主要使用了，redis的set命令，映射到jedis中就是set方法，我们加锁就一行代码：jedis.set(String key, String value, String nxxx, String expx, int time)，这个set()方法一共有五个形参： 第一个为key，我们使用key来当锁，因为key是唯一的。 第二个为value，我们传的是requestId，很多童鞋可能不明白，有key作为锁不就够了吗，为什么还要用到value？原因就是我们在上面讲到可靠性时，分布式锁要满足第四个条件解铃还须系铃人，通过给value赋值为requestId，我们就知道这把锁是哪个请求加的了，在解锁的时候就可以有依据。requestId可以使用UUID.randomUUID().toString()方法生成。 第三个为nxxx，这个参数我们填的是NX，意思是SET IF NOT EXIST，即当key不存在时，我们进行set操作；若key已经存在，则不做任何操作； 第四个为expx，这个参数我们传的是PX，意思是我们要给这个key加一个过期的设置，具体时间由第五个参数决定。 第五个为time，与第四个参数相呼应，代表key的过期时间。 这里重点就是使用了set方法，同时设置了NX模式（相当于setNX）和设置超时时间，保证了唯一性的问题 加锁错误例子分析 使用jedis.setnx()和jedis.expire()组合实现加锁 错误点是没有考虑到原子性的问题，当使用jedis.setNX(xxx,xxx)之后，如果服务器发生崩溃将会导致jedis.expire(xxx,xxxx)不再运行，将会导致死锁—–做系统一定要考虑到原子性的问题 使用value作为判断超时时间的依据 分析：问题在这里 1. 由于是客户端自己生成过期时间，所以需要强制要求分布式下每个客户端的时间必须同步。 2. 当锁过期的时候，如果多个客户端同时执行jedis.getSet()方法，那么虽然最终只有一个客户端可以加锁，但是这个客户端的锁的过期时间可能被其他客户端覆盖。3. 锁不具备拥有者标识，即任何客户端都可以解锁。 代码实现-正确解锁代码 分析：我们将Lua代码传到jedis.eval()方法里，并使参数KEYS[1]赋值为lockKey，ARGV[1]赋值为requestId。eval()方法是将Lua代码交给Redis服务端执行。源于Redis的特性，eval命令运行lua代码可以保证原子性，简单来说，就是在eval命令执行Lua代码的时候，Lua代码将被当成一个命令去执行，并且直到eval命令执行完成，Redis才会执行其他命令。 解锁错误例子分析 不判断谁能解锁-违反原则 不保证一原子性，可能造成A解了B的锁（比如A设置了超时时间，当运行到判断的时候A锁超时字段释放同时B加锁了，然后A运行进行解锁，解的就是B的锁） 总结：如果你的项目中Redis是多机部署的，那么可以尝试使用Redisson实现分布式锁，这是Redis官方提供的Java组件。在使用分布式锁的时候一定要注意之前说的4种特性","date":"2024-12-31","categories":["redis"]},{"title":"redis-命令-基础命令","url":"/2024/12/31/redis-命令-基础命令/","content":"append 指定key 的value 字段填充字符的方法 语法 append key add_value 这个方法时间复杂度o(1) 如果key 不存在的时候将会先创建key value 映射 value为”” 然后再执行append 逻辑(redis底层的实现可能并不是这样的只是为了方便记忆) bitcount key start end 默认情况下，将检查字符串中包含的所有字节。可以仅在通过附加参数start和end的间隔中指定计数操作。 与GETRANGE命令一样，start和end可以包含负值，以便从字符串末尾开始索引字节，其中-1是最后一个字节，-2是倒数第二个，依此类推。 不存在的键被视为空字符串，因此该命令将返回零。 BITFIELD key [GET type offset] [SET type offset value] [INCRBY type offset increment] [OVERFLOW WRAP|SAT|FAIL] BITOP operation destkey key [key …] BITPOS key bit [start] [end] BLPOP key [key …] timeout 阻塞出队列方法,这方法将会弹出key这个列表第一个非空值,如果key列表所有的都是空的,会将请求堵塞直到列表中存在数据,如果超过timeout时间将会返回nil 返回值是一个双元素体 1) 表示使用的列表 2) 表示的值 BRPOP key [key …] timeout 这个方法是BLPOP从列表尾部获取信息的变体","date":"2024-12-31","categories":["redis"]},{"title":"redis-命令-特殊命令","url":"/2024/12/31/redis-命令-特殊命令/","content":"auth password 请求在受密码保护的Redis服务器中进行身份验证。在允许客户端执行命令之前，可以指示Redis需要密码。这是使用requirepass配置文件中的指令完成的。如果password匹配配置文件中的密码，服务器将回复OK状态代码并开始接受命令。否则，将返回错误，客户端需要尝试新密码。 BGREWRITEAOF 指示Redis启动仅附加文件重写过程。重写将创建当前仅附加文件的小优化版本。如果BGREWRITEAOF失败，则不会丢失任何数据，因为旧的AOF将不受影响。如果还没有后台进程执行持久性，则重写将仅由Redis触发。特别：如果Redis子项正在磁盘上创建快照，则会调度 AOF重写， 但在生成RDB文件的保存子项终止之前不会启动。在这种情况下，BGREWRITEAOF仍会返回OK代码，但会显示相应的消息。您可以检查是否安排了AOF重写，从Redis 2.6开始查看INFO命令。如果AOF重写已在进行中，则该命令将返回错误，并且不会再安排AOF重写。从Redis 2.4开始，Redis会自动触发AOF重写，但 BGREWRITEAOF命令可以随时用于触发重写 BGSAVE 将DB保存在后台。立即返回OK代码。Redis分叉，父母继续为客户服务，孩子将数据库保存在磁盘上然后退出。客户端可以使用LASTSAVE 命令检查操作是否成功。","date":"2024-12-31","categories":["redis"]},{"title":"redis学习笔记1-对key的操作和key-value操作","url":"/2024/12/31/redis学习笔记1-对key的操作和key-value操作/","content":"对键的操作KEYS获得redis在这个实例的数据字典中所有的key列表 支持 glob风格的通配符定义 符号 含义 ? 匹配一个字符 * 匹配任意个字符包括0 [] 匹配括号间任意一个字符，可以使用-表示范围，比如[a-c] 表示匹配 a 或 b 或 c \\x 转译字符，比如要匹配？ 就需要使用? DELredis的del命令可以删除一批键，并且返回删除的数量 EXISTS判读一个key是否存在 如果存在就返回1反之返回0 TYPE返回这个键对应的类型返回的结果可以能是string,list,hash,set,zset key value 操作（value的默认类型就是string）SET和GET 相当于对map类型的数据进行取值 MGET和MSET同时获取或者设置多个 INCRincr是redis提供的一个特殊的key value结构 这个结构强制将value制定成integer类型，每次调用key的时候，将会自动的将value+1,并且返回+1 之后的结果 注意 incr命令可以是用get和set命令模拟，但是不能保证在分布式环境中的原子性问题，所以这种累加的方法要使用incr保证原子性 DECR和incr相反，让value-1 INCRBY相当于incr命令的进化版这个命令可以制定每次进行叠加的次数 DECRBY和INCRBY相反，将指定的value - 指定值 INCRBYFLOAT和incr意义相同，指定的key增加一个双精度浮点数 APPEND向键值末尾追加value，如果键不存在就将该键设置成value的值 注意如果插入的value拥有空格则需要使用””包裹 STRLEN获取字符串UTF-8编码后长度 二进制操作GETBIT和SETBITredis提供的位操作方法 redis 使用位操作的使用将会将value进行ascill编码，然后操作对应的二进制值 BITCOUNT返回指定key的value中1的个数（使用acill编码后） BITOP提供了AND OR XOR NOT 四种位逻辑操作运算符通过这些方法可以快速的对key的value进行位运算， 并将结果出存在一个新的key中","date":"2024-12-31","categories":["redis"]},{"title":"rust-学习笔记(1)-通用编程概念2","url":"/2024/12/31/rust-学习笔记-1--通用编程概念2/","content":"特殊的变量类型rust变量和其他的编程的变量系统是不同的,在rust中默认的变量都是不可改变的类型 如果让这个变量编程可变类型需要使用一种个关键字 mut mut 关键字能让这个变量变得可以修改 常量您使用const关键字而不是关键字声明常量let，并且值的类型必须带注释 Rust常量的命名约定是使用所有大写字母在单词之间加上下划线，并且可以在数字文字中插入下划线以提高可读性 rust的影子类型rust 支持影子模式，这种模式下，可以对统一个名称重复的进行赋值操作 rust 类型系统1. 基本类型和java相同（做java的时候很少考虑int 和long之间的区别，其实通过这种方法可以节省大量的内存） 2. 复合类型元祖类型用括号阔起来的类型 支持. 赋值和解析赋值 数组类型rust 的数组类型和java相同 创建数组 3种方法 rust的表达式和语句rust的语句没有返回值但是表达式是有的，调用函数是一个表达式。调用宏是一个表达式。作用域的块{}是一个表达式。 控制流（if else）也是表达式 比如这种赋值 输出 5 3 作用域中的变量不影响外部的 注意 x+1 如果有;号 就是语句，没有就是表达式 函数 注意rust不支持函数重载 控制流 if else 变量参数必须是boolean 类型 let if 判断表达式 因为if else 可以变成一个语句所有，可以是有不带分号的表达式来返回结果 注意： 这种方法需要保证所有的返回结果要相同 loop 相当于死循环，一般和break-可返回结果的break连用 while 和java相同 for 和python类似 rust的for循环不是很好用，类似python ，官方鼓励迭代器模式，貌似迭代器比for循环强大的多。。。。 This is an example link.","date":"2024-12-31","categories":["rust"]},{"title":"rust-学习笔记(2)-所有权","url":"/2024/12/31/rust-学习笔记-2--所有权/","content":"所有权是Rust的最独特功能，它使Rust无需垃圾收集器即可保证内存安全 所有权以及几个相关功能：借用，切片以及Rust如何在内存中布置数据 引申 栈和堆 所有权规则首先，让我们看一下所有权规则。在通过示例进行说明时，请牢记以下规则： Rust中的每个值都有一个变量，称为其所有者。一次只能有一个所有者。当所有者超出范围时，该值将被删除。 可变范围可以理解成java的作用域 变量回收rust和gc编程语言不同，他使用可变范围来控制变量的自动回收 但是有一个问题，比如这种代码 这是一个问题：当s2和s1走出去的范围，他们都将尝试释放相同的内存。这被称为双重释放错误，是我们前面提到的内存安全性错误之一。释放内存两次可能导致内存损坏，从而可能导致安全漏洞。 为了确保内存安全，在Rust中，在这种情况下会发生的事情还有一个细节。Rust认为s1不再有效，而不是尝试复制分配的内存 注意这个很重要，也就是说在rust中如果发生了已经拥有的变量再进行赋值，并且原来的值还是有效的那么一定是copy的值 rust对一些常用的对象在进行上面的操作的时候不需要调用copy方法，这些对象如下 所有整数类型，例如u32。 布尔类型，bool值true和false。 所有浮点类型，例如f64。 字符类型char。 元组（如果它们仅包含also的类型）Copy。例如， (i32, i32)是Copy，但(i32, String)不是。 所有权和职能这个很重要 用于将值传递给函数的语义类似于用于将值分配给变量的语义。就像赋值一样，将变量传递给函数将移动或复制。 函数传参语义 函数返回值语义 所有权在函数中存在的问题拥有所有权然后返回所有功能的所有权有点乏味。如果我们想让函数使用值而不是所有权，该怎么办？令人非常烦恼的是，除了我们可能还想返回的函数主体所产生的任何数据之外，如果我们想要再次使用它，则还需要将返回的信息传递回去。 可以使用元组返回多个值 不过rust提供了特殊的语义叫参考和借用来解决这个问题 参考和借用（引用和可变引用）突破所有权的限制修改方法 这种引用的方法其实是传一个引用地址，不过是只读的 如果需要可读，就需要用下面这种方法 注意如果这样写就会有错误 这个在一行来写就会有问题，因为一行的时候就是一个作用域，必须保证一致性 可变引用的限制但是可变引用有一个很大的限制：您只能在一个特定范围内对一个特定的数据进行一个可变引用 比如这种两种情况情况 如果将r1和r2 分别给两个不同的线程可能会出现征用问题，简单的说就是这样 两个或多个指针同时访问相同的数据。 至少有一个指针用于写入数据。 没有用于同步对数据访问的机制。 上面的例子2 如果写成这种就没有问题 注意问题就是在一个范围内不但使用了不可变引用，而且使用了可变引用 突破限制与往常一样，我们可以使用大括号创建新的范围，从而允许多个可变引用，而不是同时引用： 悬挂指针处理 由于s是在内部创建的dangle，当代码dangle完成时， s将被释放。但是我们试图返回对它的引用。这意味着此引用将指向无效String。 修改成这样就可以编译通过 另一个突破所有权的方法,切片切片其实本是上是连续数据的引用 比如一个 数组 0 1 2 3 4 5 6 其中变量x 指向 0 然后生成x的切片 y 指向 3 这个时候y就是x的一部分引用，注意所有的切片都是只读的不可以进行修改(类型是&str)，相当于给上面的arr开了一个口子可以看见数据 注意一个地方，如果代码这样写将会报错 原因时数据产生了脏数据，多个引用在使用前发生了变化 rust 数组切片","date":"2024-12-31","categories":["rust"]},{"title":"rust-学习笔记(3)-使用结构来构造相关数据","url":"/2024/12/31/rust-学习笔记-3--使用结构来构造相关数据/","content":"定义和实例化结构定义 注意rust中所有的结构体都默认被mut标记 函数初始化struct 使用函数的时候可以快速的参数名称和结构体快速 对应 使用结构更新语法从其他实例创建实例 其实本质上就是从别的struct体中获取相同的值，但是rust提供了一种快速同种类型转化方法 存在一个疑问点 这样写有问题吗，user2.name的值是啥 结构体方法方法和函数类似，但是方法与函数的不同之处在于，它们是在struc的上下文中定义的，并且它们的第一个参数始终为self，它表示调用该方法的struct实例。 在C和C ++中，使用两种不同的运算符来调用方法： .如果要直接在对象上调用方法，并且->要在指向对象的指针上调用方法，则需要先取消引用该指针。换句话说，如果object是指针， object->something()则类似于(*object).something()。 Rust没有与->运算符等效的对象。相反，Rust具有称为自动引用和取消引用的功能。调用方法是Rust少数具有这种行为的地方之一。 下面是它如何工作的：当你调用一个方法有object.something()锈会自动添加&，&mut或*使object该方法的签名相匹配。换句话说，以下是相同的： 第一个看起来更干净。这种自动引用行为行之有效，因为方法的接收方很明确-的类型self。给定方法的接收者和名称，Rust可以明确地确定该方法是读取（&self），变异（&mut self）还是使用（self）。Rust使方法接收者隐含借贷这一事实是在实践中使所有权符合人体工程学的重要组成部分。","date":"2024-12-31","categories":["rust"]},{"title":"rust-学习笔记(4)-枚举和模式匹配","url":"/2024/12/31/rust-学习笔记-4--枚举和模式匹配/","content":"rust的枚举类型其实是一种定义(java中更像一种对象可以储存数据) rust 中的枚举是不能给定默认值的,枚举的类型一般是通过和struct连用来进行使用的 枚举的定义和赋值 上面的列子,我们使用了结构体存储数据使用枚举来定义类型 rust提供了更加简单的方法来定义枚举- 枚举变量 枚举变量的类型支持rust 的枚举其实支持多种类型 例如，字符串，数字类型或结构。您甚至可以包含另一个枚举 rust 枚举定义通用方法rust的枚举和struct类似可以定义通用方法 rust的Option 枚举rust 实Option枚举来解决null问题的 标准库定义如下 该Option枚举是非常有用，它甚至包括中拉开序幕; 您无需将其明确纳入范围。此外，这样是它的变体：你可以使用Some和None直接不带Option::前缀。该 Option枚举仍然只是一个普通的枚举，并Some(T)和None类型仍然变种Option。 注意:如果使用None而不是Some，则需要告诉Rust Option我们拥有哪种类型 ，因为编译器无法Some 通过仅查看一个None值来推断该变量将拥有的类型。 枚举和rust match 连用Rust具有一个非常强大的控制流运算符match，该运算符使您可以将值与一系列模式进行比较，然后根据匹配的模式执行代码。模式可以由文字值，变量名，通配符和许多其他东西组成 注意 Coin枚举最后一个参数,使用 Quarter反解析可以获取state中定义的UsState枚举信息 match 语法和 Option连用假设我们要编写一个函数，该函数需要一个Option，如果内部有一个值，则将该值加1。如果内部没有值，该函数应返回该None值，而不要尝试执行任何操作。实用match进行操作 注意: rust对some的处理方法其实是一层语法糖,先反解析some中的内容然后进行其中表达式的操作 match _占位符 该_模式将匹配任何值。通过将其放在我们的其他手臂之后， _它将匹配之前未指定的所有可能情况。的() 只是单位值，所以什么都不会的发生_情况。结果，我们可以说，我们不希望对未在_占位符之前列出的所有可能值不执行任何操作。 解决match 在部分情况下的局限性的 if lef 语法比如 我们只想实用match 表达式在一个单一情况下做点什么其他情况什么都不做,那么rust需要这么去写 这种情况下我们需要写一个_ 来匹配其他情况 注意: rust 会 判断 match 可能匹配的所有情况,如果不满足情况,将会自动报错,这个时候我们需要 使用_ 来匹配所有的其他情况 if let 语法本质上其实和 if|else if|else 相同的","date":"2024-12-31","categories":["rust"]},{"title":"rust-学习笔记(5)-rust的包,模块,跳板箱","url":"/2024/12/31/rust-学习笔记-5--rust的包-模块-跳板箱/","content":"Rust具有许多功能，可让您管理代码的组织，包括哪些细节被公开，哪些细节是私有的以及程序中每个作用域中的名字。这些功能（有时统称为模块系统）包括： 包装：货运功能，可让您构建，测试和共享包装箱 板条箱：产生库或可执行文件的模块树 模块和用途：让您控制路径的组织，范围和隐私 路径：一种命名项目的方式，例如结构，函数或模块 包装和板条箱条板箱是二进制文件或库,一个软件包包含一个Cargo.toml文件，该文件描述了如何构建这些包装箱。包装箱是其它语言中库（library）或包（package）的同义词 板条箱会将范围内的相关功能分组在一起，因此该功能易于在多个项目之间共享 简单的讲解一些rust的包结构rust在规范中定义了两种类型 src&#x2F;main.rs 和 src&#x2F;lib.rs 如果路径中包含 src&#x2F;main.rs 的时候,将会生成一个执行的而二进制文件叫做包装 如果路径中包含 src&#x2F;lib.rs 的时候,将会形成一个可加载的包叫做跳板箱 如果既有 src&#x2F;lib.rs和src&#x2F;main.rs 的时候,将会生成两个包,一个包装一个跳板箱 一个简单的rust包目录 上面的其实是一个lib包的路径 如果根的cargo toml 名称叫做phrases,那么上面的模块层次结构如下 定义模块rust中可以使用mod 关键字定义一个模块, 比如 上面例子中的定义的mod其实形成了一个模块素 从一定的程度上讲rust的其实是一个可分级的模块系统 模块定义时路径和访问限制 上面例子中我从eat_at_restaurant() 方法中引用了front_of_house作用域下的hosting作用域的add_to_waitlist()方法,使用crate表示从项目跟路径的开始向上搜索的绝对路径,没有带上的表示使用以当前文件为跟路径的绝对路径 注意add_to_waitlist的pub 关键字, 如果没有这个关键字表示是私有的方法,只有自己和子类可以使用能","date":"2024-12-31","categories":["rust"]},{"title":"rust-学习记录(1)-other","url":"/2024/12/31/rust-学习记录-1--other/","content":"ps 这里只是简单的纪录一些rust相关的东西 变量1. 位置表达式和值表达式（左值和右值）、位置上下文和值上下文规则 ： 值表达式不能出现在位置上下文中 2. 不可变与可变 在 rust 中 所有变量都默认像java的final关键字，是不可改变的如果需要改变需要指定mut 参数,表示这个值是可以变的 3. 所有权和引用这个和c语言是类似的 所有权 > 直接赋值的时候将会自动的将对应的值转移到新的变量上来 rust 官方解释：在语义上，每个变量绑定都有该存储单元的所有权，这种转移内存地址的行为就是所有权的转移，在rust 中称为移动，不转移的情况下其实是一种复制语义 在日常的开发中，有时候并不需要转移所有权，rust 提供& 符号直接操作内存地址 上面的代码这样修改就可以通过了 ps ： 猜测 其实在这里发现一个问题，rust的字符串本身其实返回的就是一个内存地址，而””.to_string() 返回就是一个对象，move是针对对象的 函数rust的函数其实还是很标准的没啥特殊的语法 1. 函数定义 参数使用类型：名称分割，返回值写在-> 后面 2. 函数指针rust 的函数是一等公民，可以函数化传参或者作为返回值（注意函数参数定义必须是变量名：类型） 3. 函数闭包rust 闭包的特点 可以像函数一样被调用 可以捕获上下文环境中的自由变量 可以自动推断输入和返回的类型 1. 闭包传参 不知道为啥函数传参数函数名称必须带上类似范型的东西 2. 闭包返回值玄学了。。。。。 rust 的 CTFE机制rust 支持在编译期运行函数来进行求值操作 fn test3()&#123; ps 如果使用的rust2015 需要加上 rust流程控制1. if elserust 没有提供三元表达式运算符 2. where,for…in,looprust 提供了三种 3. matchrust的match 模式匹配相当的强大，匹配是在 rust 中很强的东西，可以匹配所有类型 &#x3D;> 后面跟的是表达式，不能是语句 (1) 基本形式类似这样 (2) match中可以应用解构match 中可以应用解构 ps : 同样可以作用于结构体，指针，枚举 (3) 可以加上if来守卫 (4) 绑定变量 if let 和 where let 表达式(1) if let 和 match 相同的地方是 (2) while let个人感觉while let 语法是为了简化如下的一种方法使用loop循环的一种方法 rust 变量系统(1) booleanrust 的boolean类型和其他编程语言中的boolean类型其实没有什么本质上的区别 rust 的 boolean 类型可以通过as 运算符强制转换成 int 类型的运算符号 (2) 数字类型rust 的数字类型其实是有规律可循的 有符号整型和无符号整型 u8-128 i8 - 128 有符号无符号动态整型 isize usize 32-64 位之间轮换 浮点类型支持单精度和双精度 0x 0o 0b 前缀分别表示十六进制\\八进制和\\二进制 (3) 符号类型rust 的符号类型是使用 ‘’ 来进行定义的 支持unicode 编码,占 4 个字节 char 类型可以使用as 进行操作, 当使用as的时候将会 自动转换成i8 类型 , 有些特殊字符可能会在高位截断 (4) rust 范围类型rust 支持一种特殊的类型叫做范围类型,提供两种方式表示范围 ,(1..2)左开右闭和(1..&#x3D;2)全闭 (5) 切片类型rust针对数组有一种特殊的类型叫做切片类型,这种类型可以将数组的一部分的索引引用出来 ###(6) String 类型 rust 提供了两种类型的字符串 str: 是字符串切片这个类型是静态类型,不可修改,固定长度 String : 是动态字符串类型,可修改长度 (7) rust特殊类型 元组 元组表示一群特殊的数据集合,可以表示为不同的类型 注意 元组可以实现函数多返回回值和使用let解构赋值 元组 rust 支持三种结构体 具体名结构体 注意: &self 方法表示的是这个结构体自己的引用 元组结构体 单元结构体","date":"2024-12-31","categories":["rust"]},{"title":"rust核心-所有权问题","url":"/2024/12/31/rust核心-所有权问题/","content":"在rust中，所有权有一下的几个规则 块级范围中，所有权向下传导 一个变量只能拥有他自己或者比自己小的作用域 ， 可以使用领域返回值扩展作用领域 使用表达式返回值即可 全局范围一个变量的所有权只能唯一持有 任何一个变量的所有权只能有一个变量持有 举一个 rust 移动的例子 函数范围传入参数和返回参数都可以转移所有权 当是函数的时候 ， 可以动态的转移对应的所有权，类似规则1 ， 调用函数的范围失去传入变量的所有权 使用引用和分片简化所有权转移上面的规则3 我们会发现所有权每次编写的时候都需要考虑进来 ， 这样就有很多准备工作去做 , rust提供了一个方法来简化所有权转移问题 引用传参可以简化所有权转移，保持调用者所有权不变 借阅和普通本质上上相同 ， 只是为了方便函数做所有权转移限制 这样会出错 rust 禁止多指针共享相同变量 ， 如果存在以最后一个为准 ,只针对可变引用 可变与不可变引用组合的情况下，要保证在不可变引用定义使用之间，不存在数据修改或者可变引用存在 注意一个地方就是 push_str 本身也是使用借阅 ， 对象内写法，等价于下面的这个逻辑 这里强制刷新了所有关于a的引用都是为最后操作了这个a的位置 rust 当函数有借阅并且返回值也是借阅的时候 ， 在rust中返回值必然为传入的借阅的子集，这样相同类型的借阅会被认为重新借阅 rust不能在函数中返回但前环境生成的借阅类型 ， 因为借阅在rust中是一个非常特殊的存在 ， 用来解决函数所有权转移的问题的，所以rust对借阅的范围方法是，对于借阅生效的作用领域范围只有最后一个借阅生效 ， rust只支持领域范围缩小 ， 不支持函数返回值导致领域范围扩大 举例子： rust只支持领域范围缩小 ， 不支持函数返回值导致领域范围扩大 看一个函数加引用的复杂例子 以上的b c d 在rust中都是认为和&mut a 一样的引用 因为函数和返回之值的类型相同 ，所以最后只能是 d rust 切片注意一个点就行了 切片类型不可修改 ， 从定义到使用之间元数组对应的切片范围不可变更 引申 > rust 对应的切片类型必须是& 开头的，因为切片本质上为元数组的一段引用，所有必须用指针","date":"2024-12-31","categories":["rust"]},{"title":"java-basev2","url":"/2024/12/31/java-basev2/","content":"js 对象 声明一个对象 针对属性类型修改 js 修改这些属性可以使用js的 Object。defineProperty方法进行修改 针对属性类型的读取 object 对象属性合并 es6 对象声明简写方法 属性名称复写 字符串（动态）属性直接赋值 快速函数命名 对象解构 原型链 继承 - 忽略 class 类 类声明 类一般都是初始化时候定义的 ， 不想函数可以在任意的位置定义，所以类需要在一开始就定义好 类构成 特殊地方， 获取表达式名称 js 类其实是一个特殊函数对象是绑定构造函数 – 继承？ 使用组合 忽略 js 函数js 的函数现在和一些基本语言的函数没啥区别 ， 主要要注意的地方那个 箭头函数 箭头函数需要和this 关键字一起说 没有箭头函数的时候 this是运行时确定的， 如果有箭头函数， this是声明时候定义的 函数内部的特殊对象 js支持这个的原因是js支持函数重命名 ， 重命名的函数递归会有问题 ， 所以一般内部使用这个方法 new target &#x3D;> 这个用来检测当前函数是不是使用new 方法创建对象了 js 函数 apply call bind 区别 call 、bind 、 apply 这三个函数的第一个参数都是 this 的指向对象，第二个参数差别就来了：call 的参数是直接放进去的，第二第三第 n 个参数全都用逗号分隔，直接放到后面 obj.myFun.call(db,’成都’, … ,’string’ )。apply 的所有参数都必须放在一个数组里面传进去 obj.myFun.apply(db,[‘成都’, …, ‘string’ ])。bind 除了返回是函数以外，它 的参数和 call 一样。 函数使用的时候需要特殊注意的地方 js尾调用优化例子-菲薄纳妾数列 匿名函数的作用域范围 匿名函数会继承父函数所有可以访问的变量（作用域） 匿名函数是永远没有办法访问到外部函数的this， this 一定是声明时候确认的 红包书 10.16 再支持","date":"2024-12-31","categories":["前端"],"tags":["javascript"]},{"title":"javascript-ECMAScript6使用和研究-let和count","url":"/2024/12/31/javascript-ECMAScript6使用和研究-let和count/","content":"一.let关键字特性(1)let关键字的闭包特性 列子一 输出: 这个例子可以体现给出let具有的以下几个特性 自闭包性:从使用var和使用let输出的差异性可以看出来,for循环(或者while循环)使用let的时候值就是自己循环时候的值而不是最终值 块级作用于独占性:let只有在自己的作用于才有效,上面的ReferenceError报错可以体现出来,注意:for循环中设置变量的时候也是一个单独的作用域(有坑点见下面的例子) 例子二 输出 第二个for循环只输出 2 是因为 let虽然提供了闭包的特性但是没有在本质上改变js的特性,是有循环2在到达时间的时候候读取的是自己块级作用域中的let元素的值 例子三 这个例子可以体现给出let的以下几个特性: let和var不同:let使用必须先进行声明否则会报错,var会输出underfind let和var不同:let不允许和var或者let变量在相同的作用域下同名 例子四暂时性死区 输出 这个例子可以体现给出let的以下特性:let属性区域性独立,外部不会影响到内部,大多数变成语言都这个特性 这个例子可以体现给出let的以下特性:TDZ 在变量声明前都会不能使用 只要出现生命前使用了变量都会出现TDZ错误 坑点记录 二.const关键字这个关键字的左右就是常量,但是他是锁定变量的地址的,其余的和let相同 注意:因为是锁定地址的所以需要进行对象的锁定是不安全的(对象中也有指向地址的东东) 如果要实现锁对象那么就使用","date":"2024-12-31","categories":["前端"],"tags":["javascript"]},{"title":"javascript-ECMAScript6使用和研究-函数","url":"/2024/12/31/javascript-ECMAScript6使用和研究-函数/","content":"ECMAScript6使用和研究-函数 注意:参数变量是默认声明的，所以不能用let或const再次声明。使用参数默认值时，函数不能有同名参数。 注意:函数的表达式中可以添加任意的表达式 默认参数可以使用underfind补位 函数参数和作用域的坑点 利用参数默认值，可以指定某一个参数不得省略，如果省略就抛出一个错误。 可以将参数默认值设为undefined，表明这个参数是可以省略的。 ES6中函数的作用域同样满足相关的私有化的特性但是一定要记住,ES6在这方面和ES5的区别本质上是,Es6使用的是定义的位置而ES5使用给的运行的位置 ES6使用默认参数还可以抛出异常 ES6升级版箭头函数 上面的箭头函数等同于 如果箭头函数不需要参数或需要多个参数，就使用一个圆括号代表参数部分。 注意如果像直接返回对象需要在对象外面套上() 箭头函数可以与变量解构结合使用。 箭头函数有几个使用注意点。 函数体内的this对象，就是定义时所在的对象，而不是使用时所在的对象。 不可以当作构造函数，也就是说，不可以使用new命令，否则会抛出一个错误。 不可以使用arguments对象，该对象在函数体内不存在。如果要用，可以用 rest 参数代替。 不可以使用yield命令，因此箭头函数不能用作 Generator 函数。 this指向的固定化，并不是因为箭头函数内部有绑定this的机制，实际原因是箭头函数根本没有自己的this，导致内部的this就是外层代码块的this。正是因为它没有this，所以也就不能用作构造函数。相当一下面这种写法 除了this，以下三个变量在箭头函数之中也是不存在的，指向外层函数的对应变量：arguments、super、new.target。 双冒号运算符箭头函数可以绑定this对象，大大减少了显式绑定this对象的写法（call、apply、bind）。但是，箭头函数并不适用于所有场合，所以现在有一个提案，提出了“函数绑定”（function bind）运算符，用来取代call、apply、bind调用。 函数绑定运算符是并排的两个冒号（::），双冒号左边是一个对象，右边是一个函数。该运算符会自动将左边的对象，作为上下文环境（即this对象），绑定到右边的函数上面。 如果双冒号左边为空，右边是一个对象的方法，则等于将该方法绑定在该对象上面。","date":"2024-12-31","categories":["前端"],"tags":["javascript"]},{"title":"javascript-ECMAScript6使用和研究-变量的解构赋值","url":"/2024/12/31/javascript-ECMAScript6使用和研究-变量的解构赋值/","content":"变量结构赋值在python中其实是非常常见的技术,这里整理一下ES6的相关变量结构赋值 一.针对数组的解构方法(1)使用简单的实例 输出结果： 引申：还有一中特殊的情况是实现了 Iterator 接口 这个之后再进行讨论 使用默认值 注意：ES6 内部使用严格相等运算符（&#x3D;&#x3D;&#x3D;），判断一个位置是否有值。所以，只有当一个数组成员严格等于undefined，默认值才会生效。 (2)针对对象 例子 输出 总体上就是这两种形式,其实也是将一些变量添加到空间中而已,只是使用对象需要指定映射关系罢了,深入理解见下 对象的组合赋值可以引用外部参数的 也可以使用默认值,默认值生效的条件是，对象的属性值严格等于undefined。 (3)字符串的解构赋值 还支持length (4)函数参数的解构赋值 函数的参数也可以使用解构赋值。 函数参数的解构也可以使用默认值。 上面代码是为函数move的参数指定默认值，而不是为变量x和y指定默认值，所以会得到与前一种写法不同的结果。 二.总结-用途（1）交换变量的值 上面代码交换变量x和y的值，这样的写法不仅简洁，而且易读，语义非常清晰。 （2）从函数返回多个值 函数只能返回一个值，如果要返回多个值，只能将它们放在数组或对象里返回。有了解构赋值，取出这些值就非常方便。 （3）函数参数的定义 解构赋值可以方便地将一组参数与变量名对应起来。 （4）提取 JSON 数据 解构赋值对提取 JSON 对象中的数据，尤其有用。 上面代码可以快速提取 JSON 数据的值。 （5）函数参数的默认值 指定参数的默认值，就避免了在函数体内部再写var foo &#x3D; config.foo || ‘default foo’;这样的语句。 （6）遍历 Map 结构 任何部署了 Iterator 接口的对象，都可以用for…of循环遍历。Map 结构原生支持 Iterator 接口，配合变量的解构赋值，获取键名和键值就非常方便。 如果只想获取键名，或者只想获取键值，可以写成下面这样。 （7）输入模块的指定方法 加载模块时，往往需要指定输入哪些方法。解构赋值使得输入语句非常清晰。","date":"2024-12-31","categories":["前端"],"tags":["javascript"]},{"title":"javascript-ECMAScript6使用和研究-块级作用域细节","url":"/2024/12/31/javascript-ECMAScript6使用和研究-块级作用域细节/","content":"块级作用域 ES5 规定，函数只能在顶层作用域和函数作用域之中声明，不能在块级作用域声明。 上面两种函数声明，根据 ES5 的规定都是非法的。 但是，浏览器没有遵守这个规定，为了兼容以前的旧代码，还是支持在块级作用域之中声明函数，因此上面两种情况实际都能运行，不会报错。 ES6 引入了块级作用域，明确允许在块级作用域之中声明函数。ES6 规定，块级作用域之中，函数声明语句的行为类似于let，在块级作用域之外不可引用。 上面代码在 ES5 中运行，会得到“I am inside!”，因为在if内声明的函数f会被提升到函数头部，实际运行的代码如下。 ES6 就完全不一样了，理论上会得到“I am outside!”。因为块级作用域内声明的函数类似于let，对作用域之外没有影响。但是，如果你真的在 ES6 浏览器中运行一下上面的代码，是会报错的，这是为什么呢？ 原来，如果改变了块级作用域内声明的函数的处理规则，显然会对老代码产生很大影响。为了减轻因此产生的不兼容问题，ES6 在附录 B里面规定，浏览器的实现可以不遵守上面的规定，有自己的行为方式。 允许在块级作用域内声明函数。函数声明类似于var，即会提升到全局作用域或函数作用域的头部。 同时，函数声明还会提升到所在的块级作用域的头部。注意，上面三条规则只对 ES6 的浏览器实现有效，其他环境的实现不用遵守，还是将块级作用域的函数声明当作let处理。 根据这三条规则，在浏览器的 ES6 环境中，块级作用域内声明的函数，行为类似于var声明的变量。 上面的代码在符合 ES6 的浏览器中，都会报错，因为实际运行的是下面的代码。 考虑到环境导致的行为差异太大，应该避免在块级作用域内声明函数。如果确实需要，也应该写成函数表达式，而不是函数声明语句。 另外，还有一个需要注意的地方。ES6 的块级作用域允许声明函数的规则，只在使用大括号的情况下成立，如果没有使用大括号，就会报错。","date":"2024-12-31","categories":["前端"],"tags":["javascript"]},{"title":"javascript-ES6-ES7-promise","url":"/2024/12/31/javascript-ES6-ES7-promise/","content":"promise是javascript异步非阻塞框架","date":"2024-12-31","categories":["前端"],"tags":["javascript"]},{"title":"javascript-element-ui学习笔(一)","url":"/2024/12/31/javascript-element-ui学习笔-一-/","content":"","date":"2024-12-31","categories":["前端"],"tags":["javascript"]},{"title":"javascript-vue学习笔记(一)","url":"/2024/12/31/javascript-vue学习笔记-一-/","content":"考虑到要提高工作的效率，通过调查发现使用vue可以提高前端进行编写时候的效率，所以准备花费两天的时间进行vue的element框架的学习和使用 声明式渲染其实这个很好理解就是指定了变量让框架自动的对相关的参数附上指定的值 条件与循环vue 通过使用v-if v-for 标签实现了循环操作 v-for 好理解类似java 中的for each 表单数据动态绑定这个特性针对 input textarea 的使用v-model 进行数据绑定 v-on 可以绑定在对象中定义的属性 组件化应用构建","date":"2024-12-31","categories":["前端"],"tags":["javascript"]},{"title":"javascript-从js来聊聊异步编程","url":"/2024/12/31/javascript-从js来聊聊异步编程/","content":"从js来聊聊异步编程文章的目的揭开go的 gorouter,c#的 async&#x2F;await等 使用同步的写法写异步代码的神秘面纱 , 证明其本质就是一个语法糖 为什么使用js来讲异步编程因为js可以通过编程语言自己的语法特性,实现原生语言不提供的同步化异步编程 js异步最底层写法promise promise出入的回调函数有一定的要求 resolve函数的作用是，将Promise对象的状态从“未完成”变为“成功”（即从 pending 变为 resolved），在异步操作成功时调用，并将异步操作的结果，作为参数传递出去 reject函数的作用是，将Promise对象的状态从“未完成”变为“失败”（即从 pending 变为 rejected），在异步操作失败时调用，并将异步操作报出的错误，作为参数传递出去。 Promise实例生成以后，可以用then方法分别指定resolved状态和rejected状态的回调函数(处理返回的结果)。 引申-注意: promise对象在js中非常特殊,比如下面的例子 这个的结果是failt 因为 p2中resolve返回一个promise对象,这个操作将会导致p2的状态升级成p1的状态(标准) promise的then链式写法promise then方法将会返回一个promise,所以js支持链式异步 promise 异常捕获 这个异常捕获和java相同,捕获在eventLoop中产生的异常 注意一点这个异常和java的try catch是不同的,如果产生了异常将不会在主线程中显示出来 promise的finally这个和java的异常体系相同,finally 无关状态,最后都会执行 更加方便的编写异步使用Promise.resolve(xxx) 注意: promise异步化结果只能在回调函数中获得,如果异步的操作太多,将会调至调用链路过长 如何解决js的promise异步编程的问题?promise 写法有什么问题? —- 调用链路过长 比如: 使用promise 实现 异步ajax请求 调用链太长,不停的promise调用 js如何解决回调地狱—同步方法写异步解决方法 使用js的协程 –Generator generator:js的特殊语法,使用yield 关键字将函数分块了,然后可以使用遍历器手动控制执行 例子: 本质上是函数分片 js在每次yield的时候都会获得当前位置的表达式,然后再手动的嵌入就可以实现分片控制的效果了 怎么用generator实现异步化呢 – yield配合promise实现异步看一下这个方法 想让他能异步执行,只要能让前一个promise的结果是下一个promise的输入就可以了 这里有两种写法 写法一递归方程: f(最终结果) &#x3D; f(到目前的结果)+f(接下来执行的结果) co 工具包的写法 写法二递归方程 f(最终结果) &#x3D; f(之前所有的结果)+f(最后一步的结果) 更简单的方法 async&#x2F;await上面复杂的代码如果变成async&#x2F;await要怎么做呢 很简单 通过上面的例子,我们可以发现其实async&#x2F;await本质上其实是 generator的一个语法糖 await就是yield , async 的作用就是将函数编程语法糖 如果背的话很简答两条规则: await后面必须是promise函数 async 标记过得函数执行后返回的promise 通过这种方法就可以简单的实现异步了","date":"2024-12-31","categories":["前端"],"tags":["javascript"]},{"title":"思路","url":"/2024/12/31/思路/","content":"设计页面的层级结构 - 虚拟dom树 设计页面的数据流转结构 - 运行态","date":"2024-12-31","categories":["前端"],"tags":["javascript"]},{"title":"nodejs","url":"/2024/12/31/nodejs/","content":"脚本化运行nodejs 安装nodejs ， 然后编写对应的nodejs 脚本 退出 这个方法会抛出一个错误码 0 正常 1 nodejs 环境初始化 检测环境信息 webAssembly npm 包管理 package.json 是核心 初始化自定义package.json (.npm-init.js) “dependencies”：您的应用程序在生产中所需的包。 “devDependencies”：只需要本地开发和测试的包。 npm 包安装 本地安装（默认）：将内容放入.&#x2F;node_modules当前包根目录。全局安装（带-g）：将东西放在 &#x2F;usr&#x2F;local 或安装节点的任何位置。如果你要去的话，在本地安装require()它。如果要在命令行上运行它，请全局安装它。如果两者都需要，则将其安装在两个位置，或使用npm link. npm 包格式支持 ， 本地加载 ， 版本控制 ， github 控制 常用命令","date":"2024-12-31","categories":["前端"],"tags":["nodejs"]},{"title":"如何做一次完美的压测","url":"/2024/12/31/如何做一次完美的压测/","content":"压力测试其实有的时候更考验人的经验 与测试相关的指标 qps : 每秒查询数 -> 这个指标一般用在数据库上 , 不过很多人都把这个和TPS混淆,这个知道怎么会是就行了 tps : 每秒内的事务数 -> 执行多组操作的性能 -> 也是数据库的一个指标 , 不过呢我们可以把我们的后台接口操作想象成一个事务 , 所以这个指标是标准的 pv : 只的是调用次数 RT : 接口的响应时间 cpu使用率 : 大家都懂 cpu loading指数 : 这个指标比较重要 , 能反映出cpu当前的”疲劳”程度 , 当前cpu处理任务数量 , 一般最大都要保证内核数量*1.2一下 内存 : 大家都懂 出向和入向流量 : 大家都懂 还有两个词 , 低延迟和高吞吐 如何获得指标使用常用的压测工具都可以 , jmeter等等 , 这类操作工具都会将qps 或者 tps , RT 整理统计出来 , 这里就不展开了 如何真正的把机器的性能压出来,做一次完美的压测 去压测性能的真的是一个技术活 , 是一个慢慢试出来的过程 首先我们要做性能预估 面对一个集群 , 性能如何应该怎么预估呢? 没别的办法 , 看经验 , 靠猜 但是也可以使用一些简单的办法 , 比如 由小道大 , 逐级测试 -> 用一个小的压力做初次预估 , 慢慢逼近性能极限 参照物法 , 逐渐逼近 -> 一组内公司内类似规模的系统 , 进行初次预估 玄学瞎猜 , 慢慢测试 -> 没有任何可以参考的就瞎猜一个吧 , 然后慢慢去寻找 制定压力层级和对应的指标 在压测过程中性能(QPS&#x2F;TPS)的随着压力的变化曲线应该是一个波浪状的 找出了预估的并发大小之后(一般是一个5的倍数的并发数) , 以这个为基准 , 增加多少并发 , 减少多少并发 , 列成表格 , 注意记录各种性能的指标 比如 : 接口 并发数量 压测时间 tps&#x2F;qps pv RT(min) RT(max) RT(average) RT(median) cpu loading cpu 使用率 goods_list 10 60s 3000 1w 1ms 30ms 15ms 20ms 6 100% ps : 这里就有一统计学的东西了 , 比如中位数和平均值 ,其实在压测的时候中位数更能体现出RT这个指标的实际效果 注意 : 这个得出的性能(qps&#x2F;tps)信息最后至少有一个下降的维度 , 是一个波浪状的效果 , 这样才能找到性能的极限 注意 : 最终性能要使用有效数据 , 如果RT 超过限制或者cpu loading 超过了限制 , 那么这个数据认为不达标 , 但是这个可以用作统计分析 性能瓶颈定位 影响性能的原因有多种 , 不一定真的是系统本身的问题 通过上面的制定压力层级和对应的指标 , 我们已经可以确定了我们当前系统的极限性能了 , 我们不禁想问这些真的是我们的极限性能了吗? 其实我们通过 cpu loading qps tps RT PV 等其实已经能确定当前场景下的一个最佳性能了 , 接写来就需要进行性能问题排查调优 瓶颈排查: 排查日志 , 找到耗时过长逻辑 , 定位分析 , 比如java 系的系统可以使用 alibaba 开源的在线性能分析工具 arthas 排查网络问题 : 当确认代码逻辑没有问题的时候 , 网络延迟过高也会影响qps , 比如跨机房调用等 . 这个时候需要找运维同学协助解决 , 做部署迁移 , 搭建专用线路等方法 最后 cpu瓶颈 : 这个时候只能通过加机器,升级机器硬件能力解决 性能调优 这个步骤就很多了, 要针对自己的技术栈进行操作 , 比如 java 可修改jvm参数 , 看一下 jvm GC , 内存状态 , 分别对应的进行一下调优 . ps java事情真多… … 性能瓶颈定位-其他方面qps上不去问题可能并不是后台的事情 , 不同类型的系统间的相互调用也可能会导致性能问题 随着现在互联网架构的升级 epoll 模式的广泛应用 ,系统之间的调用也可能成为性能的瓶颈 举个例子 传统java线程体系是一个典型的低延迟系统 , 因为线程模型是独立执行的不会正常理想情况下不会中途终止执行(线程上下文切换) , 所以说一个任务执行完成的时间就是这个任务执行在cpu中的最短时间. golang&#x2F;nodejs是高吞吐系统 , 应为底层用了epoll 模型使用事件轮询 , 其实本质上是将多个任务分配到一个线程中去执行 , 也就是说一个任务执行完的时间近似等于这一个线程所有任务完成的平均时间 . 上面介绍完了 -> 引出重点 , 上面的两个系统,如果java去调用golang或者nodejs的等类似的系统的话 , java就会出现性能问题 因为java 在线程模型下 , 要能保证高的性能必须保证延迟要足够低 , 否则会导致线程过多,线程上下文切换频繁 , 结果拉低整个性能. 但是golang&#x2F;nodejs 只是保证的吞吐 qps 没有保证延迟RT 所以 , 会导致底层服务性能很好吞吐量高但是上层服务怎么优化都没办法实现高并发. 如何解决 , java 抛弃线程模型使用NIO , 应为针对会联网应用在延迟可控的情况下应该尽可能的追求吞吐量 , 但是java NIO模型需要写异步回调 , 代码很编写这会导致开发效率降低 . 权衡下来 , 只能java增加机器来解决了. 性能瓶颈定位-压测的时候压力机器也要注意 , 有的时候瓶颈可能是压力机器的如果我们只是使用一台压力机器进行压测是片面的 , 因为网络的开销\\延迟 ,都能影响到压力机器对测试机器的负载 , 也就是说压力机到了上限但是测试机还没有道 解决方法 , 使用集群进行压力测试 最后这里只是记录了单个业务的压测方法 , 其实现在架构中还有针对整个调用链路的压测 全链路压测 其实是这种单业务压测的范化场景 , 需要调动各个部门 , 各个系统的开发人员一起做性能压测 , 技术上的操作和单业务压测是类似的 , 只是多了一些调用链路性能指标 , 各个系统之间调用的性能记录 , 还有一些故障演练 . 其实全链路更考验我们的组织能力,团队协作和沟通交流能力.","date":"2024-12-31","categories":["团队开发工作等其他软技能"]},{"title":"Devops运维遇到的一个基础问题-linux终止指令","url":"/2024/12/31/Devops运维遇到的一个基础问题-linux终止指令/","content":"其实这个东西是linux的一个信号 linux 结束任务有三种信号量 sigint sigterm sigkill sigint : 和ctrl+c关联 , 用来终止当前终端中正在运行的任务 sigterm : 相当于kill进程 , 会终止当前终端的后台任务(在当前终端的守护线程中的任务) sigkill : 相当于kill -9 , 不会和sigterm信号量会等待当前线程结束 , 而是直接的杀死它 linux 程序运行时状态切换一、&加在一个命令的最后，可以把这个命令放到后台执行，如watch -n 10 sh test.sh & #每10s在后台执行一次test.sh脚本 二、ctrl + z可以将一个正在前台执行的命令放到后台，并且处于暂停状态。 三、jobs查看当前有多少在后台运行的命令jobs -l选项可显示所有任务的PID，jobs的状态可以是running, stopped, Terminated。但是如果任务被终止了（kill），shell 从当前的shell环境已知的列表中删除任务的进程标识。 四、fg将后台中的命令调至前台继续运行。如果后台中有多个命令，可以用fg %jobnumber（是命令编号，不是进程号）将选中的命令调出。 五、bg将一个在后台暂停的命令，变成在后台继续执行。如果后台中有多个命令，可以用bg %jobnumber将选中的命令调出。 六、kill法子1：通过jobs命令查看job号（假设为num），然后执行kill %num法子2：通过ps命令查看job的进程号（PID，假设为pid），然后执行kill pid前台进程的终止：Ctrl+c 七、nohup如果让程序始终在后台执行，即使关闭当前的终端也执行（之前的&做不到），这时候需要nohup。该命令可以在你退出帐户&#x2F;关闭终端之后继续运行相应的进程。关闭中断后，在另一个终端jobs已经无法看到后台跑得程序了，此时利用ps（进程查看命令） ps -aux | grep “test.sh” #a:显示所有程序 u:以用户为主的格式来显示 x:显示所有程序，不以终端机来区分","date":"2024-12-31","categories":["新博客"]},{"title":"dubbo分析(1)-插件化模块化能力类SPI能力设计解析","url":"/2024/12/31/dubbo分析-1--插件化模块化能力类SPI能力设计解析/","content":"基于dubbo 2.7 版本 都说duboo的插件化是基于spi我并不认同着一个观点。 我认为dubbo是自己实现了一波spi类似spring boot 总结：dubbo 并没有使用java的spi而是实现了一种更加强悍的spi机制（自动类加载机制） 核心类ExtensionLoader dubbo 自己实现SPI机制其实这一块的逻辑很简单,定位到了核心类ExtensionLoader的两个方法中 第二个核心方法 createExtension 这个是真正创建类型的方法 自适应扩展这个类在整个dubbo中算是一个硬核的类了，总计1000多行代码，看名称就能知道这个类其实承载了dubbo整个动态加载的逻辑 Extension在dubbo的使用方法一般是这样的 这种方法获取的是这个类的自定义扩展信息,并且带上了参数校验 , 注意必须是被spi注解标记的类才可以进行使用 核心其实就是两个方法，getExtensionLoader和getAdaptiveExtension，前者负责实例化一个ExtensionLoader 后者进行动态加载 getExtensionLoader这个方法很简单，就是将类型和ExtensionLoder对象之间的映射关系一一对应，并且初始化ExtensionLoader对象中的ObjectFactory 类型对象实例化工程方法 getAdaptiveExtension这个类就是非常复杂了，我逻辑简化一下，只做关键点记录 发现一个关键方法createAdaptiveExtension 有点恶心了 先看看getAdaptiveExtensionClass方法干嘛了吧 有一个方法 getExtensionClasses ， 不过这个方法很重要 ， 下面的所有的逻辑都是为了加载扩展类的并缓存在自己的内存数组上 不看源码了 ， 直接说结论 从一个指定的文件夹中的文件中获取像下面这样文本 将这个文本整合出一套k v 结构的map 注意一种注解和一种类型@Adaptive和WrapperClass（ps 以当前类型为构造函数的类） getAdaptiveExtensionClass 方法同样包含了三个逻辑，如下： 调用 getExtensionClasses 获取所有的拓展类检查缓存，若缓存不为空，则返回缓存若缓存为空，则调用 createAdaptiveExtensionClass 创建自适应拓展类这三个逻辑看起来平淡无奇，似乎没有多讲的必要。但是这些平淡无奇的代码中隐藏了着一些细节，需要说明一下。首先从第一个逻辑说起，getExtensionClasses 这个方法用于获取某个接口的所有实现类。比如该方法可以获取 Protocol 接口的 DubboProtocol、HttpProtocol、InjvmProtocol 等实现类。在获取实现类的过程中，如果某个某个实现类被 Adaptive 注解修饰了，那么该类就会被赋值给 cachedAdaptiveClass 变量。此时，上面步骤中的第二步条件成立（缓存不为空），直接返回 cachedAdaptiveClass 即可 还有一个方法 createAdaptiveExtensionClass 这个方法非常特殊 , 是自动化生成扩展类的参数校验类… 注意这里的一端代码—自动生成Adaptive方法String code &#x3D; new AdaptiveClassCodeGenerator(type, cachedDefaultName).generate();,其中的generate这一块应该是整个dubbo最难理解的地方了 spi这里其实写的并不是很合理，存在大量的问题，其实本质上dubbo在生成自适应类的时候，一个核心就是使用url作为参数，各种数据其实都是在url中获取的， 然后遍历 看一个编译之后的代码 注意上面的逻辑重点其实是 extName这个方法 ， 这个方法使用自动生成逻辑中的generateExtNameAssignment生成的 其中value是在Adaptive注解中标记的名称，dubbo将会对这些名称在url中进行自动化获取，如果是protocal类型将会直接从url协议参数中获取 ps 注意，dubbo在这里做的其实并不好 ， 逻辑非常的混乱 ， 其中有一个非常重要的参数就是ExtName , 他的默认值是从AdaptiveClassCodeGenerator的构造函数中传入的，并且调用这个构造函数的ExtendLoad是在初始化ExtendLoad class 的时候，解析SPI注解中的参数进行默认注入的 。。。 其实注意，这里本质上就是封装一层从url中获取参数然后给SPI实现类调用的逻辑 其他重要方法injectExtension获取这个类依赖的spi扩展属性 , 使用set注入到这个类中 注意这个方法，dubbo的默认objectFactory使用的是ExtendsObjectFactory 所以，只能加载SPI的类型","date":"2024-12-31","categories":["新博客"]},{"title":"dubbo分析(2)-dubbo服务暴露代码分析","url":"/2024/12/31/dubbo分析-2--dubbo服务暴露代码分析/","content":"基于dubbo2.7 + 上一篇分析了dubbo有关自动化扩展的相关信息，这一片正式进入主题，研究一下dubbo服务初始化的过程 初始化代码的展示 注意这里的两块 ， 一个是setInterface初始化的是实现的接口，并且初始化ServiceConfig内部的ID值为这个接口的名称，另一个石Ref方法，对应这个接口的实现类 然后重点就是这个DubboBootstrap类了","date":"2024-12-31","categories":["新博客"]},{"title":"java-forkjoin框架使用和一些原则","url":"/2024/12/31/java-forkjoin框架使用和一些原则/","content":"先扯一波使用 两个demo解决 使用RecursiveAction无状态任务拆分(无返回值状态) 注意几个点 awaitQuiescence 是监控这个forkjoin是否都完成 awaitTermination 是监控这个forkjoin是否shutdown 使用execute表示用无返回值的方法来处理异步请求 使用RecursiveTask有状态任务查分 注意一个点 使用 submit 和 future类来实现监控返回值状态 原理这里就不说了 , 主要是几个特性 自动的线程扩展 很多线程池都有这种能力,通过监控cpu核心数量来创建线程数量 工作窃取 空闲的线程去窃取其他工作线程队列中的任务 类似WorkStealingPool 多工作队列线程池 任务分片 forkjoin框架的核心","date":"2024-12-31","categories":["新博客"]},{"title":"java-从java线程池来看java的阻塞队列","url":"/2024/12/31/java-从java线程池来看java的阻塞队列-1/","content":"一说到java的阻塞队列，我们就会想到在java的jdk中的那么多的类 1.ArrayDeque, （数组双端队列）2.PriorityQueue, （优先级队列）3.ConcurrentLinkedQueue, （基于链表的并发队列）4.DelayQueue, （延期阻塞队列）（阻塞队列实现了BlockingQueue接口）5.ArrayBlockingQueue, （基于数组的并发阻塞队列）6.LinkedBlockingQueue, （基于链表的FIFO阻塞队列）7.LinkedBlockingDeque, （基于链表的FIFO双端阻塞队列）8.PriorityBlockingQueue, （带优先级的无界阻塞队列）9.SynchronousQueue （并发同步阻塞队列） 这里不去细说的这些东西，而是从线程池的一个异常来聊聊这个事情 构造一个线程池异常 - 线程池过载异常 这段代码如果直接运行将会抛出异常 总结一下： 其实这个问题是加入的线程数量已经超过了整个线程池能负载的最大数量了（新建线程池的时候使用了有界队列），所以抛出了了异常 避免线程池溢出异常 - 使用无界队列和有界队列 BlockingQueue 这个是为了解决java并发同步问题的，本质上是解决线程间消息同步而设计的 有一下几个类： 1.DelayQueue, （延期阻塞队列）（阻塞队列实现了BlockingQueue接口） 这个队列是无界的，并且没有指定长度的构造方法2.ArrayBlockingQueue, （基于数组的并发阻塞队列） 必须设置长度3.LinkedBlockingQueue, （基于链表的FIFO阻塞队列） 没有指定长度就是有界的反之是有界的4.LinkedBlockingDeque, （基于链表的FIFO双端阻塞队列） 没有指定长度就是有界的反之是有界的5.PriorityBlockingQueue, （带优先级的无界阻塞队列） 这个只能传入Comperable接口的类新，不是有界的6.SynchronousQueue （并发同步阻塞队列）不能指定长度，只能传入一个值，有界 回过来看看上面的源码，其实线程池在加入线程时候的逻辑是这样的 构建常驻线程coreNum指定，如果超过，建立临时线程maxNum , 还不够，增加到队列中 线程池判断能不能增加仅对列是使用的队列的offset方法 如果是有界的队列党对列满了自然返回false，因为添加不进去了，然后就会抛出异常 总结一下我们在使用java线程池的时候需要做好容量规划，如果无法确定是否超出了指定的线程数量，可以使用无界队列，但是要注意到防止内存泄漏","date":"2024-12-31","categories":["新博客"]},{"title":"java-从js来看java的==操作","url":"/2024/12/31/java-从js来看java的--操作/","content":"在风中凌乱… … 最近看了一个问题如何在js中让下面的这个表达式返回true 答案 其实这个方法是利用了js的语法规则, js应为是弱类型编程语言,所以它的&#x3D;&#x3D; 操作其实是需要进行数值导出的默认的过程是先valueOf 然后是 toString java是不是类似的呢?答案是否定的… java对非基本类型直接使用内存地址进行比较 如果需要特殊的比较方法,需要重写equel方法来达到目的","date":"2024-12-31","categories":["新博客"]},{"title":"java-强弱软虚四种引用的理解","url":"/2024/12/31/java-强弱软虚四种引用的理解/","content":"java 提供了四种引用类型，强引用，弱引用，软引用，虚引用 ， 这四种引用分别有不同的作用和使用场景 强引用：只要引用存在，垃圾回收器永远不会回收Object obj &#x3D; new Object();&#x2F;&#x2F;可直接通过obj取得对应的对象 如obj.equels(new Object());而这样 obj对象对后面new Object的一个强引用，只有当obj这个引用被释放之后，对象才会被释放掉，这也是我们经常所用到的编码形式。 软引用：非必须引用，内存溢出之前进行回收，可以通过以下代码实现Object obj &#x3D; new Object();SoftReference sf &#x3D; new SoftReference(obj);obj &#x3D; null;sf.get();&#x2F;&#x2F;有时候会返回null这时候sf是对obj的一个软引用，通过sf.get()方法可以取到这个对象，当然，当这个对象被标记为需要回收的对象时，则返回null；软引用主要用户实现类似缓存的功能，在内存足够的情况下直接通过软引用取值，无需从繁忙的真实来源查询数据，提升速度；当内存不足时，自动删除这部分缓存数据，从真正的来源查询这些数据。 弱引用：第二次垃圾回收时回收，可以通过如下代码实现Object obj &#x3D; new Object();WeakReference wf &#x3D; new WeakReference(obj);obj &#x3D; null;wf.get();&#x2F;&#x2F;有时候会返回nullwf.isEnQueued();&#x2F;&#x2F;返回是否被垃圾回收器标记为即将回收的垃圾弱引用是在第二次垃圾回收时回收，短时间内通过弱引用取对应的数据，可以取到，当执行过第二次垃圾回收时，将返回null。弱引用主要用于监控对象是否已经被垃圾回收器标记为即将回收的垃圾，可以通过弱引用的isEnQueued方法返回对象是否被垃圾回收器标记。 虚引用： 垃圾回收时回收，无法通过引用取到对象值，可以通过如下代码实现Object obj &#x3D; new Object();PhantomReference pf &#x3D; new PhantomReference(obj);obj&#x3D;null;pf.get();&#x2F;&#x2F;永远返回nullpf.isEnQueued();&#x2F;&#x2F;返回是否从内存中已经删除虚引用是每次垃圾回收的时候都会被回收，通过虚引用的get方法永远获取到的数据为null，因此也被成为幽灵引用。虚引用主要用于检测对象是否已经从内存中删除。","date":"2024-12-31","categories":["新博客"]},{"title":"java-时间处理夏令时冬令时跨时区问题处理","url":"/2024/12/31/java-时间处理夏令时冬令时跨时区问题处理-1/","content":"作为一个成熟的编程语言，java自然有一堆方法来解决的时间的问题。有的时候我们会因为对java一些内置的api不是太熟悉，对某个场景不熟悉，导致遇到了棘手的问题。比如这个场景夏令时和冬令时 夏令时和冬令时在做全球性的功能时绝对少不了遇到时区转化，一般情况下使用时间戳+java内置的api就能解决99%的问题，但是如果遇到夏令时或者冬令时的时候这个问题就可能变得不是这么容易。 首先记录一下什么是夏令时和冬令时：简单的说在这个世界上的某些国家会规定在某个日期将本国所在的时区发生改变，然后在某个时间将他改回来，进行改变的日期就是夏令时或者冬令时 注意：这个概念深层次的东西可以自行用搜索引擎查找相关内容，我这里没有用其他人的那种解释比如夏令时就是把表调快一小时，而是使用修正时区这个概念，这么做是为了方便做下面的解释 夏令时和冬令时产生的业务场景举个例子： 一个实行了夏令时和冬令时的国家在夏令有一个活动，每天11点到1一点参加，为期七天，而这7天正好过了令时变化的这一天， 这样会导致什么问题呢？ 因为跨过令时所以跨令时之前一天的12点20分和后一天的12点20分之间相隔的并不是24小时，因为令时的变化携带的时区的变化，因为时区变化了，所以相同的12点20分对应的毫秒数是不同的（毫秒没有时区问题）所以为期七天这个过程不能简单使用+24小时来处理了，因为这样就可能导致跨令时前是11点到1点，跨令时之后就是12点到2点了 怎么解决java提供了一个非常牛逼的api TimeZone ，专门用来处理时区问题 有两个api 这样就很简单了,使用这两个api如果返回的值不相等,就说明当前时间处于某一个令时中 进阶一下,解决一下上面的需求,跨令时的时候保证日期是+1的解释一下这个需求, 直接用一个最基本的例子来说 我用西班牙国家的时区来做一个+24小时的天数迭代,理论上应该让2019-10-26 4点和6点,变成2019-10-27 4点和6点但是应为夏令时的问题,其实变成的是成2019-10-27 3点和5点 跑一下下面的代码 输出: 所以不能这么做,直接上代码,其实就是一个很简单的算法题了 上面的算法简单的说就是补偿时差,当前的时间和之后的进行比较,如果有差别就+上差距就行了 , 这样就能保证在跨夏令时的时候保证时间统一","date":"2024-12-31","categories":["新博客"]},{"title":"java-资源获取-文件系统中-jar包中-jar包中的jar中","url":"/2024/12/31/java-资源获取-文件系统中-jar包中-jar包中的jar中-1/","content":"一开始其实用这个题目作为文章的标题很难受，但是没有办法java的文件获取就是这么负载:cold_sweat: java获取数据获取文件系统的api这个就不罗嗦了，就是java的文件操作FileInputStream等等 java如果获取jar中的文件怎么做java 有一个特殊的api的ClassLoader.getResource()与getResources() 简单的说这个api是干嘛的,加载类路径资源文件,也就是classPath中的东西（也就是可以拿到classpath中的所有东西包括jar中的东西） 直接上源码 注意代码这一段Enumeration urls &#x3D; getClassLoader().getResources(packageName.replace(“.”, “&#x2F;“)); 返回值石URL类型，注意在java中这个url其实有多种协议构成的，file前缀表示在classpath文件路径中的资源，而jar前缀表示在jar中的资源 如果是file前缀的资源 这种情况下就当成基本的文件数据流来处理就好了其实很简单 如果石jar前缀的资源 这个地方要注意一下，有一个坑，就是如果jvm发现这个地方有我们需要的资源我们需要通过JarURLConnection jarURLConnection &#x3D; (JarURLConnection) url.openConnection();这个api来获取资源的引用，但是呢这个引用会将这个jar中所有的文件都加载进来，所以在使用的时候需要做一个过滤。 总结一下上面的代码已经实现了一个java jar中文件的搜索，网络上有类似的代码，但是实际情况是有一些bug的，这里我做了一些修改（因为我们传入的是指定的包，但是jar加载会把整个jar文件全部加载进来，所以我在里面加了一个过滤操作），现在已经可以稳定运行了。","date":"2024-12-31","categories":["新博客"]},{"title":"java8-线程池问题-jvmgc置空回收机制的排查","url":"/2024/12/31/java8-线程池问题-jvmgc置空回收机制的排查/","content":"先说结论,主观上,我们都觉得jvm貌似是在确认对象不达的时候进行回收的 问题就在这个不可达上面 , 官方的解释是 可达对象 (reachable object) 是可以从任何活动线程的任何潜在的持续访问中的任何对象；java 编译器或代码生成器可能会对不再访问的对象提前置为 null，使得对象可以被提前回收 也就是说对象的回收时机是可以在函数执行完成之前就触发的 运行之后你会发现输出的数据是这样的 程序函数在没有被执行完之前就被gc回收了 这样引入一个复杂的场景比如多线程环境gc是否还是有问题的ps: 这个情况其实是最近遇到的一个线程池莫名其妙被kill的问题引申出来的 先贴一段代码 执行一下我们上面的函数你会发现一个问题 Executor 对象这个创建线程的玩意竟然被回收了….. 这个问题映射到java jdk的问题上的话就是在jdk1.8 使用的时候会有线程池莫名其妙关闭的问题 , 贴一下代码 看上面的代码有一个 finalize方法,他调用了shutdown(); — 所以我们用一开始的代码来模拟这个jdk的bug 这里对这里面的对象进行一次可达性分析 先说一下java gc 的可达性分析的流程吧… … 在Java中采取了 可达性分析法。该方法的基本思想是通过一系列的“GC Roots”对象作为起点进行搜索，如果在“GC Roots”和一个对象之间没有可达路径，则称该对象是不可达的，不过要注意的是被判定为不可达的对象不一定就会成为可回收对象。被判定为不可达的对象要成为可回收对象必须至少经历两次标记过程，如果在这两次标记过程中仍然没有逃脱成为可回收对象的可能性，则基本上就真的成为可回收对象了。 此算法的核心思想：通过一系列称为“GC Roots”的对象作为起始点，从这些节点开始向下搜索，搜索走过的路径称为“引用链”，当一个对象到 GC Roots 没有任何的引用链相连时(从 GC Roots 到这个对象不可达)时，证明此对象不可用。 在Java语言中，可作为GC Roots的对象包含以下几种： 虚拟机栈(栈帧中的本地变量表)中引用的对象。(可以理解为:引用栈帧中的本地变量表的所有对象) 方法区中静态属性引用的对象(可以理解为:引用方法区该静态属性的所有对象) 方法区中常量引用的对象(可以理解为:引用方法区中常量的所有对象) 本地方法栈中(Native方法)引用的对象(可以理解为:引用Native方法的所有对象) 可以理解为: (1)首先第一种是虚拟机栈(线程栈)中的引用的对象，我们在程序中正常创建一个对象，对象会在堆上开辟一块空间，同时会将这块空间的地址作为引用保存到虚拟机栈中，如果对象生命周期结束了，那么引用就会从虚拟机栈中出栈，因此如果在虚拟机栈中有引用，就说明这个对象还是有用的，这种情况是最常见的。 (2)第二种是我们在类中定义了全局的静态的对象，也就是使用了static关键字，由于虚拟机栈是线程私有的，所以这种对象的引用会保存在共有的方法区中，显然将方法区中的静态引用作为GC Roots是必须的。 (3)第三种便是常量引用，就是使用了static final关键字，由于这种引用初始化之后不会修改，所以方法区常量池里的引用的对象也应该作为GC Roots。最后一种是在使用JNI技术时，有时候单纯的Java代码并不能满足我们的需求，我们可能需要在Java中调用C或C++的代码，因此会使用native方法，JVM内存中专门有一块本地方法栈，用来保存这些对象的引用，所以本地方法栈中引用的对象也会被作为GC Roots。 finalize()方法最终判定对象是否存活: 即使在可达性分析算法中不可达的对象，也并非是“非死不可”的，这时候它们暂时处于“缓刑”阶段，要真正宣告一个对象死亡，至少要经历再次标记过程。标记的前提是对象在进行可达性分析后发现没有与GC Roots相连接的引用链。 1).第一次标记并进行一次筛选。筛选的条件是此对象是否有必要执行finalize()方法。当对象没有覆盖finalize方法，或者finzlize方法已经被虚拟机调用过，虚拟机将这两种情况都视为“没有必要执行”，对象被回收。 2).第二次标记如果这个对象被判定为有必要执行finalize（）方法，那么这个对象将会被放置在一个名为：F-Queue的队列之中，并在稍后由一条虚拟机自动建立的、低优先级的Finalizer线程去执行。这里所谓的“执行”是指虚拟机会触发这个方法，但并不承诺会等待它运行结束。这样做的原因是，如果一个对象finalize（）方法中执行缓慢，或者发生死循环（更极端的情况），将很可能会导致F-Queue队列中的其他对象永久处于等待状态，甚至导致整个内存回收系统崩溃。Finalize（）方法是对象脱逃死亡命运的最后一次机会，稍后GC将对F-Queue中的对象进行第二次小规模标记，如果对象要在finalize（）中成功拯救自己—-只要重新与引用链上的任何的一个对象建立关联即可，譬如把自己赋值给某个类变量或对象的成员变量，那在第二次标记时它将移除出“即将回收”的集合。如果对象这时候还没逃脱，那基本上它就真的被回收了。 再来分析上面的问题匹配gc算法的(1)号情况 GC routs 将会以虚拟机栈引用的对象为根… … 看main函数中的创建方法 会发现这一行对所有的对象来说都是没有引用的,也就是说在触发下一次gc的时候将会被回收,执行其中的finilly函数…. 反观jdk的代码,和我们的类似,newSingleThreadExecutor方法直接return一个新的对象,如果这个对象和我们例子中用法类似将会导致类似的问题 问题来了,其实我们并不想结束这个Executor如何解决 直接强制引用就行了 修改代码 将每次new出的对象进行缓存,就能保证不会被可达性分析命中,从而保证常驻 java9+使用Reference.reachabilityFence(this);保证try finally中的对象强可达 这个方法保证了只有在调用了reachabilityFence之后,java的gc才能回收这个class – 估计是javajdk的一些特殊的编译逻辑保证了这个特性…….. 比如这个代码在java11中就不会出问题了 不过这个问题貌似在java11 中解决了,相同的代码在java11中没有bug 一脸蒙蔽 , 其实本质上还是jvm的bug jvm没有考虑到这种应用情况 其实就是jvm的一个bug , 可以实用强制引用规避…… 写代码的时候不要使用finilly","date":"2024-12-31","categories":["新博客"]},{"title":"jdk源码阅读-map","url":"/2024/12/31/jdk源码阅读-map/","content":"针对jdk11+ map类型操作最核心的就是这些方法get(),put(),contain(),computeIfAbsent(), 我们只解析这些方法如何实现 有一些核心点逻辑 , resize()(扩容逻辑) 散列值计算获取逻辑 map类型有一些通用的逻辑操作,统一梳理 最为解析的map类型是 HashMap Hashtable TreeMap WeakHashMap LinkedHashMap ConcurrentHashMap map类型散列值计算 典型Map类型 hash计算逻辑 下标计算逻辑 HashTable key的hash (hash & 0x7FFFFFFF) % tab.length HashMap (h &#x3D; key.hashCode()) ^ (h >>> 16) hash&(table长度-1) ConurrentHashMap (h ^ (h >>> 16)) & 0x7FFFFFFFTS hash&(table长度-1) 为啥 & 0x7FFFFFFFTS 呢 , 为了获得一个整形 没啥道理 map类型get方法 hashMap map类型put方法 map类型扩张因为hash散列的问题 , 所有基于table构建的hashmap 都是按照2的N次方进行扩容 , 之所以是2的N次方是方便hash散列 , 让散列更加均匀 hashMap 的扩张逻辑 两个变量 , 1. DEFAULT_INITIAL_CAPACITY -> 默认容量 2. threshold -> 默认的阀值(这个是值是默认的容量*负载系数) ConcurrentHashMap 扩张逻辑","date":"2024-12-31","categories":["新博客"]},{"title":"linux-io和文件操作详解","url":"/2024/12/31/linux-io和文件操作详解/","content":"","date":"2024-12-31","categories":["新博客"]},{"title":"mybatis和spring","url":"/2024/12/31/mybatis和spring/","content":"","date":"2024-12-31","categories":["新博客"]},{"title":"springboot-配置加载顺序","url":"/2024/12/31/springboot-配置加载顺序/","content":"优先级由高到低,高优先级覆盖低优先级 1.在命令行中传入的参数;2.SPRING_APPLICATION_JSON 中的属性。SPRING_APPLICATION_JSON 是以JSON 格式配置在系统环境变量中的内容；3.java:comp&#x2F;env 中的 JNDI 属性;4.java 的系统属性，可以通过System.getProperties()获得的内容;5.操作系统的环境变量。6.通过random.*配置的随机属性；7.位于当前应用jar包之外，针对不同{profile}环境的配置文件内容，例如 applicaiton-{profile}.properties 或是 yaml 定义的配置文件；8.位于当前应用jar包之内，针对不同{profile}环境的配置文件内容，例如 applicaiton-{profile}.properites 或是 yaml 定义的配置文件；9.位于当前应用jar包之外的 application.properties 和yaml配置内容；10.位于当前应用jar包之内的 application.properties 和yaml配置内容；11.在@Configration注解修改的类中，通过@PropertySource注解定义的属性；12.应用默认属性，使用SpringApplication.setDefaultProperties定义的内容；","date":"2024-12-31","categories":["新博客"],"tags":["spring相关一些零散的细节"]},{"title":"spring最核心的东西-spring容器初始化到bean初始化的全过程","url":"/2024/12/31/spring最核心的东西-spring容器初始化到bean初始化的全过程/","content":"引申: 其实一开始不想写这篇文章的 , 奈何发现市面上的文章大部分都是一知半解 . 索性整理一篇文章方便以后阅读查阅 这次就不一开始就长篇大论了 , 先说一个问题 ,为啥我们要研究spring 中bean的初始化顺序 为啥研究spring中javabean的初始化顺序spring 框架是个啥? -> 本质上是一个IOC框架,解决依赖问题的玩意 , 依赖问题的核心承载点是啥? 是对象\\javabean , 所以如果把这一块搞明白了,其实整个spring框架已经差不多搞透彻了. spring中bean到底是咋初始化的啊?其实扒开spring的外衣 , 其实最核心的东西是BeanFactory , spring自己能找到的所有的java bean 全在里面 , 这个就是整个spring的数据中心 如果让我说spring最大的成就是什么 , 我认为spring最大的成就就是实现整个容器中java对象逻辑的托管功能 , 也就是我认为的IOC spring容器初始化的开端ApplicationContext 这个是spring整个java对象管理运行的开端,我称它为始祖种子类 注意永远记住spring是一个庞大臃肿功能强大的java框架 , 所以ApplicationContext的实现类特多,挑几个重点来说 spring boot 中使用的ApplicationContext -> AnnotationConfigApplicationContext -> 使用java配置来实现将javabean信息注入到容器 spring中最常用的ApplicationContext -> ClassPathXmlApplicationContext -> 使用xml来将javabean信息注入到容器 这两个类都有继承自AbstractApplication 所有的spring对象初始化的时候都会调用这个通用的类 , 整个bean初始化,容器加载相关的逻辑都是在这里面的 spring IOC过程中我们最应该关注什么问题 我认为整个IOC过程中最应该关注的就是这两个问题 1. bean什么时候初始化的? 怎么初始化的? 2. bean之间的依赖关系是什么时候? 是怎么解决的? 3. 特殊状态的javabean比如lookup和lazy是如何做的 bean什么时候初始化的spring中bean的初始化的过程 ， 我认为分为如下的几个过程 java bean 加载进容器 ， 并且根据类型进行一定的初始化 处理 java bean的依赖关系 这里梳理一下他们的过程 javabean加载进容器， 并且根据类型进行一定的初始化操作 这一步中 ClassPathXMLApplicationContext和 AnnotationConfigApplicationContext的操作是不同的 1. ClassPathXMLApplication ClassPathApplicationContext 在spring中属于元老了 ， 他的着一块的支持是在AbstractApplicationContext中的refresh()方法（下面会给出源码）中实现的 最核心的逻辑是在ConfigurableListableBeanFactory beanFactory &#x3D; obtainFreshBeanFactory();这一行中 这里其实知道结论就好了 ，没有必要追究细节 ， 这里spring将会通过spring的xml文件生成一系列的javabean的包装bean -> BeanDefinition ,然后放入默认生成的DefaultListableBeanFactory之中, 注意在spring中 所有的java bean在解析的过程中都会变成 BeanDefinition(有些特殊的单例不是用BeanDefinition封装的) 2. AnnotionAppicationContext 这个需要重点关注一下 ， 这个context和classPathXmLApplicationContext有一定的区别 , 他是继承自GenericApplicationContext , GenericApplicationContext是新一代spring使用注解进行bean配置的核心Context 首先注意下这个ApplicationContext的构造方法 AnnotatedBeanDefinitionReader 和 ClassPathBeanDefinitionScanner 都是初始化用来将使用class或者basePackages名称加载的BeanDefinition注册进GenericApplicationContext的工具类 最核心的方法AbstractAplication的refresh()方法 , 这个方法完成了整个spring声明周期的初始化 我们只要注意这个方法 finishBeanFactoryInitialization() spring所有非内置的单例 javabean都是在这里初始化的 , 主要是下面的这一行 这行做了beanFactory中所有单例 javabean初始化和依赖处理的, 进入看一下 其实他的核心逻辑就是在这里,通过使用getBean方法来尽心初始化的-> 而这个方法同样也是我们每次使用ApplicationContext获取对象的时候所需要调用的类 这个getBean方法非常长,其实本质上就是尝试获取一个javabean,内部实现了bean的创建,依赖管理等等的逻辑,贴一个单例模式的核心代码吧 外层的getSingleton 方法自然就是获取单例而其中的会掉就是当单例不存在的时候,进行创建的逻辑,进入一下这个createBean 这里代码依然很多但是这个类最终依赖得了doCreateBean方法,所以直接看doCreateBean方法里有啥,但是这里执行了一个前置处理逻辑(postProcessBeforeInstantiation)方法来超前处理bean初始化 接下来就是重点了doCreateBean方法,解决了循环引用问题和实现提早曝光的逻辑都是在这里 这里有几个重点 我们的javabean属性注入是在 populateBean方法中实现的,他的内部逻辑是使用了递归来循环的填冲属性. 这里也处理了构造函数等等依赖关系. 这里要插一句AutoWriterAnnotionBeanPostProcessor AutoWriterAnnotionBeanPostProcessor , 在这里有一个特殊的处理逻辑可以借鉴一下 AutoWriterAnnotionBeanPostProcessor 将需要的属性,使用反射并将反射缓存了一下(injectionMetadataCache) , 来加快性能 依赖关系处理 其实看上面doCreate的逻辑有一个earlySingletonExposure变量, spring在执行到这里的时候将会自动的将这个还没有进行初始化的类(也就是当bean为单例 && 容器配置允许循环依赖 && bean正在创建)的时候,将会在BeanFactory中设置第三级缓存, key是当前的bean的名称,value是一个工厂方法 , 很简单就是返回当前的类 ps: spring 如何解决循环引用的? 这个可以看一下spring的getBean的逻辑 , 其中有一段这样的代码 如果第一层缓存没有找到,就从早起类里找如果早期类里还没有,就使用工厂方法获取 聊一下这三层缓存都干了啥 第一层: 所有的初始化完成的单例bean早期的bean: 被循环引用触发了,并且还没有初始化完成的bean第三层: 正在被初始化的bean 如何解决循环引用就很简单了 , 如果A依赖了B 并且B依赖了A , 当A初始化了B的时候, B初始化时将会从第三层引用中获取当前的提早曝光的类,从而实现了解决循环引用,并且spring的实现,还实现了多线程模式下也可以进行循环应用的逻辑 还没完,整个生命周期还没有彻底搞懂","date":"2024-12-31","categories":["新博客"],"tags":["spring相关一些零散的细节"]},{"title":"分布式协调和同步","url":"/2024/12/31/分布式协调和同步/","content":"本质上是让分布在不同计算机上的程序具有“团队精神”，换句话说就是让程序通过协作共同去达成一个业务目标 分布式互斥对于同一共享资源，一个程序正在使用的时候也不希望被其他程序打扰。这，就要求同一时刻只能有一个程序能够访问这种资源。 在分布式系统里，这种排他性的资源访问方式，叫作分布式互斥（Distributed Mutual Exclusion），而这种被互斥访问的共享资源就叫作临界资源（Critical Resource）。 如何才能让分布式系统里的程序互斥地访问临界资源?三种算法-> 集中式算法 , 分布式算法 , 令牌环算法集中式算法我们引入一个协调者程序，得到一个分布式互斥算法。每个程序在需要访问临界资源时，先给协调者发送一个请求。如果当前没有程序使用这个资源，协调者直接授权请求程序访问；否则，按照先来后到的顺序为请求程序“排一个号”。如果有程序使用完资源，则通知协调者，协调者从“排号”的队列里取出排在最前面的请求，并给它发送授权消息。拿到授权消息的程序，可以直接去访问临界资源。 这个互斥算法，就是我们所说的集中式算法，也可以叫做中央服务器算法。之所以这么称呼，是因为协调者代表着集中程序或中央服务器。 集中式算法的示意图如下所示： 从上述流程可以看出，一个程序完成一次临界资源访问，需要如下几个流程和消息交互： 向协调者发送请求授权信息，1 次消息交互； 协调者向程序发放授权信息，1 次消息交互； 程序使用完临界资源后，向协调者发送释放授权，1 次消息交互 不难看出，集中式算法的优点在于直观、简单、信息交互量少、易于实现，并且所有程序只需和协调者通信，程序之间无需通信。但是，这个算法的问题也出在了协调者身上 一方面，协调者会成为系统的性能瓶颈。想象一下，如果有 100 个程序要访问临界资源，那么协调者要处理 100*3&#x3D;300 条消息。也就是说，协调者处理的消息数量会随着需要访问临界资源的程序数量线性增加。 另一方面，容易引发单点故障问题。协调者故障，会导致所有的程序均无法访问临界资源，导致整个系统不可用。 因此，在使用集中式算法的时候，一定要选择性能好、可靠性高的服务器来运行协调者。 分布式算法当一个程序要访问临界资源时，先向系统中的其他程序发送一条请求消息，在接收到所有程序返回的同意消息后，才可以访问临界资源。其中，请求消息需要包含所请求的资源、请求者的 ID，以及发起请求的时间。 类似民主协商,在分布式领域中，我们称之为分布式算法，或者使用组播和逻辑时钟的算法。 一个程序完成一次临界资源的访问，需要进行如下的信息交互： 向其他 n-1 个程序发送访问临界资源的请求，总共需要 n-1 次消息交互； 需要接收到其他 n-1 个程序回复的同意消息，方可访问资源，总共需要 n-1 次消息交互。 可以看出，一个程序要成功访问临界资源，至少需要 2*(n-1) 次消息交互。假设，现在系统中的 n 个程序都要访问临界资源，则会同时产生 2n(n-1) 条消息。总结来说，在大型系统中使用分布式算法，消息数量会随着需要访问临界资源的程序数量呈指数级增加，容易导致高昂的“沟通成本”。 从上述分析不难看出，分布式算法根据“先到先得”以及“投票全票通过”的机制，让每个程序按时间顺序公平地访问资源，简单粗暴、易于实现。但，这个算法可用性很低，主要包括两个方面的原因： 当系统内需要访问临界资源的程序增多时，容易产生“信令风暴”，也就是程序收到的请求完全超过了自己的处理能力，而导致自己正常的业务无法开展。 一旦某一程序发生故障，无法发送同意消息，那么其他程序均处在等待回复的状态中，使得整个系统处于停滞状态，导致整个系统不可用。所以，相对于集中式算法的协调者故障，分布式算法的可用性更低。 针对可用性低的一种改进办法是，如果检测到一个程序故障，则直接忽略这个程序，无需再等待它的同意消息。这就好比在自助餐厅，一个人离开餐厅了，那你在使用咖啡机前，也无需征得他的同意。但这样的话，每个程序都需要对其他程序进行故障检测，这无疑带来了更大的复杂性。 因此，分布式算法适合节点数目少且变动不频繁的系统，且由于每个程序均需通信交互，因此适合 P2P 结构的系统。比如，运行在局域网中的分布式文件系统，具有 P2P 结构的系统等。 那么，在我们工作中，什么样的场景适合采用分布式算法呢？ Hadoop 是我们非常熟悉的分布式系统，其中的分布式文件系统 HDFS 的文件修改就是一个典型的应用分布式算法的场景。 如下图所示，处于同一个局域网内的计算机 1、2、3 中都有同一份文件的备份信息，且它们可以相互通信。这个共享文件，就是临界资源。当计算机 1 想要修改共享的文件时，需要进行如下操作： 计算机 1 向计算机 2、3 发送文件修改请求； 计算机 2、3 发现自己不需要使用资源，因此同意计算机 1 的请求； 计算机 1 收到其他所有计算机的同意消息后，开始修改该文件； 计算机 1 修改完成后，向计算机 2、3 发送文件修改完成的消息，并发送修改后的文件数据； 计算机 2 和 3 收到计算机 1 的新文件数据后，更新本地的备份文件 归纳一下：分布式算法是一个“先到先得”和“投票全票通过”的公平访问机制，但通信成本较高，可用性也比集中式算法低，适用于临界资源使用频度较低，且系统规模较小的场景。 令牌环算法所有程序构成一个环结构，令牌按照顺时针（或逆时针）方向在程序之间传递，收到令牌的程序有权访问临界资源，访问完成后将令牌传送到下一个程序；若该程序不需要访问临界资源，则直接把令牌传送给下一个程序。 因为在使用临界资源前，不需要像分布式算法那样挨个征求其他程序的意见了，所以相对而言，在令牌环算法里单个程序具有更高的通信效率。同时，在一个周期内，每个程序都能访问到临界资源，因此令牌环算法的公平性很好。 但是，不管环中的程序是否想要访问资源，都需要接收并传递令牌，所以也会带来一些无效通信。假设系统中有 100 个程序，那么程序 1 访问完资源后，即使其它 99 个程序不需要访问，也必须要等令牌在其他 99 个程序传递完后，才能重新访问资源，这就降低了系统的实时性。 小结一下：令牌环算法的公平性高，在改进单点故障后，稳定性也很高，适用于系统规模较小，并且系统中每个程序使用临界资源的频率高且使用时间比较短的场景。 有适合大规模系统中的分布式互斥算法吗?可以看到，上面提到的集中式、分布式和令牌环 3 个互斥算法，都不适用于规模过大、节点数量过多的系统。那么，什么样的互斥算法适用于大规模系统呢？ 由于大规模系统的复杂性，我们很自然地想到要用一个相对复杂的互斥算法。时下有一个很流行的互斥算法，两层结构的分布式令牌环算法，把整个广域网系统中的节点组织成两层结构，可以用于节点数量较多的系统，或者是广域网系统。 我们知道，广域网由多个局域网组成，因此在该算法中，局域网是较低的层次，广域网是较高的层次。每个局域网中包含若干个局部进程和一个协调进程。局部进程在逻辑上组成一个环形结构，在每个环形结构上有一个局部令牌 T 在局部进程间传递。局域网与局域网之间通过各自的协调进程进行通信，这些协调进程同样组成一个环结构，这个环就是广域网中的全局环。在这个全局环上，有一个全局令牌在多个协调进程间传递。总结 本质上可以实用集中式算法+令牌环算法组合来实现大规模互斥 集中式算法：可参照redis集群通信模式，通过hash key将大量的请求分散到不同的master,以处理大量请求,每个master由小集群主从节点来保障单点故障分布式算法：分布式算法可在集群中过半数同意就识为其同意，降低通信数，如分布式选举场景令牌环算法：可根据参与者使用频率列出权重，结合平滑加权轮询算法选出下一个参与者 分布式选举提出一个问题对于一个集群来说，多个节点到底是怎么协同，怎么管理的呢。比如，数据库集群，如何保证写入的数据在每个节点上都一致呢? 也许你会说，这还不简单，选一个“领导”来负责调度和管理其他节点就可以了啊。这个想法一点儿也没错。这个“领导”，在分布式中叫做主节点，而选“领导”的过程在分布式领域中叫作分布式选举。然后，你可能还会问，怎么选主呢? 为什么进行分布式选举主节点，在一个分布式集群中负责对其他节点的协调和管理，也就是说，其他节点都必须听从主节点的安排。 主节点的存在，就可以保证其他节点的有序运行，以及数据库集群中的写入数据在每个节点上的一致性。这里的一致性是指，数据在每个集群节点中都是一样的，不存在不同的情况。 当然，如果主故障了，集群就会天下大乱，就好比一个国家的皇帝驾崩了，国家大乱一样。比如，数据库集群中主节点故障后，可能导致每个节点上的数据会不一致。 这，就应了那句话“国不可一日无君”，对应到分布式系统中就是“集群不可一刻无主”。总结来说，选举的作用就是选出一个主节点，由它来协调和管理其他节点，以保证集群有序运行和节点间数据的一致性。 分布式选举算法目前常见的选主方法有基于序号选举的算法（ 比如，Bully 算法）、多数派算法（比如，Raft 算法、ZAB 算法）等 Bully 算法Bully 算法是一种霸道的集群选主算法，为什么说是霸道呢？因为它的选举原则是“长者”为大，即在所有活着的节点中，选取 ID 最大的节点作为主节点。 在 Bully 算法中，节点的角色有两种：普通节点和主节点。初始化时，所有节点都是平等的，都是普通节点，并且都有成为主的权利。但是，当选主成功后，有且仅有一个节点成为主节点，其他所有节点都是普通节点。当且仅当主节点故障或与其他节点失去联系后，才会重新选主。 Bully 算法在选举过程中，需要用到以下 3 种消息： Election 消息，用于发起选举； Alive 消息，对 Election 消息的应答； Victory 消息，竞选成功的主节点向其他节点发送的宣誓主权的消息。 Bully 算法选举的原则是“长者为大”，意味着它的假设条件是，集群中每个节点均知道其他节点的 ID。在此前提下，其具体的选举过程是： 集群中每个节点判断自己的 ID 是否为当前活着的节点中 ID 最大的，如果是，则直接向其他节点发送 Victory 消息，宣誓自己的主权； 如果自己不是当前活着的节点中 ID 最大的，则向比自己 ID 大的所有节点发送 Election 消息，并等待其他节点的回复； 若在给定的时间范围内，本节点没有收到其他节点回复的 Alive 消息，则认为自己成为主节点，并向其他节点发送 Victory 消息，宣誓自己成为主节点；若接收到来自比自己 ID 大的节点的 Alive 消息，则等待其他节点发送 Victory 消息； 若本节点收到比自己 ID 小的节点发送的 Election 消息，则回复一个 Alive 消息，告知其他节点，我比你大，重新选举。 其实Victory可以理解成一种特殊的Election 目前已经有很多开源软件采用了 Bully 算法进行选主，比如 MongoDB 的副本集故障转移功能。MongoDB 的分布式选举中，采用节点的最后操作时间戳来表示 ID，时间戳最新的节点其 ID 最大，也就是说时间戳最新的、活着的节点是主节点。 小结一下。Bully 算法的选择特别霸道和简单，谁活着且谁的 ID 最大谁就是主节点，其他节点必须无条件服从。这种算法的优点是，选举速度快、算法复杂度低、简单易实现。 但这种算法的缺点在于，需要每个节点有全局的节点信息，因此额外信息存储较多；其次，任意一个比当前主节点 ID 大的新节点或节点故障后恢复加入集群的时候，都可能会触发重新选举，成为新的主节点，如果该节点频繁退出、加入集群，就会导致频繁切主。 Raft 算法Raft 算法是典型的多数派投票选举算法，其选举机制与我们日常生活中的民主投票机制类似，核心思想是“少数服从多数”。也就是说，Raft 算法中，获得投票最多的节点成为主。 采用 Raft 算法选举，集群节点的角色有 3 种： 初始化时，所有节点均为 Follower 状态。 开始选主时，所有节点的状态由 Follower 转化为 Candidate，并向其他节点发送选举请求。 其他节点根据接收到的选举请求的先后顺序，回复是否同意成为主。这里需要注意的是，在每一轮选举中，一个节点只能投出一张票。 若发起选举请求的节点获得超过一半的投票，则成为主节点，其状态转化为 Leader，其他节点的状态则由 Candidate 降为 Follower。Leader 节点与 Follower 节点之间会定期发送心跳包，以检测主节点是否活着。 当 Leader 节点的任期到了，即发现其他服务器开始下一轮选主周期时，Leader 节点的状态由 Leader 降级为 Follower，进入新一轮选主。 请注意，每一轮选举，每个节点只能投一次票。这种选举就类似人大代表选举，正常情况下每个人大代表都有一定的任期，任期到后会触发重新选举，且投票者只能将自己手里唯一的票投给其中一个候选者。对应到 Raft 算法中，选主是周期进行的，包括选主和任值两个时间段，选主阶段对应投票阶段，任值阶段对应节点成为主之后的任期。但也有例外的时候，如果主节点故障，会立马发起选举，重新选出一个主节点。 Google 开源的 Kubernetes，擅长容器管理与调度，为了保证可靠性，通常会部署 3 个节点用于数据备份。这 3 个节点中，有一个会被选为主，其他节点作为备。Kubernetes 的选主采用的是开源的 etcd 组件。而，etcd 的集群管理器 etcds，是一个高可用、强一致性的服务发现存储仓库，就是采用了 Raft 算法来实现选主和一致性的。 小结一下。Raft 算法具有选举速度快、算法复杂度低、易于实现的优点；缺点是，它要求系统内每个节点都可以相互通信，且需要获得过半的投票数才能选主成功，因此通信量大。该算法选举稳定性比 Bully 算法好，这是因为当有新节点加入或节点故障恢复后，会触发选主，但不一定会真正切主，除非新节点或故障后恢复的节点获得投票数过半，才会导致切主。 ZAB 算法ZAB（ZooKeeper Atomic Broadcast）选举算法是为 ZooKeeper 实现分布式协调功能而设计的。相较于 Raft 算法的投票机制，ZAB 算法增加了通过节点 ID 和数据 ID 作为参考进行选主，节点 ID 和数据 ID 越大，表示数据越新，优先成为主。相比较于 Raft 算法，ZAB 算法尽可能保证数据的最新性。所以，ZAB 算法可以说是对 Raft 算法的改进。 使用 ZAB 算法选举时，集群中每个节点拥有 3 种角色： Leader，主节点； Follower，跟随者节点； Observer，观察者，无投票权。 选举过程中，集群中的节点拥有 4 个状态： Looking 状态，即选举状态。当节点处于该状态时，它会认为当前集群中没有 Leader，因此自己进入选举状态。 Leading 状态，即领导者状态，表示已经选出主，且当前节点为 Leader。 Following 状态，即跟随者状态，集群中已经选出主后，其他非主节点状态更新为 Following，表示对 Leader 的追随。 Observing 状态，即观察者状态，表示当前节点为 Observer，持观望态度，没有投票权和选举权。 投票过程中，每个节点都有一个唯一的三元组 (server_id, server_zxID, epoch)，其中 server_id 表示本节点的唯一 ID；server_zxID 表示本节点存放的数据 ID，数据 ID 越大表示数据越新，选举权重越大；epoch 表示当前选取轮数，一般用逻辑时钟表示。 ZAB 选举算法的核心是“少数服从多数，ID 大的节点优先成为主”，因此选举过程中通过 (vote_id, vote_zxID) 来表明投票给哪个节点，其中 vote_id 表示被投票节点的 ID，vote_zxID 表示被投票节点的服务器 zxID。ZAB 算法选主的原则是：server_zxID 最大者成为 Leader；若 server_zxID 相同，则 server_id 最大者成为 Leader。 接下来，我以 3 个 Server 的集群为例，此处每个 Server 代表一个节点，与你介绍 ZAB 选主的过程。 第一步：当系统刚启动时，3 个服务器当前投票均为第一轮投票，即 epoch&#x3D;1，且 zxID 均为 0。此时每个服务器都推选自己，并将选票信息 <epoch, vote_id, vote_zxID> 广播出去。 第二步：根据判断规则，由于 3 个 Server 的 epoch、zxID 都相同，因此比较 server_id，较大者即为推选对象，因此 Server 1 和 Server 2 将 vote_id 改为 3，更新自己的投票箱并重新广播自己的投票。 第三步：此时系统内所有服务器都推选了 Server 3，因此 Server 3 当选 Leader，处于 Leading 状态，向其他服务器发送心跳包并维护连接；Server1 和 Server2 处于 Following 状态。 小结一下。ZAB 算法性能高，对系统无特殊要求，采用广播方式发送信息，若节点中有 n 个节点，每个节点同时广播，则集群中信息量为 n*(n-1) 个消息，容易出现广播风暴；且除了投票，还增加了对比节点 ID 和数据 ID，这就意味着还需要知道所有节点的 ID 和数据 ID，所以选举时间相对较长。但该算法选举稳定性比较好，当有新节点加入或节点故障恢复后，会触发选主，但不一定会真正切主，除非新节点或故障后恢复的节点数据 ID 和节点 ID 最大，且获得投票数过半，才会导致切主。 三种选举算法的对比分析 引申 知识扩展：为什么“多数派”选主算法通常采用奇数节点，而不是偶数节点呢？ 多数派选主算法的核心是少数服从多数，获得投票多的节点胜出。想象一下，如果现在采用偶数节点集群，当两个节点均获得一半投票时，到底应该选谁为主呢？ 答案是，在这种情况下，无法选出主，必须重新投票选举。但即使重新投票选举，两个节点拥有相同投票数的概率也会很大。因此，多数派选主算法通常采用奇数节点。 这，也是大家通常看到 ZooKeeper、 etcd、Kubernetes 等开源软件选主均采用奇数节点的一个关键原因。 分布式共识从本质上看，分布式选举问题，其实就是传统的分布式共识方法，主要是基于多数投票策略实现的。基于多数投票策略的分布式选举方法，如果用于分布式在线记账一致性问题中，那么记账权通常会完全掌握到主节点的手里，这使得主节点非常容易造假，且存在性能瓶颈 分布式共识就是在多个节点均可独自操作或记录的情况下，使得所有节点针对某个状态达成一致的过程。通过共识机制，我们可以使得分布式系统中的多个节点的数据达成一致。 分布式共识技术，就是区块链技术共识机制的核心 知识扩展：一致性与共识的区别是什么？一致性是指，分布式系统中的多个节点之间，给定一系列的操作，在约定协议的保障下，对外界呈现的数据或状态是一致的。 共识是指，分布式系统中多个节点之间，彼此对某个状态达成一致结果的过程。 也就是说，一致性强调的是结果，共识强调的是达成一致的过程，共识算法是保障系统满足不同程度一致性的核心技术。 分布式事务 分布式事务，就是在分布式系统中运行的事务，由多个本地事务组合而成 电商处理订单问题，就是典型的分布式事务 要深入理解分布式事务，我们首先需要了解它的特征。分布式事务是多个事务的组合，那么事务的特征 ACID，也是分布式事务的基本特征，其中 ACID 具体含义如下 原子性（Atomicity），即事务最终的状态只有两种，全部执行成功和全部不执行。若处理事务的任何一项操作不成功，就会导致整个事务失败。一旦操作失败，所有操作都会被取消（即回滚），使得事务仿佛没有被执行过一样。 一致性（Consistency），是指事务操作前和操作后，数据的完整性保持一致或满足完整性约束。比如，用户 A 和用户 B 在银行分别有 800 元和 600 元，总共 1400 元，用户 A 给用户 B 转账 200 元，分为两个步骤，从 A 的账户扣除 200 元和对 B 的账户增加 200 元 ; 一致性就是要求上述步骤操作后，最后的结果是用户 A 还有 600 元，用户 B 有 800 元，总共 1400 元，而不会出现用户 A 扣除了 200 元，但用户 B 未增加的情况 (该情况，用户 A 和 B 均为 600 元，总共 1200 元)。 隔离性（Isolation），是指当系统内有多个事务并发执行时，多个事务不会相互干扰，即一个事务内部的操作及使用的数据，对其他并发事务是隔离的。 持久性（Durability），也被称为永久性，是指一个事务完成了，那么它对数据库所做的更新就被永久保存下来了。即使发生系统崩溃或宕机等故障，只要数据库能够重新被访问，那么一定能够将其恢复到事务完成时的状态。 如何实现分布式事务实现分布式事务有以下 3 种基本方法： 基于 XA 协议的二阶段提交协议方法 三阶段提交协议方法 基于消息的最终一致性方法 其中，基于 XA 协议的二阶段提交协议方法和三阶段提交协议方法，采用了强一致性，遵从 ACID，基于消息的最终一致性方法，采用了最终一致性，遵从 BASE 理论 基于 XA 协议的二阶段提交方法XA 是一个分布式事务协议，规定了事务管理器和资源管理器接口。因此，XA 协议可以分为两部分，即事务管理器和本地资源管理器。 XA 实现分布式事务的原理，就类似于我在第 3 篇文章中与你介绍的集中式算法：事务管理器作为协调者，负责各个本地资源的提交和回滚；而资源管理器就是分布式事务的参与者，通常由数据库实现，比如 Oracle、DB2 等商业数据库都实现了 XA 接口。 基于 XA 协议的二阶段提交方法中，二阶段提交协议（The two-phase commit protocol，2PC），用于保证分布式系统中事务提交时的数据一致性，是 XA 在全局事务中用于协调多个资源的机制。 为了保证它们的一致性，我们需要引入一个协调者来管理所有的节点，并确保这些节点正确提交操作结果，若提交失败则放弃事务。接下来，我们看看两阶段提交协议的具体过程。 两阶段提交协议的执行过程，分为投票（voting）和提交（commit）两个阶段。 投票为第一阶段，协调者（Coordinator，即事务管理器）会向事务的参与者（Cohort，即本地资源管理器）发起执行操作的 CanCommit 请求，并等待参与者的响应。参与者接收到请求后，会执行请求中的事务操作，记录日志信息但不提交，待参与者执行成功，则向协调者发送“Yes”消息，表示同意操作；若不成功，则发送“No”消息，表示终止操作。 当所有的参与者都返回了操作结果（Yes 或 No 消息）后，系统进入了提交阶段。在提交阶段，协调者会根据所有参与者返回的信息向参与者发送 DoCommit 或 DoAbort 指令： 若协调者收到的都是“Yes”消息，则向参与者发送“DoCommit”消息，参与者会完成剩余的操作并释放资源，然后向协调者返回“HaveCommitted”消息； 如果协调者收到的消息中包含“No”消息，则向所有参与者发送“DoAbort”消息，此时发送“Yes”的参与者则会根据之前执行操作时的回滚日志对操作进行回滚，然后所有参与者会向协调者发送“HaveCommitted”消息； 协调者接收到“HaveCommitted”消息，就意味着整个事务结束了 必须保证对于单个机器的回滚操作是可以实现的无故障的 二阶段提交的算法思路可以概括为：协调者下发请求事务操作，参与者将操作结果通知协调者，协调者根据所有参与者的反馈结果决定各参与者是要提交操作还是撤销操作。 虽然基于 XA 的二阶段提交算法基本满足了事务的 ACID 特性，但依然有些不足 同步阻塞问题：二阶段提交算法在执行过程中，所有参与节点都是事务阻塞型的。也就是说，当本地资源管理器占有临界资源时，其他资源管理器如果要访问同一临界资源，会处于阻塞状态。 单点故障问题：基于 XA 的二阶段提交算法类似于集中式算法，一旦事务管理器发生故障，整个系统都处于停滞状态。尤其是在提交阶段，一旦事务管理器发生故障，资源管理器会由于等待管理器的消息，而一直锁定事务资源，导致整个系统被阻塞。 数据不一致问题：在提交阶段，当协调者向参与者发送 DoCommit 请求之后，如果发生了局部网络异常，或者在发送提交请求的过程中协调者发生了故障，就会导致只有一部分参与者接收到了提交请求并执行提交操作，但其他未接到提交请求的那部分参与者则无法执行事务提交。于是整个分布式系统便出现了数据不一致的问题。 三阶段提交方法三阶段提交协议（Three-phase commit protocol，3PC），是对二阶段提交（2PC）的改进。为了解决两阶段提交的同步阻塞和数据不一致问题，三阶段提交引入了超时机制和准备阶段。 同时在协调者和参与者中引入超时机制。如果协调者或参与者在规定的时间内没有接收到来自其他节点的响应，就会根据当前的状态选择提交或者终止整个事务。 在第一阶段和第二阶段中间引入了一个准备阶段，也就是在提交阶段之前，加入了一个预提交阶段。在预提交阶段排除一些不一致的情况，保证在最后提交之前各参与节点的状态是一致的。 也就是说，除了引入超时机制之外，3PC 把 2PC 的提交阶段一分为二，这样三阶段提交协议就有 CanCommit、PreCommit、DoCommit 三个阶段。 第一，CanCommit 阶段。CanCommit 阶段与 2PC 的投票阶段类似：协调者向参与者发送请求操作（CanCommit 请求），询问参与者是否可以执行事务提交操作，然后等待参与者的响应；参与者收到 CanCommit 请求之后，回复 Yes，表示可以顺利执行事务；否则回复 No。CanCommit 阶段不同节点之间的事务请求成功和失败的流程，如下所示。 第二，PreCommit 阶段。协调者根据参与者的回复情况，来决定是否可以进行 PreCommit 操作。 如果所有参与者回复的都是“Yes”，那么协调者就会执行事务的预执行： 发送预提交请求。协调者向参与者发送 PreCommit 请求，进入预提交阶段。 事务预提交。参与者接收到 PreCommit 请求后执行事务操作，并将 Undo 和 Redo 信息记录到事务日志中。 响应反馈。如果参与者成功执行了事务操作，则返回 ACK 响应，同时开始等待最终指令。 假如任何一个参与者向协调者发送了“No”消息，或者等待超时之后，协调者都没有收到参与者的响应，就执行中断事务的操作： 发送中断请求。协调者向所有参与者发送“Abort”消息。 中断事务。参与者收到“Abort”消息之后，或超时后仍未收到协调者的消息，执行事务的中断操作。 第三，DoCommit 阶段。DoCmmit 阶段进行真正的事务提交，根据 PreCommit 阶段协调者发送的消息，进入执行提交阶段或事务中断阶段。 执行提交阶段： 发送提交请求。协调者接收到所有参与者发送的 Ack 响应，从预提交状态进入到提交状态，并向所有参与者发送 DoCommit 消息。 事务提交。参与者接收到 DoCommit 消息之后，正式提交事务。完成事务提交之后，释放所有锁住的资源。 响应反馈。参与者提交完事务之后，向协调者发送 Ack 响应。 完成事务。协调者接收到所有参与者的 Ack 响应之后，完成事务。 事务中断阶段： 发送中断请求。协调者向所有参与者发送 Abort 请求。 事务回滚。参与者接收到 Abort 消息之后，利用其在 PreCommit 阶段记录的 Undo 信息执行事务的回滚操作，并释放所有锁住的资源。 反馈结果。参与者完成事务回滚之后，向协调者发送 Ack 消息。 中断事务。协调者接收到参与者反馈的 Ack 消息之后，执行事务的中断，并结束事务。 执行阶段不同节点上事务执行成功和失败 (事务中断) 的流程，如下所示。 在 DoCommit 阶段，当参与者向协调者发送 Ack 消息后，如果长时间没有得到协调者的响应，在默认情况下，参与者会自动将超时的事务进行提交，不会像两阶段提交那样被阻塞住。 基于分布式消息的最终一致性方案2PC 和 3PC 这两种方法，有两个共同的缺点，一是都需要锁定资源，降低系统性能；二是，没有解决数据不一致的问题。因此，便有了通过分布式消息来确保事务最终一致性的方案。 在 eBay 的分布式系统架构中，架构师解决一致性问题的核心思想就是：将需要分布式处理的事务通过消息或者日志的方式异步执行，消息或日志可以存到本地文件、数据库或消息队列中，再通过业务规则进行失败重试。这个案例，就是使用基于分布式消息的最终一致性方案解决了分布式事务的问题。 基于分布式消息的最终一致性方案的事务处理，引入了一个消息中间件（Message Queue，MQ），用于在多个应用之间进行消息传递。基于消息中间件协商多个节点分布式事务执行操作的示意图，如下所示。 刚性事务与柔性事务在讨论事务的时候，我们经常会提到刚性事务与柔性事务，但却很难区分这两种事务。所以，今天的知识扩展内容，我就来和你说说什么是刚性事务、柔性事务，以及两者之间有何区别？ 刚性事务，遵循 ACID 原则，具有强一致性。比如，数据库事务。 柔性事务，其实就是根据不同的业务场景使用不同的方法实现最终一致性，也就是说我们可以根据业务的特性做部分取舍，容忍一定时间内的数据不一致。 总结来讲，与刚性事务不同，柔性事务允许一定时间内，不同节点的数据不一致，但要求最终一致。而柔性事务的最终一致性，遵循的是 BASE 理论。 BASE 理论BASE 理论包括基本可用（Basically Available）、柔性状态（Soft State）和最终一致性（Eventual Consistency）。 基本可用：分布式系统出现故障的时候，允许损失一部分功能的可用性。比如，某些电商 618 大促的时候，会对一些非核心链路的功能进行降级处理。 柔性状态：在柔性事务中，允许系统存在中间状态，且这个中间状态不会影响系统整体可用性。比如，数据库读写分离，写库同步到读库（主库同步到从库）会有一个延时，其实就是一种柔性状态。 最终一致性：事务在操作过程中可能会由于同步延迟等问题导致不一致，但最终状态下，数据都是一致的。 可见，BASE 理论为了支持大型分布式系统，通过牺牲强一致性，保证最终一致性，来获得高可用性，是对 ACID 原则的弱化。具体到今天的三种分布式事务实现方式，二阶段提交、三阶段提交方法，遵循的是 ACID 原则，而消息最终一致性方案遵循的就是 BASE 理论。 分布式锁锁是实现多线程同时访问同一共享资源，保证同一时刻只有一个线程可访问共享资源所做的一种标记 与普通锁不同的是，分布式锁是指分布式环境下，系统部署在多个机器中，实现多进程分布式互斥的一种锁。为了保证多个进程能看到锁，锁被存在公共存储（比如 Redis、Memcache、数据库等三方存储中），以实现多个进程并发访问同一个临界资源，同一时刻只有一个进程可访问共享资源，确保数据的一致性。 分布式锁的三种实现方法及对比接下来，我带你看看实现分布式锁的 3 种主流方法，即： 基于数据库实现分布式锁，这里的数据库指的是关系型数据库； 基于缓存实现分布式锁； 基于 ZooKeeper 实现分布式锁。 基于数据库实现分布式锁当我们要锁住某个资源时，就在该表中增加一条记录，想要释放锁的时候就删除这条记录。 基于数据库实现的分布式锁，是最容易理解的。但是，因为数据库需要落到硬盘上，频繁读取数据库会导致 IO 开销大，因此这种分布式锁适用于并发量低，对性能要求低的场景。 基于数据库实现的分布式锁比较简易，绝招在于创建一张锁表，为申请者在锁表里建立一条记录，记录建立成功则获得锁，消除记录则释放锁。该方法依赖于数据库，主要有两个缺点： 单点故障问题。一旦数据库不可用，会导致整个系统崩溃。 死锁问题。数据库中的数据没有失效时间，获取了锁的服务如果崩溃了 , 没有修改数据库表中的信息将会导致异常 -> 可以使用超时时间进行判断 基于缓存实现分布式锁Redis 通常可以使用 setnx(key, value) 函数来实现分布式锁。key 和 value 就是基于缓存的分布式锁的两个属性，其中 key 表示锁 id，value &#x3D; currentTime + timeOut，表示当前时间 + 超时时间。也就是说，某个进程获得 key 这把锁后，如果在 value 的时间内未释放锁，系统就会主动释放锁。 setnx 函数的返回值有 0 和 1： 返回 1，说明该服务器获得锁，setnx 将 key 对应的 value 设置为当前时间 + 锁的有效时间。 返回 0，说明其他服务器已经获得了锁，进程不能进入临界区。该服务器可以不断尝试 setnx 操作，以获得锁。 相对于基于数据库实现分布式锁的方案来说，基于缓存实现的分布式锁的优势表现在以下几个方面： 性能更好。数据被存放在内存，而不是磁盘，避免了频繁的 IO 操作。 很多缓存可以跨集群部署，避免了单点故障问题。 多缓存服务都提供了可以用来实现分布式锁的方法，比如 Redis 的 setnx 方法等。 可以直接设置超时时间来控制锁的释放，因为这些缓存服务器一般支持自动删除过期数据。 这个方案的不足是，通过超时时间来控制锁的失效时间，并不是十分靠谱，因为一个进程执行时间可能比较长，或受系统进程做内存回收等影响，导致时间超时，从而不正确地释放了锁。 基于 ZooKeeper 实现分布式锁ZooKeeper 基于树形数据存储结构实现分布式锁，来解决多个进程同时访问同一临界资源时，数据的一致性问题。ZooKeeper 的树形数据存储结构主要由 4 种节点构成： 持久节点。这是默认的节点类型，一直存在于 ZooKeeper 中。 持久顺序节点。也就是说，在创建节点时，ZooKeeper 根据节点创建的时间顺序对节点进行编号。 临时节点。与持久节点不同，当客户端与 ZooKeeper 断开连接后，该进程创建的临时节点就会被删除。 临时顺序节点，就是按时间顺序编号的临时节点。 根据它们的特征，ZooKeeper 基于临时顺序节点实现了分布锁。还是以电商售卖吹风机的场景为例。假设用户 A、B、C 同时在 11 月 11 日的零点整提交了购买吹风机的请求，ZooKeeper 会采用如下方法来实现分布式锁： 在与该方法对应的持久节点 shared_lock 的目录下，为每个进程创建一个临时顺序节点。如下图所示，吹风机就是一个拥有 shared_lock 的目录，当有人买吹风机时，会为他创建一个临时顺序节点。 每个进程获取 shared_lock 目录下的所有临时节点列表，注册子节点变更的 Watcher，并监听节点。 每个节点确定自己的编号是否是 shared_lock 下所有子节点中最小的，若最小，则获得锁。例如，用户 A 的订单最先到服务器，因此创建了编号为 1 的临时顺序节点 LockNode1。该节点的编号是持久节点目录下最小的，因此获取到分布式锁，可以访问临界资源，从而可以购买吹风机。 若本进程对应的临时节点编号不是最小的，则分为两种情况： a. 本进程为读请求，如果比自己序号小的节点中有写请求，则等待； b. 本进程为写请求，如果比自己序号小的节点中有读请求，则等待。 例如，用户 B 也想要买吹风机，但在他之前，用户 C 想看看吹风机的库存量。因此，用户 B 只能等用户 A 买完吹风机、用户 C 查询完库存量后，才能购买吹风机。 可以看到，使用 ZooKeeper 可以完美解决设计分布式锁时遇到的各种问题，比如单点故障、不可重入、死锁等问题。虽然 ZooKeeper 实现的分布式锁，几乎能涵盖所有分布式锁的特性，且易于实现，但需要频繁地添加和删除节点，所以性能不如基于缓存实现的分布式锁。 值得注意的是，这里的实现复杂性，是针对同样的分布式锁的实现复杂性，与之前提到的基于数据库的实现非常简易不一样。基于数据库实现的分布式锁存在单点故障和死锁问题，仅仅利用数据库技术去解决单点故障和死锁问题，是非常复杂的。而 ZooKeeper 已定义相关的功能组件，因此可以很轻易地解决设计分布式锁时遇到的各种问题。所以说，要实现一个完整的、无任何缺陷的分布式锁，ZooKeeper 是一个最简单的选择。 从上述分析可看出，为了确保分布式锁的可用性，我们在设计时应考虑到以下几点： 互斥性，即在分布式系统环境下，分布式锁应该能保证一个资源或一个方法在同一时间只能被一个机器的一个线程或进程操作。 具备锁失效机制，防止死锁。即使有一个进程在持有锁的期间因为崩溃而没有主动解锁，也能保证后续其他进程可以获得锁。 可重入性，即进程未释放锁时，可以多次访问临界资源。 有高可用的获取锁和释放锁的功能，且性能要好。 如何解决分布式锁的羊群效应问题？在分布式锁问题中，会经常遇到羊群效应。所谓羊群效应，就是在整个分布式锁的竞争过程中，大量的“Watcher 通知”和“子节点列表的获取”操作重复运行，并且大多数节点的运行结果都是判断出自己当前并不是编号最小的节点，继续等待下一次通知，而不是执行业务逻辑。 这，就会对 ZooKeeper 服务器造成巨大的性能影响和网络冲击。更甚的是，如果同一时间多个节点对应的客户端完成事务或事务中断引起节点消失，ZooKeeper 服务器就会在短时间内向其他客户端发送大量的事件通知。 那如何解决这个问题呢？具体方法可以分为以下三步。 在与该方法对应的持久节点的目录下，为每个进程创建一个临时顺序节点。 每个进程获取所有临时节点列表，对比自己的编号是否最小，若最小，则获得锁。 若本进程对应的临时节点编号不是最小的，则继续判断： 若本进程为读请求，则向比自己序号小的最后一个写请求节点注册 watch 监听，当监听到该节点释放锁后，则获取锁； 若本进程为写请求，则向比自己序号小的最后一个请求节点注册 watch 监听，当监听到该节点释放锁后，获取锁。","date":"2024-12-31","categories":["新博客"]},{"title":"分布式计算","url":"/2024/12/31/分布式计算/","content":"分布式计算模式之MR","date":"2024-12-31","categories":["新博客"]},{"title":"分布式资源管理与负载调度","url":"/2024/12/31/分布式资源管理与负载调度/","content":"分布式体系结构之集中式结构：一人在上，万人在下很多场景下，我们的请求都会汇总到一台服务器上，由这台服务器统一协调我们的请求和其他服务器之间的关系。这种由一台服务器统一管理其他服务器的方式，就是分布式体系结构中的集中式结构（也称为 Master&#x2F;Slave 架构），其中统一管理其他服务器的服务器是主，其他服务器是从，可以形象地比喻为“一人在上，万人在下”。 集中式结构就是，由一台或多台服务器组成中央服务器，系统内的所有数据都存储在中央服务器中，系统内所有的业务也均先由中央服务器处理。多个节点服务器与中央服务器连接，并将自己的信息汇报给中央服务器，由中央服务器统一进行资源和任务调度：中央服务器根据这些信息，将任务下达给节点服务器；节点服务器执行任务，并将结果反馈给中央服务器。 集中式结构最大的特点，就是部署结构简单。这是因为，集中式系统的中央服务器往往是多个具有较强计算能力和存储能力的计算机，为此中央服务器进行统一管理和调度任务时，无需考虑对任务的多节点部署，而节点服务器之间无需通信和协作，只要与中央服务器通信协作即可，具体示意图如下所示： 经典集中式结构Borg 是 Google 内部使用的集群管理系统，采用了典型的集中式结构，负责提交、调度、开始、重启和管理 Google 运行在其上的所有应用。 在 Borg 中，一个集群称为一个 Cell，每个 Cell 里面有一个 Leader，称为 BorgMaster，即为中央服务器；其他服务器为节点服务器或从服务器，被称为 Borglet。 首先，我们一起看看 BorgMaster。它由两个进程组成，一个是 Borgmaster 主进程，一个是独立的 scheduler 进程： 主进程处理客户端的 RPC 请求，比如任务的执行状态更新或者查询等；同时，管理系统中所有实体的状态（比如，服务器、任务等），并负责和 Borglet 通信。 scheduler 进程负责任务调度，通过任务对资源的需求以及当前 Borglet 所在服务器的资源情况进行匹配，为任务寻找一个合适的节点服务器执行。我会在第 11 篇文章“分布式调度之单体调度：物质文明、精神文明一手抓”中与你详细讲述具体的调度流程。 接下来，我们一起看看 Borglet。它是运行在每个节点机器的一个 agent，负责任务的拉起、停止、重启等，并管理和搜集本服务器资源，将任务的状态、服务器状态等信息上报给 BorgMaster。而 BorgMaster 会周期性地轮询每个 Borglet，以获取节点服务器的状态和资源信息等。 Borg 的整体架构示意图如下所示： Borg 的主要用户是 Google 的开发者以及运行 Google 应用和服务的系统管理员（网站可靠性工程师，简称 SRE）。用户以 Job 的形式向 Borg 提交工作，每个 Job 由运行一个或多个运行相同程序的 Task 组成。每个 Job 运行在一个 Borg Cell 中，并将一组机器当做一个单元进行管理。 Borg 可以运行各种各样的任务，这些任务主要分为两类： 第一类是长服务，即长时间运行不停止的服务，并且要求能够处理短暂的、延迟敏感的请求（延迟要求在几微秒到几百毫秒之间）。这些服务主要用于面向终端用户的服务（比如 Gmail、Google Docs、Web 搜索），以及内部的一些基础设施服务（比如 BigTable）。 第二类是批处理任务。通常需要几秒到几天的时间来完成的批处理 Job，这些 Job 对短时间的性能波动并不是非常敏感。 这些负载通常在 Cell 之间混合分布，每个 Cell 随着主要租户以及时间的不同会运行各种不同的应用：批处理类型的 Job 来了又走，而许多面向终端用户的 Job 又期望一个能长时间使用的模式。 对于这些不同的服务，要求 Borg 能很好地处理所有的情况。Borg 主要有三大优点： 开发者只需关注应用，不需要关注底层资源管理。它隐藏了资源管理以及错误处理，因此用户能集中精力开发应用。 高可靠性和可用性，支持多种应用。 支持上千级服务器的管理和运行。 Borg 并不是第一个解决这些问题的系统，但却是少数能在这么大规模处理这些问题的同时，还能实现这样的弹性和完整性的系统之一。 KubernetesKubernetes 是 Google 开源的容器集群管理系统，是 Borg 的一个开源版本。Kubernetes 是用于自动部署、扩展和管理容器化应用程序的开源系统。其核心是，在集群的节点上运行容器化应用，可以进行自动化容器操作，包括部署、调度和在节点间弹性伸缩等。 Kubernetes 也是典型的集中式结构，一个 Kubernetes 集群，主要由 Master 节点和 Worker 节点组成，以及客户端命令行工具 kubectl 和其他附加项。 我们先来看看 Master 节点。它运行在中心服务器上，Master 节点由 API Server、Scheduler、Cluster State Store 和 Control Manger Server 组成，负责对集群进行调度管理。 API Server：是所有 REST 命令的入口，负责处理 REST 的操作，确保它们生效，并执行相关业务逻辑。 Scheduler：根据容器需要的资源以及当前 Worker 节点所在节点服务器的资源信息，自动为容器选择合适的节点服务器。 Cluster State Store：集群状态存储，默认采用 etcd，etcd 是一个分布式 key-value 存储，主要用来做共享配置和服务发现。 Control Manager：用于执行大部分的集群层次的功能，比如执行生命周期功能（命名空间创建和生命周期、事件垃圾收集、已终止垃圾收集、级联删除垃圾收集等）和 API 业务逻辑。 接下来，我们看看 Worker 节点吧。它作为真正的工作节点，运行在从节点服务器，包括 kubelet 和 kube-proxy 核心组件，负责运行业务应用的容器。 kubelet：用于通过命令行与 API Server 进行交互，根据接收到的请求对 Worker 节点进行操作。也就是说，通过与 API Server 进行通信，接收 Master 节点根据调度策略发出的请求或命令，在 Worker 节点上管控容器（Pod），并管控容器的运行状态（比如，重新启动出现故障的 Pod）等。Pod 是 Kubernetes 的最小工作单元，每个 Pod 包含一个或多个容器。 kube-proxy：负责为容器（Pod）创建网络代理 &#x2F; 负载平衡服务，从 API Server 获取所有 Server 信息，并根据 Server 信息创建代理服务，这种代理服务称之为 Service。Kube-proxy 主要负责管理 Service 的访问入口，即实现集群内的 Pod 客户端访问 Service，或者是集群外访问 Service，具有相同服务的一组 Pod 可抽象为一个 Service。每个 Service 都有一个虚拟 IP 地址（VIP）和端口号供客户端访问。 Kubernetes 架构示意图如下所示： 图中， Kube DNS 负责为整个集群提供 DNS 服务；CNI 是 Container Network Interface 的一个标准的通用接口，用于连接容器管理系统和网络插件。 与 Borg 不同的是，Kubernetes 主要是一个容器编排引擎，不仅支持 Docker，还支持 Rocket(另一种容器技术)。 Kubernetes 也已经被很多公司采用，比如网易云、华为在需要使用容器进行资源隔离以运行相关业务的场景下，采用了大规模 Kubernetes 集群。 在容器管理方面，Kubernetes 有很多优势。 自动化容器的部署和复制。Kubernetes 执行容器编排，因此不必人工编写这些任务的脚本。 将容器组织为组，弹性伸缩。Kubernetes 引入 Pod 机制，Pod 代表着能够作为单一应用程序加以控制的一组容器集合。通过 Pod 机制，Kubernetes 实现了多个容器的协作，能够有效避免将太多功能集中到单一容器镜像这样的错误实践中。此外，软件可以向外扩展跨越多个 Pods 实现初步部署，且相关部署可随时进行规模伸缩。 容器间负载均衡。Services 用于将具备类似功能的多个 Pod 整合为一组，可轻松进行配置以实现其可发现性、可观察性、横向扩展以及负载均衡。 易于版本控制与滚动更新。Kubernetes 采取“滚动方式”实现编排，且可跨越部署范围内的全部 Pod。这些滚动更新可进行编排，并以预定义方式配合当前可能尚不可用的 Pods 数量，以及暂时存在的闲置 Pods 数量。Kubernetes 利用新的应用程序镜像版本对已部署 Pods 进行更新，并在发现当前版本存在不稳定问题时回滚至早期部署版本。 Mesos理解了 Google Borg 和 Kubernetes 的集中式结构，接下来我们再看看 Apache 旗下的开源分布式资源管理框架 Mesos 吧。它被称为是分布式系统的内核，最初由加州大学伯克利分校的 AMPLab 开发，后在 Twitter 得到广泛使用。 Mesos 的开发受到了 Borg 系统的启发，也是采用的典型的集中式架构。Mesos 与 Borg 不同之处在于，Borg 的 Master 直接对接用户应用，也就是说用户可以向 Borg 的 Master 直接请求任务。但 Mesos 不可以，Mesos 只负责底层资源的管理和分配，并不涉及存储、 任务调度等功能，因此 Mesos Master 对接的是 Spark、Hadoop、Marathon 等框架，用户的任务需要提交到这些框架上。也正因为此，Mesos 的任务调度框架是双层结构。 在 Mesos 中，一个集群包括 Mesos Master 和多个 Mesos Agent。其中，Mesos Master 运行在中央服务器，Mesos Agent 运行在节点服务器上。 Mesos Master 负责收集和管理所有 Agent 所在服务器的资源和状态，并且对接 Spark、Hadoop 等框架，将集群中服务器的资源信息告知给这些框架，以便这些框架进行任务资源匹配和调度。Mesos Agent 负责任务的拉起、停止、重启等，并负责收集所在服务器的资源 (比如 CPU、内存等) 信息和状态，上报给 Mesos Master。 Mesos Master 通常采用一主两备的方式，以方便故障处理和恢复。而 Mesos Master 的选主策略，采用的就是 ZAB 算法。 Mesos 架构示意图如下所示： 如上所述，Mesos 对接的是框架，并且可以同时对接多个框架，目前已经被很多公司使用。比如，国外的 Twitter、Apple、Airbnb、Uber 等，国内的爱奇艺、去哪儿、携程、当当等。 这些公司选择 Mesos，主要是因为它具有如下优势： 效率。Mesos 对物理资源进行了逻辑抽象，在应用层而不是物理层分配资源，通过容器而不是虚拟机（VM）分配任务。因为应用程序的调度器知道如何最有效地利用资源，所以在应用层分配资源能够为每个应用程序的特殊需求做考量 ; 而通过容器分配任务则能更好地进行“装箱”。 可扩展性。Mesos 可扩展设计的关键是两级调度架构，其中 Framework 进行任务调度，Mesos Master 进行资源分配。由于 Master 不必知道每种类型的应用程序背后复杂的调度逻辑，不必为每个任务做调度，因此可以用非常轻量级的代码实现，更易于扩展集群规模。 模块化。每接入一种新的框架，Master 无需增加新的代码，并且 Agent 模块可以复用，为此开发者可以专注于应用和框架的选择。这，就使得 Mesos 可以支持多种框架，适应不同的应用场景。 随着分布式应用程序和微服务的流行，越来越多的用户正在寻找一种技术，来帮助他们管理这些复杂的应用程序。而 Mesos 为数据中心带来的这些好处，就使得越来越多的人关注 Mesos 及其相关项目。 引申: 知识扩展：Mesos 是如何支持容器部署的？Mesos 本身只负责资源管理，不负责任务调度。但 Mesos 可以对接不同的框架，Mesos+Marathon 可以支持容器调度和部署。Marathon 支持容器的调度，将容器部署请求发给 Mesos Master，Mesos Master 再将请求转发给 Mesos Agent，Mesos Agent 的执行器会将容器拉起。 目前，Mesos+Marathon 支持的容器，主要包括 Docker 和 cgroups。","date":"2024-12-31","categories":["新博客"]},{"title":"如何设计一个高并发系统","url":"/2024/12/31/如何设计一个高并发系统/","content":"高并发架构设计都有哪些关键点？秒杀其实主要解决两个问题，一个是并发读，一个是并发写 其实，秒杀的整体架构可以概括为“稳、准、快”几个关键字 然后就是“准”，就是秒杀 10 台 iPhone，那就只能成交 10 台，多一台少一台都不行。一旦库存不对，那平台就要承担损失，所以“准”就是要求保证数据的一致性。 最后再看“快”，“快”其实很好理解，它就是说系统的性能要足够高，否则你怎么支撑这么大的流量呢？不光是服务端要做极致的性能优化，而且在整个请求链路上都要做协同的优化，每个地方快一点，整个系统就完美了。 所以从技术角度上看“稳、准、快”，就对应了我们架构上的高可用、一致性和高性能的要求，我们的专栏也将主要围绕这几个方面来展开，具体如下。 高性能。 秒杀涉及大量的并发读和并发写，因此支持高并发访问这点非常关键。本专栏将从设计数据的动静分离方案、热点的发现与隔离、请求的削峰与分层过滤、服务端的极致优化这 4 个方面重点介绍。 一致性。 秒杀中商品减库存的实现方式同样关键。可想而知，有限数量的商品在同一时刻被很多倍的请求同时来减库存，减库存又分为“拍下减库存”“付款减库存”以及预扣等几种，在大并发更新的过程中都要保证数据的准确性，其难度可想而知。因此，我将用一篇文章来专门讲解如何设计秒杀减库存方案。 高可用。 虽然我介绍了很多极致的优化思路，但现实中总难免出现一些我们考虑不到的情况，所以要保证系统的高可用和正确性，我们还要设计一个 PlanB 来兜底，以便在最坏情况发生时仍然能够从容应对。专栏的最后，我将带你思考可以从哪些环节来设计兜底方案。 设计高并发系统时应该注意的5个架构原则 高并发系统本质上就是一个满足大并发、高性能和高可用的分布式系统 设计原则数据操作要尽量少 请求的数据能少就少。请求的数据包括上传给系统的数据和系统返回给用户的数据 返回的数据能少就少, 减少后端序列化的时间 数据库操作能少就少 , 这个就不说了 请求数要尽量少这个就很直观了 , 请求数少并发量就少 调用链路要尽量短高并发系统 1. 要降低系统依赖 , 防止因为依赖造成的各种问题 , 提高可用性 2. 降低流量入侵 , 大流量尽量隔绝在外面 不要有单点系统中的单点可以说是系统架构上的一个大忌，因为单点意味着没有备份，风险不可控 , 其次流量不能分发像redis这种会有热点数据问题 一些基本案例其实构建一个高并发系统并没有那么复杂 , 有一下的几个方法可以扛住比较高的并发 把高并发系统独立出来单独打造一个系统，这样可以有针对性地做优化，例如这个独立出来的系统就减少了店铺装修的功能，减少了页面的复杂度 在系统部署上也独立做一个机器集群，这样秒杀的大流量就不会影响到正常的商品购买集群的机器负载 将热点数据（如库存数据）单独放到一个缓存系统中，以提高“读性能” 增加秒杀答题，防止有秒杀器抢单 一些数据放入cdn中进行缓存 使用本地缓存部分数据 对电商来说系统差不多是这种样子 : 高并发系统场景优化1 - 动静分离所谓动静分离 , 就是将一些不常变化 , 可以静态化 , 无状态 , 不需要逻辑处理的一些字段放在一个专门的系统或者地方 , 获取的时候不需要走后端系统的方法 动静分离原则1. 把静态数据缓存到离用户最近的地方常见的就三种 , 用户浏览器里、CDN 上 或者 在服务端的 Cache 中 2. 缓存http链接信息减少解析过程Web 代理服务器根据请求URL查找缓存,直接取出对应的 HTTP 响应头和响应体然后直接返回，这个响应过程简单得连 HTTP 协议都不用重新组装，甚至连 HTTP 请求头也不需要解析 如何动静分离改造静态数据处理方法 URL 唯一化 : 为啥要 URL 唯一呢？前面说了我们是要缓存整个 HTTP 连接，那么以什么作为 Key 呢？就以 URL 作为缓存的 Key，例如以 id&#x3D;xxx 这个格式进行区分 分离浏览者相关的因素。浏览者相关的因素包括是否已登录，以及登录身份等，这些相关因素我们可以单独拆分出来，通过动态请求来获取 分离时间因素。服务端输出的时间也通过动态请求获取。 异步化地域因素。详情页面上与地域相关的因素做成异步方式获取，当然你也可以通过动态请求方式获取，只是这里通过异步获取更合适。 去掉 Cookie。服务端输出的页面包含的 Cookie 可以通过代码软件来删除，如 Web 服务器 Varnish 可以通过 unset req.http.cookie 命令去掉 Cookie。注意，这里说的去掉 Cookie 并不是用户端收到的页面就不含 Cookie 了，而是说，在缓存的静态数据中不含有 Cookie 动态数据处理方法这个没有什么好办法 , 动态数据一定会将流量打到后端 , 所以尽可能的减少这部分 , 如果不行就加机器 动静分离的几种架构方案有 3 种方案可选： 单机本地cache层 统一 Cache 层 上 CDN。 1. 单机本地cache层就是使用内存缓存比如java的ehcache等 优点 缺点 无网络开销 占用内存大 使用简单 同步机制需要使用其他方法保证 统一 Cache 层典型的就是redis集群 优点 缺点 StartFragment单独一个 Cache 层，可以减少多个应用接入时使用 Cache 的成本。这样接入的应用只要维护自己的 Java 系统就好，不需要单独维护 Cache，而只关心如何使用即可 EndFragment StartFragmentCache 层内部交换网络成为瓶颈 EndFragment StartFragment统一 Cache 的方案更易于维护，如后面加强监控、配置的自动化，只需要一套解决方案就行，统一起来维护升级也比较方便。 EndFragment StartFragment缓存服务器的网卡也会是瓶颈； EndFragment StartFragment可以共享内存，最大化利用内存，不同系统之间的内存可以动态切换，从而能够有效应对各种攻击。 EndFragment StartFragment机器少风险较大，挂掉一台就会影响很大一部分缓存数据。 EndFragment 要解决上面这些问题，可以再对 Cache 做 Hash 分组，即一组 Cache 缓存的内容相同，这样能够避免热点数据过度集中导致新的瓶颈产生。 比如redis 热点数据分组 上 CDN在将整个系统做动静分离后，我们自然会想到更进一步的方案，就是将 Cache 进一步前移到 CDN 上，因为 CDN 离用户最近，效果会更好 有以下几个问题需要解决 失效问题。前面我们也有提到过缓存时效的问题，不知道你有没有理解，我再来解释一下。谈到静态数据时，我说过一个关键词叫“相对不变”，它的言外之意是“可能会变化”。比如一篇文章，现在不变，但如果你发现个错别字，是不是就会变化了？如果你的缓存时效很长，那用户端在很长一段时间内看到的都是错的。所以，这个方案中也是，我们需要保证 CDN 可以在秒级时间内，让分布在全国各地的 Cache 同时失效，这对 CDN 的失效系统要求很高 命中率问题。Cache 最重要的一个衡量指标就是“高命中率”，不然 Cache 的存在就失去了意义。同样，如果将数据全部放到全国的 CDN 上，必然导致 Cache 分散，而 Cache 分散又会导致访问请求命中同一个 Cache 的可能性降低，那么命中率就成为一个问题。 发布更新问题。如果一个业务系统每周都有日常业务需要发布，那么发布系统必须足够简洁高效，而且你还要考虑有问题时快速回滚和排查问题的简便性。 因为上面的这些问题 , 所以cdn的部署方法一般都是分网络分区域的中心化部署 高并发系统场景优化2 - 处理热点要关注热点首先，热点请求会大量占用服务器处理资源，虽然这个热点可能只占请求总量的亿分之一，然而却可能抢占 90% 的服务器资源，如果这个热点请求还是没有价值的无效请求，那么对系统资源来说完全是浪费。 其次，即使这些热点是有效的请求，我们也要识别出来做针对性的优化，从而用更低的代价来支撑这些热点请求 热点操作和热点数据所谓“热点操作”，例如大量的刷新页面、大量的添加购物车、双十一零点大量的下单等都属于此类操作。对系统来说，这些操作可以抽象为“读请求”和“写请求”，这两种热点请求的处理方式大相径庭，读请求的优化空间要大一些，而写请求的瓶颈一般都在存储层，优化的思路就是根据 CAP 理论做平衡 热点数据”比较好理解，那就是用户的热点请求对应的数据。而热点数据又分为“静态热点数据”和“动态热点数据” 静态热点数据和动态热点数据所谓“静态热点数据”，就是能够提前预测的热点数据。例如，我们可以通过卖家报名的方式提前筛选出来，通过报名系统对这些热点商品进行打标。另外，我们还可以通过大数据分析来提前发现热点商品，比如我们分析历史成交记录、用户的购物车记录，来发现哪些商品可能更热门、更好卖，这些都是可以提前分析出来的热点 所谓“动态热点数据”，就是不能被提前预测到的，系统在运行过程中临时产生的热点。例如，卖家在抖音上做了广告，然后商品一下就火了，导致它在短时间内被大量购买 我们如何处理热点数据发现热点数据热点数据静态发现静态热点数据可以通过商业手段，例如强制让卖家通过报名参加的方式提前把热点商品筛选出来，实现方式是通过一个运营系统，把参加活动的商品数据进行打标，然后通过一个后台系统对这些热点商品进行预处理，如提前进行缓存 . 或者使用技术手段提前预测，例如对买家每天访问的商品进行大数据计算，然后统计出 TOP N 的商品，我们可以认为这些 TOP N 的商品就是热点商品。 热点数据动态发现主要处理动态热点数据的 , 都是使用技术手段实现的 构建一个异步的系统，它可以收集交易链路上各个环节中的中间件产品的热点 Key，如 Nginx、缓存、RPC 服务框架等这些中间件（一些中间件产品本身已经有热点统计模块）。 建立一个热点上报和可以按照需求订阅的热点服务的下发规范，主要目的是通过交易链路上各个系统（包括详情、购物车、交易、优惠、库存、物流等）访问的时间差，把上游已经发现的热点透传给下游系统，提前做好保护。比如，对于大促高峰期，详情系统是最早知道的，在统一接入层上 Nginx 模块统计的热点 URL。 将上游系统收集的热点数据发送到热点服务台，然后下游系统（如交易系统）就会知道哪些商品会被频繁调用，然后做热点保护。 这里我给出了一个图，其中用户访问商品时经过的路径有很多，我们主要是依赖前面的导购页面（包括首页、搜索页面、商品详情、购物车等）提前识别哪些商品的访问量高，通过这些系统中的中间件来收集热点数据，并记录到日志中。 我们通过部署在每台机器上的 Agent 把日志汇总到聚合和分析集群中，然后把符合一定规则的热点数据，通过订阅分发系统再推送到相应的系统中。你可以是把热点数据填充到 Cache 中，或者直接推送到应用服务器的内存中，还可以对这些数据进行拦截，总之下游系统可以订阅这些数据，然后根据自己的需求决定如何处理这些数据。 打造热点发现系统时，我根据以往经验总结了几点注意事项。 这个热点服务后台抓取热点数据日志最好采用异步方式，因为“异步”一方面便于保证通用性，另一方面又不影响业务系统和中间件产品的主流程。 热点服务发现和中间件自身的热点保护模块并存，每个中间件和应用还需要保护自己。热点服务台提供热点数据的收集和订阅服务，便于把各个系统的热点数据透明出来。 热点发现要做到接近实时（3s 内完成热点数据的发现），因为只有做到接近实时，动态发现才有意义，才能实时地对下游系统提供保护。 处理热点数据处理热点数据通常有几种思路：一是优化，二是限制，三是隔离。 优化优化热点数据最有效的办法就是缓存热点数据，如果热点数据做了动静分离，那么可以长期缓存静态数据。但是，缓存热点数据更多的是“临时”缓存，即不管是静态数据还是动态数据，都用一个队列短暂地缓存数秒钟，由于队列长度有限，可以采用 LRU 淘汰算法替换。 限制限制更多的是一种保护机制，限制的办法也有很多，例如对被访问商品的 ID 做一致性 Hash，然后根据 Hash 做分桶，每个分桶设置一个处理队列，这样可以把热点商品限制在一个请求队列里，防止因某些热点商品占用太多的服务器资源，而使其他请求始终得不到服务器的处理资源。 隔离高并发系统设计的第一个原则就是将这种热点数据隔离出来，不要让 1% 的请求影响到另外的 99%，隔离出来后也更方便对这 1% 的请求做针对性的优化。 具体到“秒杀”业务，我们可以在以下几个层次实现隔离。 业务隔离。把秒杀做成一种营销活动，卖家要参加秒杀这种营销活动需要单独报名，从技术上来说，卖家报名后对我们来说就有了已知热点，因此可以提前做好预热。 系统隔离。系统隔离更多的是运行时的隔离，可以通过分组部署的方式和另外 99% 分开。秒杀可以申请单独的域名，目的也是让请求落到不同的集群中。 数据隔离。秒杀所调用的数据大部分都是热点数据，比如会启用单独的 Cache 集群或者 MySQL 数据库来放热点数据，目的也是不想 0.01% 的数据有机会影响 99.99% 数据。 当然了，实现隔离有很多种办法。比如，你可以按照用户来区分，给不同的用户分配不同的 Cookie，在接入层，路由到不同的服务接口中；再比如，你还可以在接入层针对 URL 中的不同 Path 来设置限流策略。服务层调用不同的服务接口，以及数据层通过给数据打标来区分等等这些措施，其目的都是把已经识别出来的热点请求和普通的请求区分开 高并发系统场景优化3 - 流量削峰为什么要削峰我们知道服务器的处理资源是恒定的，你用或者不用它的处理能力都是一样的，所以出现峰值的话，很容易导致忙到处理不过来，闲的时候却又没有什么要处理。但是由于要保证服务质量，我们的很多处理资源只能按照忙的时候来预估，而这会导致资源的一个浪费 流量削峰的一些操作思路：排队、答题、分层过滤排队要对流量进行削峰，最容易想到的解决方案就是用消息队列来缓冲瞬时流量，把同步的直接调用转换成异步的间接推送，中间通过一个队列在一端承接瞬时的流量洪峰，在另一端平滑地将消息推送出去 如果流量峰值持续一段时间达到了消息队列的处理上限，例如本机的消息积压达到了存储空间的上限，消息队列同样也会被压垮，这样虽然保护了下游的系统，但是和直接把请求丢弃也没多大的区别 消息队列，类似的排队方式还有很多，例如： 利用线程池加锁等待也是一种常用的排队方式； 先进先出、先进后出等常用的内存排队算法的实现方式； 把请求序列化到文件中，然后再顺序地读文件（例如基于 MySQL binlog 的同步机制）来恢复请求等方式。 可以看到，这些方式都有一个共同特征，就是把“一步的操作”变成“两步的操作”，其中增加的一步操作用来起到缓冲的作用。 答题增加答题其实有很多目的 第一个目的是防止部分买家使用秒杀器在参加秒杀时作弊 , 过滤无用请求 延缓请求，起到对请求流量进行削峰的作用，从而让系统能够更好地支持瞬时的流量高峰 分层过滤前面介绍的排队和答题要么是少发请求，要么对发出来的请求进行缓冲，而针对秒杀场景还有一种方法，就是对请求进行分层过滤，从而过滤掉一些无效的请求。分层过滤其实就是采用“漏斗”式设计来处理请求的 假如请求分别经过 CDN、前台读系统（如商品详情系统）、后台系统（如交易系统）和数据库这几层，那么： 大部分数据和流量在用户浏览器或者 CDN 上获取，这一层可以拦截大部分数据的读取； 经过第二层（即前台系统）时数据（包括强一致性的数据）尽量得走 Cache，过滤一些无效的请求； 再到第三层后台系统，主要做数据的二次检验，对系统做好保护和限流，这样数据量和请求就进一步减少； 最后在数据层完成数据的强一致性校验。 分层过滤的核心思想是：在不同的层次尽可能地过滤掉无效请求，让“漏斗”最末端的才是有效请求。而要达到这种效果，我们就必须对数据做分层的校验。 分层校验的基本原则是： 将动态请求的读数据缓存（Cache）在 Web 端，过滤掉无效的数据读； 对读数据不做强一致性校验，减少因为一致性校验产生瓶颈的问题； 对写数据进行基于时间的合理分片，过滤掉过期的失效请求； 对写请求做限流保护，将超出系统承载能力的请求过滤掉； 对写数据进行强一致性校验，只保留最后有效的数据。 分层校验的目的是：在读系统中，尽量减少由于一致性校验带来的系统瓶颈，但是尽量将不影响性能的检查条件提前，如用户是否具有秒杀资格、商品状态是否正常、用户答题是否正确、秒杀是否已经结束、是否非法请求、营销等价物是否充足等；在写数据系统中，主要对写的数据（如“库存”）做一致性检查，最后在数据库层保证数据的最终准确性（如“库存”不能减为负数）。 高并发系统场景优化4 - 类库存扣减场景库存场景非常典型 , 是高并发情况下对数据进行读写操作的场景 先说一下场景 总结来说，减库存操作一般有如下几个方式： 下单减库存，即当买家下单后，在商品的总库存中减去买家购买数量。下单减库存是最简单的减库存方式，也是控制最精确的一种，下单时直接通过数据库的事务机制控制商品库存，这样一定不会出现超卖的情况。但是你要知道，有些人下完单可能并不会付款。 付款减库存，即买家下单后，并不立即减库存，而是等到有用户付款后才真正减库存，否则库存一直保留给其他买家。但因为付款时才减库存，如果并发比较高，有可能出现买家下单后付不了款的情况，因为可能商品已经被其他人买走了。 预扣库存，这种方式相对复杂一些，买家下单后，库存为其保留一定的时间（如 10 分钟），超过这个时间，库存将会自动释放，释放后其他买家就可以继续购买。在买家付款前，系统会校验该订单的库存是否还有保留：如果没有保留，则再次尝试预扣；如果库存不足（也就是预扣失败）则不允许继续付款；如果预扣成功，则完成付款并实际地减去库存。 “下单减库存”在数据一致性上，主要就是保证大并发请求时库存数据不能为负数，也就是要保证数据库中的库存字段值不能为负数，一般我们有多种解决方案 一种是在应用程序中通过事务来判断，即保证减后库存不能为负数，否则就回滚； 直接设置数据库的字段数据为无符号整数，这样减后库存字段值小于零时会直接执行 SQL 语句来报错 再有一种就是使用 CASE WHEN 判断语句，例如这样的 SQL 语句：UPDATE item SET inventory &#x3D; CASE WHEN inventory >&#x3D; xxx THEN inventory-xxx ELSE inventory END 秒杀减库存的极致优化在交易环节中，“库存”是个关键数据，也是个热点数据，因为交易的各个环节中都可能涉及对库存的查询。但是，我在前面介绍分层过滤时提到过，秒杀中并不需要对库存有精确的一致性读，把库存数据放到缓存（Cache）中，可以大大提升读性能。 解决大并发读问题，可以采用 LocalCache（即在秒杀系统的单机上缓存商品相关的数据）和对数据进行分层过滤的方式，但是像减库存这种大并发写无论如何还是避免不了，这也是秒杀场景下最为核心的一个技术难题。 因此，这里我想专门来说一下秒杀场景下减库存的极致优化思路，包括如何在缓存中减库存以及如何在数据库中减库存。 比如使用redis , 其实我们可以使用lua脚本来保证一致性 但是使用redis 有一定的局限性如果你的秒杀商品的减库存逻辑非常单一，比如没有复杂的 SKU 库存和总库存这种联动关系,或者多组sku同时扣减的这种不涉及复杂事务的场景，我觉得完全可以. 如果涉及到多组扣减 , 如果有比较复杂的减库存逻辑，或者需要使用事务，还是建议必须在数据库中完成减库存-> 或者缓存支持事务 由于 MySQL 存储数据的特点，同一数据在数据库里肯定是一行存储（MySQL），因此会有大量线程来竞争 InnoDB 行锁，而并发度越高时等待线程会越多，TPS（Transaction Per Second，即每秒处理的消息数）会下降，响应时间（RT）会上升，数据库的吞吐量就会严重受影响 这就可能引发一个问题，就是单个热点商品会影响整个数据库的性能， 导致 0.01% 的商品影响 99.99% 的商品的售卖，这是我们不愿意看到的情况。一个解决思路是遵循前面介绍的原则进行隔离，把热点商品放到单独的热点库中。但是这无疑会带来维护上的麻烦，比如要做热点数据的动态迁移以及单独的数据库等 而分离热点商品到单独的数据库还是没有解决并发锁的问题，我们应该怎么办呢？要解决并发锁的问题，有两种办法： 应用层做排队。按照商品维度设置队列顺序执行，这样能减少同一台机器对数据库同一行记录进行操作的并发度，同时也能控制单个商品占用数据库连接的数量，防止热点商品占用太多的数据库连接。 数据库层做排队。应用层只能做到单机的排队，但是应用机器数本身很多，这种排队方式控制并发的能力仍然有限，所以如果能在数据库层做全局排队是最理想的。阿里的数据库团队开发了针对这种 MySQL 的 InnoDB 层上的补丁程序（patch），可以在数据库层上对单行记录做到并发排队。 你可能有疑问了，排队和锁竞争不都是要等待吗，有啥区别？如果熟悉 MySQL 的话，你会知道 InnoDB 内部的死锁检测，以及 MySQL Server 和 InnoDB 的切换会比较消耗性能，淘宝的 MySQL 核心团队还做了很多其他方面的优化，如 COMMIT_ON_SUCCESS 和 ROLLBACK_ON_FAIL 的补丁程序，配合在 SQL 里面加提示（hint），在事务里不需要等待应用层提交（COMMIT），而在数据执行完最后一条 SQL 后，直接根据 TARGET_AFFECT_ROW 的结果进行提交或回滚，可以减少网络等待时间（平均约 0.7ms）。据我所知，目前阿里 MySQL 团队已经将包含这些补丁程序的 MySQL 开源。另外，数据更新问题除了前面介绍的热点隔离和排队处理之外，还有些场景（如对商品的 lastmodifytime 字段的）更新会非常频繁，在某些场景下这些多条 SQL 是可以合并的，一定时间内只要执行最后一条 SQL 就行了，以便减少对数据库的更新操作。 高并发系统场景优化4 - 兜底方案高并发系统为了保证系统的高可用，我们必须设计一个 Plan B 方案来兜底 高可用建设应该从哪里着手说到系统的高可用建设，它其实是一个系统工程，需要考虑到系统建设的各个阶段，也就是说它其实贯穿了系统建设的整个生命周期，如下图所示： 具体来说，系统的高可用建设涉及架构阶段、编码阶段、测试阶段、发布阶段、运行阶段，以及故障发生时。接下来，我们分别看一下。 架构阶段：架构阶段主要考虑系统的可扩展性和容错性，要避免系统出现单点问题。例如多机房单元化部署，即使某个城市的某个机房出现整体故障，仍然不会影响整体网站的运转。 编码阶段：编码最重要的是保证代码的健壮性，例如涉及远程调用问题时，要设置合理的超时退出机制，防止被其他系统拖垮，也要对调用的返回结果集有预期，防止返回的结果超出程序处理范围，最常见的做法就是对错误异常进行捕获，对无法预料的错误要有默认处理结果。 测试阶段：测试主要是保证测试用例的覆盖度，保证最坏情况发生时，我们也有相应的处理流程。 发布阶段：发布时也有一些地方需要注意，因为发布时最容易出现错误，因此要有紧急的回滚机制。 运行阶段：运行时是系统的常态，系统大部分时间都会处于运行态，运行态最重要的是对系统的监控要准确及时，发现问题能够准确报警并且报警数据要准确详细，以便于排查问题。 故障发生：故障发生时首先最重要的就是及时止损，例如由于程序问题导致商品价格错误，那就要及时下架商品或者关闭购买链接，防止造成重大资产损失。然后就是要能够及时恢复服务，并定位原因解决问题。 为什么系统的高可用建设要放到整个生命周期中全面考虑？因为我们在每个环节中都可能犯错，而有些环节犯的错，你在后面是无法弥补的。例如在架构阶段，你没有消除单点问题，那么系统上线后，遇到突发流量把单点给挂了，你就只能干瞪眼，有时候想加机器都加不进去。所以高可用建设是一个系统工程，必须在每个环节都做好。 那么针对秒杀系统，我们重点介绍在遇到大流量时，应该从哪些方面来保障系统的稳定运行，所以更多的是看如何针对运行阶段进行处理，这就引出了接下来的内容：降级、限流和拒绝服务。 降级所谓“降级”，就是当系统的容量达到一定程度时，限制或者关闭系统的某些非核心功能，从而把有限的资源保留给更核心的业务。它是一个有目的、有计划的执行过程，所以对降级我们一般需要有一套预案来配合执行。如果我们把它系统化，就可以通过预案系统和开关系统来实现降级。 降级方案可以这样设计：当秒杀流量达到 5w&#x2F;s 时，把成交记录的获取从展示 20 条降级到只展示 5 条。“从 20 改到 5”这个操作由一个开关来实现，也就是设置一个能够从开关系统动态获取的系统参数。 这里，我给出开关系统的示意图。它分为两部分，一部分是开关控制台，它保存了开关的具体配置信息，以及具体执行开关所对应的机器列表；另一部分是执行下发开关数据的 Agent，主要任务就是保证开关被正确执行，即使系统重启后也会生效。 执行降级无疑是在系统性能和用户体验之间选择了前者，降级后肯定会影响一部分用户的体验，例如在双 11 零点时，如果优惠券系统扛不住，可能会临时降级商品详情的优惠信息展示，把有限的系统资源用在保障交易系统正确展示优惠信息上，即保障用户真正下单时的价格是正确的。所以降级的核心目标是牺牲次要的功能和用户体验来保证核心业务流程的稳定，是一个不得已而为之的举措。 限流如果说降级是牺牲了一部分次要的功能和用户的体验效果，那么限流就是更极端的一种保护措施了。限流就是当系统容量达到瓶颈时，我们需要通过限制一部分流量来保护系统，并做到既可以人工执行开关，也支持自动化保护的措施。 这里，我同样给出了限流系统的示意图。总体来说，限流既可以是在客户端限流，也可以是在服务端限流。此外，限流的实现方式既要支持 URL 以及方法级别的限流，也要支持基于 QPS 和线程的限流。 首先，我以内部的系统调用为例，来分别说下客户端限流和服务端限流的优缺点。 客户端限流，好处可以限制请求的发出，通过减少发出无用请求从而减少对系统的消耗。缺点就是当客户端比较分散时，没法设置合理的限流阈值：如果阈值设的太小，会导致服务端没有达到瓶颈时客户端已经被限制；而如果设的太大，则起不到限制的作用。 服务端限流，好处是可以根据服务端的性能设置合理的阈值，而缺点就是被限制的请求都是无效的请求，处理这些无效的请求本身也会消耗服务器资源。 在限流的实现手段上来讲，基于 QPS 和线程数的限流应用最多，最大 QPS 很容易通过压测提前获取，例如我们的系统最高支持 1w QPS 时，可以设置 8000 来进行限流保护。线程数限流在客户端比较有效，例如在远程调用时我们设置连接池的线程数，超出这个并发线程请求，就将线程进行排队或者直接超时丢弃。 限流无疑会影响用户的正常请求，所以必然会导致一部分用户请求失败，因此在系统处理这种异常时一定要设置超时时间，防止因被限流的请求不能 fast fail（快速失败）而拖垮系统。 拒绝服务如果限流还不能解决问题，最后一招就是直接拒绝服务了。 当系统负载达到一定阈值时，例如 CPU 使用率达到 90% 或者系统 load 值达到 2*CPU 核数时，系统直接拒绝所有请求，这种方式是最暴力但也最有效的系统保护方式。例如秒杀系统，我们在如下几个环节设计过载保护： 在最前端的 Nginx 上设置过载保护，当机器负载达到某个值时直接拒绝HTTP 请求并返回 503 错误码，在 Java 层同样也可以设计过载保护。 拒绝服务可以说是一种不得已的兜底方案，用以防止最坏情况发生，防止因把服务器压跨而长时间彻底无法提供服务。像这种系统过载保护虽然在过载时无法提供服务，但是系统仍然可以运作，当负载下降时又很容易恢复，所以每个系统和每个环节都应该设置这个兜底方案，对系统做最坏情况下的保护。 最后,以java系统为例 , 如何提高系统性能我们讨论的主要是系统服务端性能，一般用 QPS（Query Per Second，每秒请求数）来衡量，还有一个影响和 QPS 也息息相关，那就是响应时间（Response Time，RT），它可以理解为服务器处理响应的耗时正常情况下响应时间（RT）越短，一秒钟处理的请求数（QPS）自然也就会越多，这在单线程处理的情况下看起来是线性的关系，即我们只要把每个请求的响应时间降到最低，那么性能就会最高。但是你可能想到响应时间总有一个极限，不可能无限下降，所以又出现了另外一个维度，即通过多线程，来处理请求。这样理论上就变成了“总 QPS &#x3D;（1000ms &#x2F; 响应时间）× 线程数量”，这样性能就和两个因素相关了，一个是一次响应的服务端耗时，一个是处理请求的线程数。 响应时间和QPS的关系对于大部分的 Web 系统而言，响应时间一般都是由 CPU 执行时间和线程等待时间（比如 RPC、IO 等待、Sleep、Wait 等）组成，即服务器在处理一个请求时，一部分是 CPU 本身在做运算，还有一部分是在各种等待。 理解了服务器处理请求的逻辑，估计你会说为什么我们不去减少这种等待时间。很遗憾，根据我们实际的测试发现，减少线程等待时间对提升性能的影响没有我们想象得那么大，它并不是线性的提升关系，这点在很多代理服务器（Proxy）上可以做验证。 如果代理服务器本身没有 CPU 消耗，我们在每次给代理服务器代理的请求加个延时，即增加响应时间，但是这对代理服务器本身的吞吐量并没有多大的影响，因为代理服务器本身的资源并没有被消耗，可以通过增加代理服务器的处理线程数，来弥补响应时间对代理服务器的 QPS 的影响。 其实，真正对性能有影响的是 CPU 的执行时间。这也很好理解，因为 CPU 的执行真正消耗了服务器的资源。经过实际的测试，如果减少 CPU 一半的执行时间，就可以增加一倍的 QPS。 也就是说，我们应该致力于减少 CPU 的执行时间。 线程数对 QPS 的影响单看“总 QPS”的计算公式，你会觉得线程数越多 QPS 也就会越高，但这会一直正确吗？显然不是，线程数不是越多越好，因为线程本身也消耗资源，也受到其他因素的制约。例如，线程越多系统的线程切换成本就会越高，而且每个线程也都会耗费一定内存。 那么，设置什么样的线程数最合理呢？其实很多多线程的场景都有一个默认配置，即“线程数 &#x3D; 2 * CPU 核数 + 1”。除去这个配置，还有一个根据最佳实践得出来的公式： 线程数 &#x3D; [(线程等待时间 + 线程 CPU 时间) &#x2F; 线程 CPU 时间] × CPU 数量 &#x3D;> 这个公式的核心思想就行将等待的时间让给其他线程去处理 当然，最好的办法是通过性能测试来发现最佳的线程数。 如何优化系统对 Java 系统来说，可以优化的地方很多，这里我重点说一下比较有效的几种手段，供你参考，它们是：减少编码、减少序列化。接下来，我们分别来看一下。 1. 减少编码Java 的编码运行比较慢，这是 Java 的一大硬伤。在很多场景下，只要涉及字符串的操作（如输入输出操作、I&#x2F;O 操作）都比较耗 CPU 资源，不管它是磁盘 I&#x2F;O 还是网络 I&#x2F;O，因为都需要将字符转换成字节，而这个转换必须编码。 每个字符的编码都需要查表，而这种查表的操作非常耗资源，所以减少字符到字节或者相反的转换、减少字符编码会非常有成效。减少编码就可以大大提升性能。 那么如何才能减少编码呢？例如，网页输出是可以直接进行流输出的，即用 resp.getOutputStream() 函数写数据，把一些静态的数据提前转化成字节，等到真正往外写的时候再直接用 OutputStream() 函数写，就可以减少静态数据的编码转换。比如 把静态的字符串提前编码成字节并缓存，然后直接输出字节内容到页面，从而大大减少编码的性能消耗的，网页输出的性能比没有提前进行字符到字节转换时提升了 30% 左右。 2. 减少序列化序列化也是 Java 性能的一大天敌，减少 Java 中的序列化操作也能大大提升性能。又因为序列化往往是和编码同时发生的，所以减少序列化也就减少了编码。 序列化大部分是在 RPC 中发生的，因此避免或者减少 RPC 就可以减少序列化，当然当前的序列化协议也已经做了很多优化来提升性能。有一种新的方案，就是可以将多个关联性比较强的应用进行“合并部署”，而减少不同应用之间的 RPC 也可以减少序列化的消耗。 所谓“合并部署”，就是把两个原本在不同机器上的不同应用合并部署到一台机器上，当然不仅仅是部署在一台机器上，还要在同一个 Tomcat 容器中，且不能走本机的 Socket，这样才能避免序列化的产生。","date":"2024-12-31","categories":["新博客"]},{"title":"数据库-重新理解数据库事物,隔离级别","url":"/2024/12/31/数据库-重新理解数据库事物-隔离级别/","content":"说实话之前,知道数据库的隔离级别,但是都是局限在背上面,没有真正思考过,这个重新整理一下数据库隔离级别的东西,顺便带上自己的思考 老生长谈数据库事物四大特性⑴ 原子性（Atomicity） 原子性是指事务包含的所有操作要么全部成功，要么全部失败回滚，这和前面两篇博客介绍事务的功能是一样的概念，因此事务的操作如果成功就必须要完全应用到数据库，如果操作失败则不能对数据库有任何影响。 ⑵ 一致性（Consistency） 一致性是指事务必须使数据库从一个一致性状态变换到另一个一致性状态，也就是说一个事务执行之前和执行之后都必须处于一致性状态。 拿转账来说，假设用户A和用户B两者的钱加起来一共是5000，那么不管A和B之间如何转账，转几次账，事务结束后两个用户的钱相加起来应该还得是5000，这就是事务的一致性。 ⑶ 隔离性（Isolation） 隔离性是当多个用户并发访问数据库时，比如操作同一张表时，数据库为每一个用户开启的事务，不能被其他事务的操作所干扰，多个并发事务之间要相互隔离。 即要达到这么一种效果：对于任意两个并发的事务T1和T2，在事务T1看来，T2要么在T1开始之前就已经结束，要么在T1结束之后才开始，这样每个事务都感觉不到有其他事务在并发地执行。 关于事务的隔离性数据库提供了多种隔离级别，稍后会介绍到。 ⑷ 持久性（Durability） 持久性是指一个事务一旦被提交了，那么对数据库中的数据的改变就是永久性的，即便是在数据库系统遇到故障的情况下也不会丢失提交事务的操作。 例如我们在使用JDBC操作数据库时，在提交事务方法后，提示用户事务操作完成，当我们程序执行完成直到看到提示后，就可以认定事务以及正确提交，即使这时候数据库出现了问题，也必须要将我们的事务完全执行完成，否则就会造成我们看到提示事务处理完毕，但是数据库因为故障而没有执行事务的重大错误。 不注意数据库隔离级别将导致的问题1，脏读脏读是指在一个事务处理过程里读取了另一个未提交的事务中的数据。 2，不可重复读不可重复读是指在对于数据库中的某个数据，一个事务范围内多次查询却返回了不同的数据值，这是由于在查询间隔，被另一个事务修改并提交了。 3，虚读(幻读)幻读是事务非独立执行时发生的一种现象。例如事务T1对一个表中所有的行的某个数据项做了从“1”修改为“2”的操作，这时事务T2又对这个表中插入了一行数据项，而这个数据项的数值还是为“1”并且提交给数据库。而操作事务T1的用户如果再查看刚刚修改的数据，会发现还有一行没有修改，其实这行是从事务T2中添加的，就好像产生幻觉一样，这就是发生了幻读。 幻读和不可重复读都是读取了另一条已经提交的事务（这点就脏读不同），所不同的是不可重复读查询的都是同一个数据项，而幻读针对的是一批数据整体（比如数据的个数）。 其实整理到这里应该就差不多了,以后在添加一些底层实现吧","date":"2024-12-31","categories":["新博客"]},{"title":"设计一个优雅的营销系统的反思","url":"/2024/12/31/设计一个优雅的营销系统的反思/","content":"营销系统构建开发的核心的思路 -> 去设计模式,规则化为什么要去设计模式? -> 设计模式很优秀,但是设计模式是有极限的.设计模式的时候,需要对系统的扩展点有充分的了解才能真正的将设计模式设计好 , 但是作为互联网的营销业务,存在变化快,玩法多,规则复杂,存在特殊等情况 , 导致我们很难真正的预估扩展点到底在哪里 , 导致系统过度设计 , 增加许多复杂逻辑 设计模式是写框架的技术,并不是写业务的技术 为什么要规则化? 其实是一种想法 , 将复杂的业务逻辑简化成一种规则 , 一种流程 ps : 这个东西比较抽象 , 需要经历一定的业务复杂度才能正真的理解 如何用流程化和规则化来设计营销中心1. 营销中心规则梳理 通过上面的各种活动的形式,我们的活动计算逻辑(不包括用户维度限制)可以抽象一个这种模型 将所有的活动的优惠结果想象成一种结论 , 包括 对活动对应的商品单品的金额影响 对活动对应的商品组合的金额影响 对整个订单的金额影响 对活动对应的商品单品的附属商品的影响 对活动对应的商品组合的附属商品的影响 对活动对应整个订单的附属商品的影响 将所有的活动条件想象成一种规则包括 (1) 单商品,多梯度,价格限制 (2) 单商品,多梯度,数量限制 (3) 任意数量 (1) (2) 逻辑运算 抽象成这样, 一个活动的计算流程就会简化成由活动条件到优惠结果或者说规则到结论的一种映射 2. 营销活动类具体的计算流程应该怎么做 本质上就是将各个部分分解 其中每一个handle都可以做水平扩展 , 最大限度的保证逻辑的清晰 这套系统的设计哲学营销本身很复杂 , 所以我们就抛开复杂的营销的活动 , 用基本的模型去抽象匹配它 , 这样我们就能简化计算逻辑 , 实现更灵活 , 更以扩展的营销系统","date":"2024-12-31","categories":["新博客"]},{"title":"oauth2.0-认证授权的深入理解","url":"/2024/12/31/oauth2-0-认证授权的深入理解/","content":"oAuth2.0 是一个广泛应用的认证授权方法 在整个oAuth2.0的认证有以下的参与者 (1)Third-party application：第三方应用程序,称”客户端”(client) (2)HTTP service：HTTP服务提供商,简称”服务提供商” (3)Resource Owner：资源所有者,称”用户”(user). (4)User Agent：用户代理,本文中就是指浏览器. (5)Authorization server：认证服务器,即服务提供商专门用来处理认证的服务器. (6)Resource server：资源服务器,即服务提供商存放用户生成的资源的服务器.它与认证服务器,可以是同一台服务器,也可以是不同的服务器. oAuth2.0的官方流程是这样的 (A)用户打开客户端以后,客户端要求用户给予授权. (B)用户同意给予客户端授权. (C)客户端使用上一步获得的授权openToken,向认证服务器申请令牌accesssToken. (D)认证服务器对客户端进行认证以后,确认无误,同意发放令牌accesssToken. (E)客户端使用令牌,向资源服务器申请获取资源. (F)资源服务器确认令牌无误,同意向客户端开放资源. 其实这个登入流程的核心就是如何获取这个accessToken,这里列一个比较详细的授权流程,做一个整理 这里说明一些关键步骤 第三步 : 我们返回授权服务提供的一个接口,这里重点是这个接口中的参数,我们需要这几个重要的参数 地址唯一标识 : 这个是关联授权地址的唯一id 授权地址 : 这个地址是表明授权平台在认证通过后,将认证的accessToken获取code或者accessToken,发送到哪里的地址 status : 传入一个认证随机数,防止csfr攻击,这种方法需要第三方服务器自己做status校验 注意:这里要进行第一次权限校验,查看这个域名有没有权限进行认证.授权系统应该提供权限配置的功能页面 第十步 : 微信回传accessToken获取code 其实本质上授权系统在这一步就可以返回accessToken,但是微信考虑到,第三步回传的调用地址可能会被篡改或code被偷取(比如没有使用https加密),所以这里在规范的Oauth2.0 认证体系外又添加一个流程 但是还有一种情况没有考虑到,如果地址没有被篡改,但是第三方认证系统的DNS被劫持的话,accessToken还是会被非法获取到 所以,这一步还要再增加一步验证,可以参考https的验证方法,授权系统可以再授权的域中发放密钥文件,被授权的系统必须在授权系统系统中下载授权文件,放在自己的文件目录中,并且被授权系统必须保证这个文件不能被其他人非法获得. 这样授权系统在发放code的时候会先认证有没有这个文件,如果请求到了这个件,就认为这个系统没有被dns劫持 注意一点,这里的会用到重定向地址返回第三方系统的用户登入界面(返回了浏览器),本质上登入和注册需要的只是用户的信息,当使用了认证系统提供的code时候,后端皆可使用accessToken换取接口获取用户新消息了,更加方便的实现集成.(这里回调地址使用的是浏览器的原因是方便第三方登入系统集成,这样就没有必要使用长连接保证code和用户的一一对应和http协议的前端推送) 第十一步 : 这一步其实是第十步的进一步解释 我们之前说的第十部的时候已经解决的DNS劫持的问题了,接下来要解决就是code被暴力破解或者回调url被篡改的问题 这里使用这种方法保证,被认证系统要保证所有的被认证认证系统都拥有一个系统唯一id和对应的唯一密码,被认证系统有一定要保证密码不能泄露出去 当用户在使用code换取accessToken的过程中,要带上这个被认证系统的唯一id和对应的密码,并且相互之间的交互必须使用HTTPS加密,认证服务器在进行认证的时候需要校验id和密码是否匹配,code和Id时候匹配,如果通过,证明验证合法,这样就保证用户信息不泄露","date":"2024-12-31","categories":["架构设计"]},{"title":"0-总纲","url":"/2024/12/31/0-总纲/","content":"什么是A&#x2F;B实验简单来说，A&#x2F;B实验就是针对想调研的问题，提供两种不同的备选解决方案，然后让一部分用户使用方案A，另一部分用户使用方案B，最终通过实验数据对比来确定最优方案。通常我们在进行A&#x2F;B实验的时候，会从总体用户中抽取一小部分进行实验，一来防止产品出现bug干扰大批用户的体验，二来防止实验方案效果不佳造成大范围的负增长。在实验时，我们将用户完全随机地分配给方案A和方案B，尽可能减轻实验样本分配不均对实验结果造成的干扰。当然，关于A&#x2F;B实验的定义也有更加完整的版本。比如王晔博士（耶鲁大学计算机科学博士，曾在微软、谷歌等公司任职，较早地将A&#x2F;B实验思想引入国内）认为互联网产品迭代实践中的A&#x2F;B实验是指： 为了验证一个新的产品交互设计、产品功能或者策略、算法的效果，在同一时间段，给多组用户（一般叫做对照组和实验组，用户分组方法统计上随机，使多组用户在统计角度无差别）分别展示优化前（对照组）和优化后（实验组，可以有多组）的产品交互设计、产品功能或者策略、算法，并通过数据分析，判断优化前后的产品交互设计、产品功能或者策略、算法在一个或者多个评估指标上是符合预期的一种实验方法。 实际上，A&#x2F;B实验早就广泛地应用于科研领域，比如医学界会使用A&#x2F;B实验来判断药品在治疗某种疾病时是否真的有效（实验中一半患者会领到药品，另一半患者会在不知情的情况下，领到与药品外形一样的安慰剂）。现如今，A&#x2F;B实验的思想被引入互联网领域，广泛地应用于互联网产品的增长之中，成为增长黑客们手中的利器。增长黑客的核心在于，能够利用小范围的数据驱动快速迭代、快速验证，而后再大规模地投入运用。那么，增长黑客们如何验证自己的假设是否有助于增长呢？A&#x2F;B测试就是他们的秘密武器。如果你想要成为一个增长黑客，那么A&#x2F;B实验恐怕是你必学的技能。 为什么要做A&#x2F;B实验最早通过实验思路获取增长的公司是谷歌。谷歌在2007年就建设了完善的A&#x2F;B实验系统，之后实验的频率越来越高。现在谷歌每个月都会上线几百个A&#x2F;B实验，这些实验帮助谷歌获得每个月2%的增长。不要小看这2%，实际上，谷歌通过它可以获得全年20%+的增长，增加超过10亿美元的营收。这便是A&#x2F;B实验的力量。我们可以看到：Facebook的CEO会亲自参与众多的A&#x2F;B实验；Linkedin把A&#x2F;B实验作为产品研发上线过程中的基本流程；在国内，以我司为代表的互联网公司有着浓浓的A&#x2F;B实验氛围……为什么互联网行业越来越青睐A&#x2F;B实验呢？互联网行业走到今天，人口红利已然不在，大多数互联网产品野蛮生长的时期已经过去，紧接着到来的是技术红利的时代。在这一时期里，企业开始关注用户的整个生命周期，用数据驱动进行精细化运营，用实验的方式进行科学决策。在这里，我想引用我司实验评估组同学的一段话，来阐释我们为什么要不断进行A&#x2F;B实验： 一方面我们无法承担任何一个错误特性影响上亿用户体验的严重后果，另一方面我们又希望能够分离并量化每个特性的影响。这就需要我们设计并坚持使用一套数据驱动的方法，使得我们可以以较小的风险对新特性进行评估，积极试错积累经验；并且这个方法有能力排除其他因素（如同时开发的其他特性、时间因素等）的干扰；最后，除了“好’或者‘不好“，我们希望这个方法也能够给出定量的结果。为了解决上述问题，普遍使用的方法是小流量随机实验，也就是我们常说的A&#x2F;B实验。 （特性：常写做feature，大致可以理解为功能，尤其是较小的功能）利用大量的实验，企业能够不断提高自己的试错能力、提升试错效率、降低试错风险。从某种程度上来说，互联网企业的实力与其实施A&#x2F;B实验的能力紧密相关。A&#x2F;B实验如何参与增长流程曾有学者提出，A&#x2F;B实验的频率一定程度上决定着互联网企业增长的速度。为了能够在激烈的互联网战场中杀出一条血路，增长团队需要接连不断地进行实验。通常他们会围绕目标，在分析数据、提出想法、进行实验、归纳总结几个步骤中循环往复。 A&#x2F;B实验如何参与增长流程曾有学者提出，A&#x2F;B实验的频率一定程度上决定着互联网企业增长的速度。为了能够在激烈的互联网战场中杀出一条血路，增长团队需要接连不断地进行实验。通常他们会围绕目标，在分析数据、提出想法、进行实验、归纳总结几个步骤中循环往复。 — 这里有一个循环的图 目标：在A&#x2F;B测试的流程体系中，增长团队首先要明确想要优化的“目标”。这种目标需要是具体、可量化的，这样才能更加清晰地追踪实验效果是否符合预期。 分析数据：通过分析数据，发现现有产品某个环节中可能存在的问题。 提出想法：针对数据分析结果，提出产品优化想法，并设计相应实验方案。在A&#x2F;B测试中，我们的想法需要转化为“假设”，例如：“假设把商品售卖页面主色调从蓝色改成红色，用户购买率会提升5%”。 A&#x2F;B实验：根据假设和方案进行小流量实验。 归纳总结：总结实验数据和结论。如果实验结果符合预期，那么迅速迭代产品，把优化过的版本推送至全部用户，以实现增长；如果实验结果不符合预期，迅速清空实验代码，回滚到旧的版本。实验过后，总结经验，带着此次实验的经验和结论，再进入下一轮实验闭环之中。 实际上，曾有国外大型互联网公司分享道，在他们进行的A&#x2F;B实验中，只有1&#x2F;3的实验为产品带来了正向的提升。前亚马逊数据分析团队负责人罗尼·科哈维也说过：“60%—90%的想法都没能改善他们想改善的指标。”想要成为增长黑客，你就要接受这个残酷的事实——你的大多数想法并没有特别大的作用。但是如果不做实验，你永远不会知道到底哪一部分想法是有用的，哪一部分想法是没用的。Always be testing，才是当今互联网公司应有的态度。","date":"2024-12-31","categories":["架构设计"],"tags":["分流能力"]},{"title":"幂等问题解决方案汇总","url":"/2024/12/31/幂等问题解决方案汇总/","content":"概念 幂等（idempotent、idempotence）是一个数学与计算机学概念，常见于抽象代数中。 在编程中，一个幂等操作的特点是其任意多次执行所产生的影响均与一次执行的影响相同。幂等函数，或幂等方法，是指可以使用相同参数重复执行，并能获得相同结果的函数。这些函数不会影响系统状态，也不用担心重复执行会对系统造成改变。例如，“getUsername()和setTrue()”函数就是一个幂等函数。 用通俗的话讲：就是针对一个操作，不管做多少次，产生效果或返回的结果都是一样的 举几个例子： 1.比如前端对同一表单数据的重复提交，后台应该只会产生一个结果。 2.比如我们发起一笔付款请求，应该只扣用户账户一次钱，当遇到网络重发或系统bug重发，也应该只扣一次钱。 3.比如发送消息，也应该只发一次，同样的短信如果多次发给用户，用户会崩溃。 4.比如创建业务订单，一次业务请求只能创建一个，不能出现创建多个订单。还有很多诸如此类的，这些逻辑都需要幂等的特性来支持。 幂等性问题涉及的场景的解决方案所有幂等性问题处理的时候的通用解决方法-标记征用对一种操作,增加一个标记,如果有相同的标记非第一次操作就不进行 例子1. 数据库操作可以使用索引我们针对一条记录使用唯一的索引或者唯一组合索引来进行幂等,比如一个订单的订单号 ps : 这种情况其实对应上米娜的通用方法的话,唯一索引或者唯一组合索引就是token 引申:使用数据操作也可以使用版本号实现对应的逻辑 2. 使用分布式锁和索引方法类似,使用redis或者zk,在其中添加一个唯一索引构造分布式锁来实现幂等 3. 全局状态机幂等其实这个方法是使用的顺序来保证幂等的,这种场景在订单转化的过程中比较常见 比如,订单的状态是已下单->已推仓->未出库->已出库 如果现在状态是已推仓接下来的状态应该是未出库,但是这个时候如果系统传进来一个状态是已下单,说明不满足幂等,这种情况下就可以通过业务逻辑保证幂等","date":"2024-12-31","categories":["架构设计"]},{"title":"库存问题处理上的一些思考","url":"/2024/12/31/库存问题处理上的一些思考/","content":"思考 对于库存管理来说，最重要的库存的分配与扣减 1）、最优仓调度逻辑如果一物一仓（一个sku只在一个仓库有货），这种逻辑比较简单，扣减响应的仓库库存即可。 实际上，一个sku可能会分布在多个仓库中，此时需要根据物流时效，仓库等级去扣减仓库中商品。 1、 查询时，先筛选出有库存的仓库。2、 将收货地址与这些仓库的一级配送区域（24小时到达）匹配，若没有再去匹配二级区域（2日到达），以此列推，筛选出最优仓库。3、 筛选出同级区域（物流时效相同）的仓库后，再根据仓库的优先级进行筛选4、 若扔有多个仓库（同一类型的仓库）待选，例如保定从北京发，和从天津发时效相同，再根据同级仓库之间的优先级进行选择，最后选择仓库。5、 选择好仓库之后，获取仓库中的库存数量 2）、下单扣减库存以购买2个商品为例： 1、提交订单后，判断可销售库存数量是否够，若无法满足，则提示库存不足2、若可销售库存满足条件时，可销售库存减2，锁定库存数加2，成功提交订单。3、在用户支付以后，销售总库存减2，现货库存减2，锁定库存减2，已销售库存加2，做完这一系列动作之后，扣减库存才算结束。4、若买家支付前取消了订单，则返还库存，可销售库存数加2，锁定库存减2。 如何正确扣减库存？ 利用数据库行锁保证库存的正确扣减 select sale_stock_num, lock_stock_num from stock where sku &#x3D; :sku update stock set sale_stock_num &#x3D; sale_stock_num -2, lock_stock_num &#x3D; lock_stock_num +2 where sale_stock_num &#x3D; :old_sale_stock_num and lock_stock_num &#x3D; old_lock_stock_num and sku &#x3D; :sku 如果查到的old_sotck_num已经被其他用户扣减，则update 更新影响行数是0，提示 “库存存扣减失败，请重试 ” 如果使用此种方式，大量用户同一时刻必将查到相同的库存数stock_num，则必将有大量用户提示库存扣减失败。 利用redis单线程保证库存的正确扣减 将库存提前放到redis中， 扣减库存的时候，先扣减redis，然后发MQ消息更新数据库。 如果使用此种方式，如何保证redis和数据库之间的数据一致性。","date":"2024-12-31","categories":["架构设计"]},{"title":"架构设计-我的一个支持异步拉取数据和本地缓存client是如何设计的","url":"/2024/12/31/架构设计-我的一个支持异步拉取数据和本地缓存client是如何设计的/","content":"最近的工作内容是封装一个高性能的sdk，以便和我们的配置中心进行交互，这里整理一下我是如何设计 1. 需求分析 我们要支持和配置中心的交互（网络IO） 支持客户端缓存功能，并且支持断电重启（内存缓存+文件缓存） 配置中心没有推送功能，sdk需要进行轮询（使用NIO网络模型进行轮询） 这个点其实设计的并不好，最好是开启一个长链接，由服务端推送请求是最佳实现，但是因为配置中心和客户端之间有一个中间层，无法实现长链接，不过这两个在实现上是类似的 2. 任务分解 针对网络IO 提供两套模型 当缓存中没有所需要的数据的时候，使用BIO直接读服务端（缓存击穿） 当缓存的存在的时候， 将缓存的数据添加到轮询队列中，等待下一次异步同步更新 针对缓存设计 内存缓存：除了需要提供一个数据缓存，还需要提供一个状态同步集合来标记缓存中数据的状态，比如是否超时，是否有效等 针对这个情况我们需要实现一个功能类—状态处理类，通过这个类来处理缓存中状态同步集合的状态，并同步处理数据缓存对应的状态 ps : 为什么设计成数据和状态分离 -> 为了更高的可控性和解藕 1.如果需要状态特殊处理的时候只需要拿出状态队列就可以进行处理了不需要动缓存 2. 如果状态需要添加字段或者逻辑，只需要修改对应的状态队列即可，不需要修改数据缓存 文件缓存： 这个本质上很内存缓存相似，只有一个点就是当使用nio进行数据同步的时候，nio从网络中拉取的新数据需要同步更新缓存和文件，此时的文件操作应该使用nio保证不堵塞。 轮询方法 我在框架中使用的NIO网络框架是vert.x 其实可以理解成java中的promise方法 这里我做的轮询逻辑其实非常简单过程如下 拿到需要进行轮询队列 for循环发起NIO网络请求，并且处理结果 更新缓存状态（更新缓存，处理超时等问题） 还需要那些改进？ 需要添加一个速率控制器，考虑这样一个场景如果轮询发起的请求过大，将可能会把配置中心打垮，所以sdk需要控制请求的速率动态的调整qps来保证服务的稳定。 3. sdk整体的架构图如下","date":"2024-12-31","categories":["架构设计"]},{"title":"系统架构-Serverless(baas & faas)无服务器计算","url":"/2024/12/31/系统架构-Serverless-baas---faas-无服务器计算/","content":"自从2014年AWS推出Lambda服务后，Serverless一词越来越热，已经成为一种新型的软件设计架构，即Serverless Architecture。作为一种原生于公共云的架构，Serverless有什么优缺点？是否能应用于传统企业程序？是否适合私有云场景？是否像很多文章宣称的一样，会成为未来改变云计算的中坚力量？作为一名云计算行业的老兵，作者想在此文中分享一些自己的观点。 什么是ServerlessServerless并不神秘，用一个简单的例子就可讲明。我们设计了一个AI应用，可以识别出图片中人物的人种，我们把它作为一种SaaS服务架设在公共云上提供给客户使用，其典型的后端架构设计如下： 在该架构中，我们购买的云主机上运行了Tomcat Web Server，用于承载Java编写的AI应用。用户通过API上传图片。受限于云主机的本地存储空间，为了满足大量客户同时上传图片，AI应用实现了一个存储网关将图片导入公共云的对象存储。图片导入完成后，AI应用从对象存储读入图片进行识别，并将结果存入公共云的数据库中（例如RDS），用户使用API查询结果。 AI应用上线一段时间后受到了用户的欢迎，越来越多的公司开始使用该服务。根据统计数据，大多数公司在上午9点11点、下午2点5点集中上传图片，为了满足该时间段的突发访问量，我们设置了公共云的 Auto-Scaling策略，在访问增加时动态创建更多的云主机来响应客户。AI应用的架构演化成： 在这个架构中，我们需要做如下事情： 管理云主机。我们要关心CPU数量、内存大小、IP地址等等系统级的配置。同时还要关心云主机的操作系统，为部署AI应用拟定策略。操作系统和Tomcat的安全补丁也不能忽视，否则竞争对手可能雇佣黑客来攻击我们的系统。 配置公共云的Auto-Scaling的策略，应对高峰期突发访问量。 使用公共云的对象存储和数据库。 编写AI应用。 要完成这些工作，我们既要开发AI应用，又要营运支撑业务（例如管理云主机生命周期、管理操作系统）。这是当前架构的现实：为20%的核心业务营运80%的支撑业务。 下面用Serverless架构改写AI应用： 多个运行AI应用代码的进程被启动，并发处理用户上传的图片。 在Serverless架构的AI应用中，我们只需要做两件事情： 使用公共云的对象存储和数据库。 用公共云的Serverless框架编写AI应用。 与之前的架构相比，我们不再营运云主机、操作系统、Tomcat，同时也不需要配置Auto-Scaling Group，公共云的Serverless框架会在每个图片上传完成后启动一个进程运行AI应用，自动实现水平扩展。我们终于只需要关心核心业务了，用Serverless框架支持的语言（例如AWS Lambda就支持Java, Python和JVM系语言）编写AI应用，一切非核心业务都外包给了公共云营运商。 我们的Serverless AI应用用到了两种技术。首先使用了公共云提供的对象存储和数据库服务，统称为BaaS（Backend as a Service，后端即服务）。其次用了Lambda框架，称为FaaS(Functions as a Service，函数即服务)。 使用BaaS和FaaS是Serverless应用的基本特征，符合这两个基本特征的应用可称为Serverless应用。 是BaaS，不是PaaS AI应用用到了对象存储和数据库，将来或许还会用到消息队列。直观感觉是在使用PaaS，为什么还要造一个新词BaaS？技术圈有太多令人混淆的术语了。 BaaS并非PaaS，它们的区别在于：PaaS需要参与应用的生命周期管理，BaaS则仅仅提供应用依赖的第三方服务。典型的PaaS平台需要提供手段让开发者部署和配置应用，例如自动将应用部署到Tomcat容器中，并管理应用的生命周期。BaaS不包含这些内容，BaaS只以API的方式提供应用依赖的后端服务，例如数据库和对象存储。BaaS可以是公共云服务商提供的，也可以是第三方厂商提供的，例如Facebook收购的Parse就是著名的MBaaS提供商（Mobile Backend as a Service）。从功能上讲，BaaS可以看作PaaS的一个子集，即提供第三方依赖组件的部分。 FaaS是Serverless的核心 AI应用最初是一个典型Java程序，它可能使用Spring这样的技术，因为我们需要一个框架确保程序的各个组件能够被正确加载，需要MVC来保证REST API被正确的Controller处理。AI应用部署在Tomcat容器中，运行在云主机上，7 x 24小时运行，我们提供不间断的服务。在夜里12点到早晨8点，几乎没有用户使用，但我们还得让它待在那里，防止深夜偶尔使用的用户得到一个503错误而误会AI服务不稳定。我们为购买的云主机付钱，尽管一半的时间它的CPU使用率几乎为0，但没有公共云是按CPU使用率计费的，不工作的时间也得付钱。我们必须关心Auto-Scaling Group的配置，如何准确的配置Auto-Scaling策略是一个技术活，需要长期的经验积累，在早期我们不得不多部署一些空闲的云主机以保证服务不会因Auto-Scaling的配置不当而拥塞。 用Serverless架构改写了AI应用后，这些痛苦就通通消失了。Spring框架和Tomcat去掉了，用Lambda的Java SDK，只需要实现一个Function Handler处理图片上传完成这个事件，这跟写一个Callback一样简单。在Function Handler中调用图片识别的相关逻辑，然后调用数据库的REST API存储结果。也不用构建MVC，不用配置Tomcat的XML文件，我们将存储网关这个功能完全去除掉了，因为用户可以直接上传图片到对象存储。 AI应用不用7 x 24小时运行了，没有用户上传图片时它只是一份编译好的代码。当用户图片上传完成时，FaaS会为AI应用启动一个新的进程执行代码。该进程在代码执行完成后自动销毁。我们只需为代码执行的这几十秒钟付钱，节省了很多开支。 最后我们无需操心Auto-Scaling的问题，FaaS会在需要的时候自动扩展。 这些就是FaaS的核心，从上面的例子里面可以归纳出它的特点： FaaS运行的是后端代码而不是整个后端程序。例如AI应用仅仅包含处理图片上传完成这个事件的逻辑，并不是一个完整的后端程序，而是一段后端代码。 代码通过事件触发。由于不再有一个长期运行的进程等待或轮询用户请求，代码只能通过特殊的事件触发。这些事件由FaaS框架定义，例如上传文件到对象存储、消息队列收到一条新的消息、API Gateway收到一个新的API请求等。 代码的生命周期很短。例如我们的AI应用，从收到事件后Function Handler被调用开始，到调用返回结束，不会有常驻内存的进程运行。此外公共云提供商还会限制代码执行的时间，超出时间后执行代码的进程会被强行销毁。例如AWS的Lambda可执行的最长时间为5分钟。 代码必须做到彻底无状态，两次调用间不能共享内存状态。我们的AI应用最早使用了一个全局变量统计处理的图片数，每处理完一张图片该计数器就加一。使用FaaS后我们不能再用任何全局变量或内存数据结构（例如Hashmap）在调用间共享数据，因为代码运行在独立的进程中，无法访问对方的内存地址空间。于是我们对代码进行了改造，将全局计数器放到了公共云的Redis服务中，这为代码增加了额外的复杂性。 水平扩展不再是需要担心的问题，FaaS会为每个事件和请求运行一份新的代码。 应用的部署方式从上传、配置整个程序变成上传一份打包代码的文件（例如Jar文件或一个Zip文件）。 Serverless为我们带来了什么 对比传统架构，用Serverless架构改写的AI应用具有显著的优势。我们不再运维任何云主机和操作系统，甚至不再运维Tomcat这样的Web容器，只需要专注于代码本身，所有配置、应用生命周期管理的工作都由FaaS框架负责。公共云的出现让我们从物理硬件管理中解放出来，Serverless架构让我们进一步从操作系统管理中解放出来，第一次真正专注于核心业务。 业务也变得更加敏捷了。我们只需要编写核心业务相关的代码，例如AI应用中图像识别的部分。无需编写任何加载、部署、配置应用的代码，例如不再需要配置systemd在系统启动时加载应用。 水平扩展也不是问题。正如前面反复提及的，FaaS框架会为每一个事件、每一个API请求都启动一份新的进程执行代码。这跟传统应用的线程池方式类似，每个请求都在一个单独的线程中执行，区别在于线程之间共享同一内存地址空间，FaaS的进程间不共享任何内存。与线程池有最大线程数限制类似，FaaS框架通常也限制了最大进程数，例如AWS Lambda在一个Region默认能执行的最大并发调用是600，也就是说我们的AI应用最多能在600个进程中同时执行。 最后，也是最重要的，Serverless架构为我们节省了大量开支。我们只需为AI应用运行的时间付钱，无需为应用等待请求的时间付钱。水平扩展的粒度从原来的云主机细化到进程，节省了额外的开支，不用再购买闲置的云主机来抵消Auto-Scaling的配置不精确带来的影响。业务的敏捷性提高也降低了营运成本，我们不再需要精通操作系统配置和管理的营运人员，不仅节省了人力成本，也节省了应用从开发到上线的时间。 Serverless不是银子弹，是后端小程序的未来 serverless架构在某些应用场景的优势如此明显，有些支持者已经开始炒作它会成为颠覆性的云计算新架构了。技术圈向来如此，一些人总在孜孜不倦的寻找包治百病的灵药，和解决一切问题的银子弹。“All design is about tradeoff”，Serverless也不是银子弹，它有独特的优势，而这些优势也带来了不可避免的局限。 为每个事件&#x2F;请求启动一个全新的进程运行代码是FaaS的核心，进程的启动延时是Serverless面临的第一个问题。取决于编写应用的语言，启动延时可以是10毫秒（如简单的Python应用），也可以是1分钟（复杂的Java应用）。这样的延时对于realtime的程序是难以接受的。目前Serverless应用通常运行在公共云的多租户环境中，启动延时还受系统负载影响，很难保证应用在规定时间内被运行。公共云提供商目前没有对Serverless提供相应的SLA保证，笔者写这篇文章的时候，AWS Lambda还没有相关的SLA条款。 Serverless无法用于高并发应用，为每个请求启动一个进程开销太高。例如双十一支付宝高峰期每秒处理的交易数为8.59万笔，如果使用Serverless架构，意味着我们的系统内每秒有8.59万个进程被创建又被销毁，这是难以负担的开销。 Serverless应用无法常驻内存，运行的时间是受限的。如果你的应用无法在数分钟内完成的工作，那Serverless不是你的选择，例如AWS Lambda给予进程的最长运行时间是5分钟，超时后进程将被强制终止。这对程序设计提出了挑战，例如我们的AI应用必须优化到在5分钟内完成复杂图像的识别。我们也不能编写执行长时间IO操作的应用，例如对对象存储中1T的数据进行复杂编码。 Serverless调用之间不能共享状态让编写复杂程序变得极度困难。无状态是互连网应用追求的目标，例如满足“12要素”的应用。但Serverless将无状态进行的更加彻底，在不同的调用之间无法共享内存状态，例如使用hashmap。我们的AI应用中统计已处理图片总数的全局计数器在传统架构中只是一个全局变量，但在Serverless架构中它变成存储在内存数据库（Redis）中的一条记录，更新成本、保证原子性等因素让我们的编码变得数倍复杂。对于大多云原生的互联网应用来说，这种彻底的无状态架构是一个巨大的挑战，而对于动辄有几十万、上百万行代码的、充满了状态的企业应用来说，Serverless的无状态改造几乎是一个无法完成的任务。 熟练的微服务的架构师，对将业务拆分成一个个单独的服务非常熟悉，也有不少的经典书籍（例如《Building Microservices: Designing Fine-Grained Systems》）指导我们如何做。但即使是他们，在面对Serverless架构时也会感到头痛，如何将业务拆分成成百上千个运行在独立进程、运行时间受限的函数是巨大的挑战。而是否需要如此细粒度的拆分是需要回答的第一个问题。有些问题或许变成无解难题又或成本极高，例如分布式数据库事务。 上面都是Serverless架构的一些固有局限，它们源于Serverless架构的特点，很难随着时间的推移、技术的完善而解决。除此之外，作为一个新的技术，Serverless还面临着集成测试困难、Vendor Lock-in、调试监控困难、版本控制等诸多不足，每一项都会成为采用Serverless架构的阻碍。 由于这些局限性，Serverless架构不会成为复杂应用的架构首选，相反，它应该是后端小程序的未来。 云端的应用有大量的小程序场景，例如识别一张图片、对一段音频&#x2F;视频进行编解码、对IOT设备的请求返回一小段数据、将客户提交的工单通过邮件通知客服人员等等。这些基于事件触发的小程序在传统架构中实现起来是相对复杂的，你往往需要为20%的核心业务运营80%的支撑业务。Serverless完美的解决了这些问题，它可以成为复杂应用的一种补充架构。我们可以将无状态的、事件触发的业务拆分成Serverless应用，让整个架构变得更加的简洁和高效。 Serverless也在不断演变，例如AWS最近引入的Step Functions就尝试解决调用间共享状态的问题，其效果有待观察。 Serverless不是传统的PaaS Serverless跟PaaS之间的界线比较模糊，很多人认为Serverless是PaaS的一种，笔者也倾向于认为Serverless是特殊的PaaS形态。 Serverless由BaaS和FaaS两部分构成，BaaS负责提供业务的依赖服务，FaaS负责业务的部署和生命周期管理，从这个意义上来看，Serverless的角色跟PaaS一样。与传统PaaS的区别在于，传统PaaS是以程序为粒度管理应用的生命周期，而Serverless是以函数粒度管理应用生命周期。传统PaaS中的应用为常驻内存的进程，而Serverless应用运行完即销毁。此外，使用传统PaaS，用户仍需要关心水平扩展，例如如何配置Auto-Scaling Group，但Serverless没有这个问题，水平扩展是架构天然自带的功能。 Serverless和微服务 Serverless和微服务没有直接关系，但两者有相似之处，例如都需要做业务拆分、强调无状态、具有敏捷特性等。Serverless在很多方面比微服务粒度更细，要求也更严格。例如微服务以服务为边界拆分业务，Serverless以函数为边界拆分业务；微服务可以有跨调用的内存状态共享，Serverless要求调用彻底无状态。此外，Serverless依赖BaaS提供第三方依赖，而微服务可以自由选择第三方依赖来源，例如使用本地搭建的传统中间件栈（如本地MySql和消息总线）。 Serverless和容器 Serverless和容器是苹果和桔子的比较，不在一个平面上。Serverless是一种软件设计架构，容器是软件架构的承载者。虽然没有公开资料，但我们可以推测类似于AWS Lambda这样的Serverless框架使用了某种程度的容器技术，否者难以实现语言无关和毫秒级的启动。尽管已经有一些开源项目使用Docker实现Serverless中的FaaS部分，笔者不认为AWS Lambda这样的公共Serverless框架直接使用了Docker，一定是一种更为轻量级、体积更小的容器技术，我们或许可以将它称为Nano-Container。 Serverless对私有云有意义吗？ 对于私有云来说，现在将业务迁往Serverless架构还为时过早。首先Serverless是从公共云中演化出来的新型架构，适用于运行在公共云上的小程序。而私有云更多承载的是老而笨重的传统业务，难以用Serverless架构改造。其次Serverless依赖BaaS，在私有云中搭建和运维BaaS成本都不低，使用公共BaaS服务又受限于网络带宽和延时，容易导致系统不稳定。 随着企业应用的进一步云化、开源Serverless框架的成熟，私有云的Devops场景也可以采用Serverless作CI&#x2F;CD，例如目前Jenkins承担的大部分工作都可以用Serverless替代，如用FaaS框架对应Jenkins本身，上传的代码对应Jenkins Job中的Bash脚本，将原来的Jenkins API触发Job改为触发FaaS中的代码。 总结 Serverless作为一种全新的架构，是云计算发展演化的必然结果。追求更细粒度的计费单元，更加专注于核心业务、将支撑业务外包给基础设施提供商是云计算的趋势。Serverless架构的特点，让编写事件触发的后端小程序变得更加容易。同时它也有自身内在的局限性，并不适合复杂的应用架构。从目前的情况看，部分采用Serverless的混合架构对公共云应用是个不错的选择，私有应用采用Serverless还为时过早。云计算技术正在飞速发展，未来还有无限可能。","date":"2024-12-31","categories":["架构设计"]},{"title":"对于编写代码的思路的思考","url":"/2024/12/31/对于编写代码的思路的思考/","content":"设计的模式在实际场景中的应用说这个事情的原因是因为之前比较迷信设计模式的， 喜欢去做一定的抽象 但是实际的项目运用场景的时候其实设计模式的应用还是比较少的， 更对的是抽象的方法。 这里从设计模式出发， 归纳一下抽象代码的基本方法和逻辑 单一职责原则 (Single Responsibility Principle)开放-关闭原则 (Open-Closed Principle)里氏替换原则 (Liskov Substitution Principle)依赖倒转原则 (Dependence Inversion Principle)接口隔离原则 (Interface Segregation Principle)迪米特法则（Law Of Demeter）组合&#x2F;聚合复用原则 (Composite&#x2F;Aggregate Reuse Principle) 抽象的时候要把细化功能 （最小单一原则）举个例子 ， 我们要抽象一个功能是打电话 ， 打电话的具体能力包括 ， 拨通电话，开始通话 ， 结束对话 我们抽象java的时候本能的想可以进行这样的抽象 但是这种其实不好的， 我们没有抽象到跟家具体的细节上 。 比如说， 打电话其实对应计算机而言是协议开始， 而结束对话对应的功能是协议关闭 ， 而通过中对应的是数据传输 。 所以我们可以通过这种方法在进行抽象 其实这个比较依赖经验程度的，具体的拆分的判断是依赖于我们自己， 但是有一个核心点就是要保证最小单一原则 ， 尽可能的归纳和细化能力 ， 这就要求我们写代码的时候需要做好前期规划和准备 开放关闭原则这个基本上没办法满足 他有两个核心点 OCP 可以具有良好的可扩展性，可维护性。 不可能让一个系统的所有模块都满足 OCP 原则，我们能做到的是尽可能地不要修改已经写好的代码，已有的功能，而是去扩展它。 重点记一下两个工厂方法的实现吧 合理使用继承 - 里氏替换原则里氏替换原则通俗的来讲就是：子类可以扩展父类的功能，但不能改变父类原有的功能。 这个就不需要延伸了 ， 现在的编程语言基本上都保持了这个属性 ， 包括继承/多态/范型的能力。 我对于这个的理解其实有两个方面 1. 尽可能保持功能的唯一和确定性-一种类型的方法或者对象， 具备相同的能力 2. 尽可能保持编写代码的唯一和确定性-对统一种方法和类型操作的流程是相同的， 只不过不同的类型之间可能有删减或者增加 3. 尽可能保持心智理解上唯一性和确定性 - 针对与理解，系统功能的使用上接口的类型&#x2F;对象拥有的属性都是相同的 依赖倒转原则 DIP我们在实现类的时候尽可能不要做到直接引用， 而是尽可能的使用接口， 这样的方法会导致抽象的层级和能力不够后续维护成本过高 ， 举个例子 注意这里： 实现Driver类的时候不要写入driver实现类， 而是要写入接口 接口分离原则强调不应该依赖它不需要的接口；一个类对另一个类的依赖应该建立在最小的接口上。这个规则和 依赖倒转原则 相辅相成， 强调接口大小需要合理 比如接口 不同的实现类可能需要的方法 有相同的和不想同的地方， 可以对接口进行再次拆分， 相同的变成一个接口， 不相同的变成另一个接口， 实现接口简化 迪米特法则 LOD核心点， 使用方法对外提供数据而不是使用直接的引用， 而且需要注意一下引用的层级，迪米特法则又称为 最少知道原则，它表示一个对象应该对其它对象保持最少的了解。通俗来说就是，只与直接的朋友通信。例子就不聚了， 核心点是要合理规划类、对象、方法之间的关闭， 不要想不嵌套引用，层级需要清晰 组合&#x2F;聚合复用原则 CRP合理处理组合和服用的关系，组合优于继承。因为继承会导致子类无法感知到父类的变化","date":"2024-12-31","categories":["系统设计"]},{"title":"Friendship 关于友谊","url":"/2024/12/31/Friendship-关于友谊/","content":"Friends play an important part in our lives, and although we may take the friendship for granted, we often don’t clearly understand how we make friends. While we get on well with a number of people, we are usually friends with only a very few, for example, the average among students is about 6 per person. In all the cases of friendly relationships, two people like one another and enjoy being together, but beyond that, the degree of intimacy between them and the reasons for their shared interest vary enormously. As we get to know people we take into account things like age, race, economic condition, social position, and intelligence. Although these factors are not of prime importance, it is more difficult to get on with people when there is a marked difference in age and background. Some friendly relationships can be kept on argument and discussion, but it is usual for close friends to have similar ideas and beliefs, to have attitudes and interests in common—they often talk about “being on the same wavelength”. It generally takes time to reach this point. And the more intimately involved people become, the more they rely on one another. People want to do friends favors and hate to break a promise. Equally, friends have to learn to put up with annoying habits and to tolerate differences of opinion. In contrast with marriage, there are no friendship ceremonies to strengthen the association between two people. But the supporting and understanding of each other that results from shared experiences and emotions does seem to create a powerful bond, which can overcome differences in background, and break down barriers of age, class or race. 生活中，朋友扮演着一个极为重要的角色。然而，我们可能把友谊视为理所当然，却通常并不清楚朋友是怎么结识的。尽管我们与很多人都相处融洽，但真正成为朋友的却只有少数几个——比如，学生平均每人有6个朋友，其中两人志趣相投，相处甚好；除此之外，朋友间的亲密程度及志趣相投的原因大有不同。我们在彼此结识时，常会考虑对方的年龄，种族，经济条件，社会地位和聪明才智等。尽管这些因素并非特别重要，但当人们在年龄与背景方面存在太大差异时，往往很难相处。 有些朋友关系能在相互争论和讨论中维持。但亲密的朋友通常有着相似的观点和信仰，相同的见解和兴趣——就是常说的“志趣相投”，要达到这种境界，一般需要很长时间的磨合。而且，彼此关系越密切，依赖性就越强烈。人们总希望朋友间互帮互助，憎恶背信弃义。同样，朋友间必须学会容忍对方的坏习性，并接受对方的不同观点。 与婚姻相反的是，友谊没有仪式来强化二者的关系。但是，基于双方共同的经历，情感而产生的理解和支持，能克服背景的差异，年龄的界限，打破性别，阶层与种族的屏障，把二人牢牢地拴在一起。 链接: https://en-brief.xiao84.com/meiwen/2015/26779.html","date":"2024-12-31","categories":["英语"]},{"title":"网络-http协议响应源代码","url":"/2024/12/31/网络-http协议响应源代码/","content":"最为一个中国人,在进行后端开发的时候,中文乱码是最长遇到的问题,这次好好记录一下在http请求中浏览器和后台的服务器如何处理http请求中的文字编码问题 问题存在的根本位置浏览器和服务端的一次http请求可理解为一次前端请求一次后端响应,将这个过程分解,我们将会看到发生http字符转码主要集中在四个点 浏览器发送数据 服务端解析浏览器发送数据 服务端返回数据 浏览器即系服务端返回数据 然后引申一点 , 无论前端还是后端进行转码的方法无非两种 http 标签中使用的 指定格式 在http头中指定相关的http 编码格式 首先看看第一点浏览器发送数据 编码浏览器发送请求时候主要为三类 url 地址 这个时候的编码取决于浏览器,浏览器使用何种编码,就是用何种编码 form表单 取决于页面的编码也就是 标签, 或者后端返回的contentType的chartset字段 注意当没有这个字段的时候将会使用浏览器的默认编码 ajax 这个时候取决于自己发送时设置的http报头 ,如果没有将会使用浏览器默认的编码方式 浏览器解析编码方式这个就很简单了 读取请求头的contentType 中的编码格式就可以了 服务器返回数据 编码控制两种方式 后端设置contextType 值 中的chartset指定 前端解析方式html 标签中 指定 ps 并没有注意到优先级,以后测试","date":"2024-12-31","categories":["计算机网络"]},{"title":"网络-详解http协议-基础概念","url":"/2024/12/31/网络-详解http协议-基础概念/","content":"http协议的基本流程 url和uri url：统一资源标识符-表示一个网络地址 uri：统一资源定位符，表示互联网上的一个资源，使用的url是uri的一部分（http），其他相关的比如ftp等 http协议的特点基于请求和响应一次请求必定跟随一次相应,无状态，可以使用cookie和session技术让状态有响应 http协议支持的请求头 http1.1性能提升 持久链接：之前的http协议都是一次请求创建一次tcp链接，1.1升级为创建一次多次tcp请求 管线化：让http请求可以进行异步操作 http协议报文结构 压缩传输内容：将传输的的报文实体使用gzip的方法进行压缩然后在发送出去 分块传输编码：降低出错重传的代价，使用16位进行块标识，最后一个块标识为”0(CR+LF)” 与http协作的web服务器（1）代理 作用：接受请求并转发个响应的其他请求 优势：利用缓存减少带宽，流量控制，访问日志 （2）网关 相当于升级http请求，转化成其它协议 （3）隧道 不进行协议解析直接进行信息转化","date":"2024-12-31","categories":["计算机网络"]},{"title":"网络-详解http协议-状态码和报文信息","url":"/2024/12/31/网络-详解http协议-状态码和报文信息/","content":"http状态碼 2xx表示成功 200：OK 表示服务器正常处理 206：表示客户端进行了范围请求，服务返回行了范围请求 3xx重定向 301：永久重定向，表示uri已经分配了新的地址，以后使用这个uri访问资源 302：临时重定向，表示只是暂时的分配了新的地址，本次请求使用新的地址 303：临时重定向，和302区别时要求客户端使用get方法进行请求 304:为满足条件，表示服务器可以进行请求，但是没有满足head中的（if-×）类型字段的条件 4xx表示客户端错误 400：语法错误 401：需要进行验证 403：访问被拒绝 404：服务器上无法找到被请求的资源 5xx服务器错误 500：服务器故障 503：服务器超负荷，或停机维护，服务器返回Retry-after表示需要的时间 http协议首部字段的书写格式 http协议通用首部字段 cache-Control格式 作用：操作缓存机制 缓存请求指令 缓存响应指令 connection控制代理表明相关的字段不被转发 和管理持久链接 关闭持久链接 开启持久链接 data 创建时间的报文 Trailer 首部字段Trailer会事先说明报文主体后记录了那些首部字段。该首部字段可应用在HTTP&#x2F;1.1版本分块传输编码时。 Transfer-Encoding:chunk http1.1中表示分块请求 upgrade表明是否支持更加高级的协议 via 一般和trace连用用来追踪服务器信息 http协议请求首部字段 Accept等 authorization和proxy-authrization认证相关：401等 form使用代理的时候告诉电子邮箱的地址 if-xxxif-match：比对match参数和文件的ETag值 相同返回200 否则412 if-Modified-Since:表示返回文件的最后更新底线，如果文件的实际更新时间在字段时间之前则失败反之成功 （if-unmodifed-since 和这个作用相反）if-no-match:和if-mathch 作用相反，不匹配的时候返回成功if-Range：如果该字段的ETag值或者时间相同就允许做范围资源请求 Max-forwards指定转发的次数 Range范围返回 TE指定传输编码的方式– 可以使用trailers指定的相关字段 User-Agent用户的浏览器相关的信息 http协议响应首部字段 ETag资源的唯一标识符，当使用不同的语言环境的时候可能会出现相同的uri地址但是指向的却是不同的资源，这就使用ETag进一步标识所要使用的资源 location配合3xx 表示重定向的地址 Retry-after指定多久呵在进行相应，配合503 server表示服务器信息 Vary http协议实体首部字段 allow指定服务器指定的uri请求方法GET等 Content-*实体部分的编码 Expires指定资源失效的日期，优先级小于Cache-Control：max-age last-Modified返回文件最终修改日期 其他非http&#x2F;1.1首部cookie和setcookiecookie相应格式 相关字段属性 Domain：域，表示当前cookie所属于哪个域或子域下面。 Path：表示cookie的所属路径。 Expire time&#x2F;Max-age：表示了cookie的有效期。expire的值，是一个时间，过了这个时间，该cookie就失效了。或者是用max-age指定当前cookie是在多长时间之后而失效。如果服务器返回的一个cookie，没有指定其expire time，那么表明此cookie有效期只是当前的session，即是session cookie，当前session会话结束后，就过期了。对应的，当关闭（浏览器中）该页面的时候，此cookie就应该被浏览器所删除了。 secure：表示该cookie只能用https传输。一般用于包含认证信息的cookie，要求传输此cookie的时候，必须用https传输。 httponly：表示此cookie必须用于http或https传输。这意味着，浏览器脚本，比如javascript中，是不允许访问操作此cookie的。 从客户端发送cookie给服务器的时候，是不发送cookie的各个属性的，而只是发送对应的名称和值。 http首部基于缓存代理和非缓存代理行为的分类 端到端首部 : 转换过程中不能丢失，必须转发的首部 逐跳首部 : http1.1需要提供connection字段，经过一层代理之后可以不进行转发","date":"2024-12-31","categories":["计算机网络"]},{"title":"aouth2.0-可能存在的安全隐患","url":"/2024/12/31/aouth2-0-可能存在的安全隐患/","content":"客户端安全隐患1. CSRF 跨站攻击解决办法： 使用唯一的status码来标记请求 我们知道最基本的aouth2.0在进行认证的过程中，认证中心将会返回302header头的响应，重定向callback地址，并且携带参数 ， 如果第三方应用直接调用客户端已经认证过的信息的话 ， 将会导致凭据窃取 解决CSRF最通用有效的方法时使用上下文参数 从本质上来讲使用status参数是对每一个http请求加上一个唯一code 来证明这个code是合法的 oauth标准其实是是要求一个code 其实我们可以制定多个status或者一个hash map 来降低status 伪造的命中率 2. 客户端凭据失窃解决办法： 动态注册客户端 2.1. 重定向URL劫持解决办法：使用完全匹配的模式来进行重定向uri校验 2.2 通过HTTP Referrer 协议，窃取密码2.3 通过开放重定向器盗取令牌解决办法：确切匹配 3. 授权码失窃4. 凭据失窃在某些情况下,攻击者也能够通过恶意手段用授权码换取访问令牌,或者使用授权码进行某种 CSRF 攻击。 使用 state 参数,这是规范中建议的(虽然不强制要求) 。 理解并慎重选择适用于应用的许可类型(流程) 。 隐式许可类型不应该用在原生应用中,它是专门供浏览器内的客户端使用的。 原生应用无法对 client_secret 保密,除非是在动态注册的情况下在运行时配置client_secret。 注册 redirect_uri 时应该尽可能地具体。 如果能避免,请不要以 URI 参数的形式传递 access_token。 受保护资源安全隐患1. xxs 攻击本质 ：从服务端返回的数据存在非法注入的js文件 解决办法 ： 使用context-type 来过滤 当oauth2.0 只使用前端流程，容易导致授权码丢失 2. 令牌重放在返回令牌的过程中被拦截窃取 解决办法： 使用https 使用 HSTS 协议 ， 增加Strict-Transport-Security: max-age&#x3D;31536000 头部，强制使用HTTPS ， 如果不是https将会导致内部重定向到307 来总结一下确保受保护资源安全的方法。 在受保护资源的响应中过滤所有不可信的数据。 为每个端点选择合适的 Content-Type。 尽可能利用浏览器的防护机制以及安全头部。 如果资源端点要支持隐式许可类型,则要使用 CORS。 避免让受保护资源支持 JSONP(如果可以的话) 。 总是将 HTTPS 与 HSTS 结合使用 认证系统安全1. web常规安全2. 授权码劫持黑客可能会通过浏览器 历史记录拦截或者各种http监听手段获得了 授权的回调函数中的各种参数，比如 &#x2F;callback?code&#x3D;SyWhvRM2&state&#x3D;Lwt50DDQKUB8U7jtfLQCVGDL9cnmwHH 为了防止这种拦截的情况 ， oauth 规定了一系列的流程来处理他客户端不能多次使用同一个授权码。如果一个客户端使用了已经被用过的授权码,授权服务器必须拒绝该请求,并且应该尽可能地撤回之前通过该授权码颁发的所有令牌。 3. 重定向 url篡改解决办法：完全匹配 重定向url 4. 重定向url篡改 模拟授权这个流程很经典，之前没有想到 解决办法 ， 认证服务器需要知道之前的回调地址，客户端在同授权凭证换取认证凭证的时候需要校验 ，二者是否一致 保障授权服务器安全的责任重大,因为它是整个 OAuth 安全生态系统的关键。 授权码使用一次之后将其销毁。 授权服务器应该采用精确匹配的重定向 URI 校验算法,这是唯一安全的方法。 完全按照 OAuth 核心规范来实现授权服务器可能会导致它成为一个开放重定向器。如果这个重定向器能受到妥善的监控,则情况还好,但稍有不慎则会面临风险。 留意在进行错误提示的过程中,信息有可能通过 URI 片段或者 Referrer 头部遭泄露。 Oauth令牌漏洞令牌伪造。攻击者可能会构造假令牌或者篡改已有的有效令牌,导致资源服务器授予客户端不当的访问权限。例如,攻击者可以构造一个令牌,用于获取他本不能访问的信息。或者,攻击者可以篡改令牌来扩大令牌原本的权限范围。 令牌重放。攻击者会尝试使用过去使用过并且已经过期的旧令牌。在这种情况下,资源服务器不应该返回任何有效的信息,而应该提示错误。具体来说有这样的情形:攻击者首先通过合法手段获取访问令牌,然后在令牌过期很久之后再尝试使用该令牌。 令牌重定向。攻击者将用于某一资源服务器的令牌用来访问另一资源服务器,而该资源服务器误认为令牌有效。此情形是这样的:攻击者先合法地获取某一特定资源服务器的访问令牌,然后将该令牌出示给另外一个资源服务器。 令牌信息泄露。令牌可能会含有一些关于系统的敏感信息,而这些信息是不应该透露给攻击者的。与前一个问题相比,信息泄露似乎是个小问题,但仍然需要小心。 一种优化方法，使用PKCE的客户端获取方式来获取授权码 但正是这样的简洁性对贯穿系统始终的令牌保护提出了要求。 必须使用 TLS 这样的安全传输层机制来传递访问令牌。 客户端应该请求尽可能少的信息(在权限范围的设置上尽量保守) 。 授权服务器应该存储访问令牌的散列值,而不是令牌明文。 授权服务器应该保持较短的访问令牌生命周期,以最小化因单个令牌泄露而导致的风险。 资源服务器应该将访问令牌存储在瞬态内存中。 PKCE 可用于提高授权码的安全性。 oauth 其他生态系统 携带信息的加密令牌 令牌内省","date":"2024-12-31","categories":["软件开发"],"tags":["aouth2.0"]},{"title":"uml-plantUML-流程图画法","url":"/2024/12/31/uml-plantUML-流程图画法/","content":"从大学开始就对软件工程中的各种图表示反感，除了在做毕业设计的时候有一些应用的地方之外，平时很少能用到。 最近老大分配任务让我去梳理一些业务的逻辑，这才发现软件工程中的一些图是非常有必要的，所以这里准备系统的学习一下plantUML的使用 plantUML 流程图","date":"2024-12-31","categories":["软件开发"]},{"title":"些业务代码的一些原则","url":"/2024/12/31/些业务代码的一些原则/","content":"接口是不是要幂等 数据库 update delete select 等范围是否正确， 范围是否正确 兜底的处理方法是否有运行 - defer try catch等 nil指针处理方法是否是正常的","date":"2024-12-31","categories":["软件开发"]},{"title":"GoF设计模式(1)-iteratoer","url":"/2024/12/31/GoF设计模式-1--iteratoer/","content":"GoF设计模式-iteratoer迭代器模式这个设计模式个人在感觉上是非常常用的一种设计模式，通过将数据的遍历过程和业务逻辑结构，可以在遍历过程中分解出一些共用的遍历方法，提高效率。 反思：目前的感觉迭代器模式的核心思想就是将数据的遍历和处理和调用过程接偶，其原因是遍历过程具有一定的通用性，通过解耦可以更加方便的降低代码的重复率。重要的思想还有数据的桥接方法，使用继承方式还是使用数据引用传递的方式，通过这种思想将业务独立的itertor和真实的业务数据book组合起来 迭代器模式类图 aggregate：这个接口的作用是生成迭代器，也就是说数据的集成类需要集成和实现这个接口，从而可以产生自己的迭代器 iterator：这个接口就是迭代器的接口类，定义了一组迭代器通用的方法 concreteAggregate：这个为aggregate接口的实现类（实际使用中一般把数据的集合类作为这个类的实现） concreteIterator：这个类就是迭代器的实现类，这个类需要针对元素类（就是迭代出来的元素）有针对性的思考要怎么设计 迭代器例子 book 迭代的元素 Aggergate接口 concretaAggregate-Aggergate接口的实现类 这里使用bookself作为创建迭代器的实现类 iterator接口 iterator实现类 主函数 Main 思路： 其实大体上和迭代器原本的思想相同，但是在这个地方使用将bookshef（book元素的实现类）作为aggergate的实现类，在调用的时候将通过本身生成迭代器，然后进行迭代功能的实现，我觉得这里才是迭代器真正的难点和实现痛点-集合和迭代器的通信。 反思：目前的感觉迭代器模式的核心思想就是将数据的遍历和处理和调用过程接偶，其原因是遍历过程具有一定的通用性，通过解耦可以更加方便的降低代码的重复率。重要的思想还有数据的桥接方法，使用继承方式还是使用数据引用传递的方式，通过这种思想将业务独立的itertor和真实的业务数据book组合起来","date":"2024-12-31","categories":["软件开发"],"tags":["设计模式"]},{"title":"GoF设计模式(10)-strategy","url":"/2024/12/31/GoF设计模式-10--strategy/","content":"策略模式这个模式和bridge 模式非常像，如果说区别可能就是bridge强调的是扩展性而stragtegy强调使用方法 策略模式类图 策略模式例子context 策略的使用者 strategy 定义的策略接口 策略接口的实现类 策略的数据载体 主函数","date":"2024-12-31","categories":["软件开发"],"tags":["设计模式"]},{"title":"GoF设计模式(2)-Adapter","url":"/2024/12/31/GoF设计模式-2--Adapter/","content":"适配器模式这个模式个人的感觉是解决了代码特殊部分和通用部分差异性的问题，在处理逻辑中有一部分存在相同的地方但是有部分地方不同的时候可以使用这个方法将相关的不同点进行进一步的适配从而实现代码的复用。 适配器模式类图适配器模式在GAF书中定义了两种 使用引用的方式进行构建 使用继承的方式进行构建 注意： 使用接口和使用抽象类的原因是因为java只能支持单继承，如果业务数据和逻辑操作之间的数据交互使用的集成方法而不是使用引用的方式的时候，将有局限性 适配器模式例子接口和抽象类 使用接口方式进行构建 使用抽象类实现 实现类 接口的实现类，使用集成进行数据桥接 抽象类的实现类，使用引用进行数据桥接 桥接数据 mian 实现类 总结反思：针对之前的迭代器模式，我觉得这个模式处理文章开头说的东西，还有一个重要的思想就是使用继承还是使用数据引用。","date":"2024-12-31","categories":["软件开发"],"tags":["设计模式"]},{"title":"GoF设计模式(3)-Template Method","url":"/2024/12/31/GoF设计模式-3--Template-Method/","content":"模板方法模式这个设计模式个人的感觉，其实实现了一个标准化的流程，实现类是实现标准的类，而流程由一开始的抽先类或者接口定义了，是标准的继承用法 模板方法模式类图 模板方法例子使用抽象类的模板 抽象类的实现类 char 实现类 string 实现类 main 函数","date":"2024-12-31","categories":["软件开发"],"tags":["设计模式"]},{"title":"GoF设计模式(4)-Factory Method","url":"/2024/12/31/GoF设计模式-4--Factory-Method/","content":"工厂方法模式这个模式之前的时候感觉比较困难，现在感觉这个模式同样是比较简单的，要注意一下的几个思想。 工厂方法模式一开始的时候是将工厂接口和工厂生产的产品同时定义出来的。 工厂生产的东西需要传入指定的数据才能生产出来比如下面例子中定义的IDcard类 扩展思路> 其实感觉这个模式其实可以和模板方法模式结合使用，在构建产品的过程中使用模板方法模式可以进一步的解耦和优化流程。 工厂方法模式的类图 从类图中可以看出项目中工厂和产品之间的关系 工厂方法模式例子接口 工厂接口 产品接口 接口的实现类 工厂的实现类 商品的实现类 信息载体 运行主函数","date":"2024-12-31","categories":["软件开发"],"tags":["设计模式"]},{"title":"GoF设计模式(5)-单例模式","url":"/2024/12/31/GoF设计模式-5--单例模式/","content":"单例模式这个模式其实并不需要过多的阐释，重点是如何构建好一个单例模式，最基础的代码如下 以上的方法属于饿汉模式：一开始就生成的实例 其实单例模式还有一种懒加载模式-饿汉模式，不过这种模式有许多的坑 懒加载单例模式最基础样式 这种写法有线程安全问题，比如1线程运行到if语句中2线程也运行到if中，之后将会创建两个对象造成对象不同。 添加线程安全的单例模式 基本思路 synchronized解决 缺点性能低下 synchronized 99% 不需要线程同步 synchronized代码块优化 没有使用全局锁但是还是有问题，代码块同样需要每次运行 双重锁家线程可见 这里使用两个if 来解决99%情况下不需要对if进行线程隔离的情况，使用volatile是为了防止指令重拍续导致的问题，分析如下： 这段代码看起来很完美，很可惜，它是有问题。主要在于instance &#x3D; new Singleton()这句，这并非是一个原子操作，事实上在 JVM 中这句话大概做了下面 3 件事情。 给 instance 分配内存 调用 Singleton 的构造函数来初始化成员变量 将instance对象指向分配的内存空间（执行完这步 instance 就为非 null 了） 但是在 JVM 的即时编译器中存在指令重排序的优化。也就是说上面的第二步和第三步的顺序是不能保证的，最终的执行顺序可能是 1-2-3 也可能是 1-3-2。如果是后者，则在 3 执行完毕、2 未执行之前，被线程二抢占了，这时 instance 已经是非 null 了（但却没有初始化），所以线程二会直接返回 instance，然后使用，然后顺理成章地报错。 使用内部静态类 这种写法仍然使用JVM本身机制保证了线程安全问题；由于 SingletonHolder 是私有的，除了 getInstance() 之外没有办法访问它，因此它是懒汉式的；同时读取实例的时候不会进行同步，没有性能缺陷；也不依赖 JDK 版本。 枚举 Enum用枚举写单例实在太简单了！这也是它最大的优点。下面这段代码就是声明枚举实例的通常做法。 我们可以通过EasySingleton.INSTANCE来访问实例，这比调用getInstance()方法简单多了。创建枚举默认就是线程安全的，所以不需要担心double checked locking，而且还能防止反序列化导致重新创建新的对象。但是还是很少看到有人这样写，可能是因为不太熟悉吧。 总结一般来说，单例模式有五种写法：懒汉、饿汉、双重检验锁、静态内部类、枚举。上述所说都是线程安全的实现，文章开头给出的第一种方法不算正确的写法。","date":"2024-12-31","categories":["软件开发"],"tags":["设计模式"]},{"title":"GoF设计模式(6)-prototype","url":"/2024/12/31/GoF设计模式-6--prototype/","content":"原型模式原型模式其实本质上解决了如下的两个问题： 类名称和类的绑定，无法实现动态的（通过字符串或者名称很方便的）生成想要生成的相关累的实例。 一个统一的入口将相同种类（实现同一个借口或者继承自相同的类的类）进行统一的管理 自我引申： 感觉这个模式，相当于商品的买卖，而圆形模式中的字符串就相当于需要金钱（一种通用的可共识的东西），而产出的类，就相当于商品 原型模式类图 根据上面所说的我的理解可以这么类比 client 相当于用户 users 的过程相当于付款 prototype的createClone 相当于取货的过程 最后 concretePrototype（prototype） 就是商品 通过这样的分析，其实理论上createClone和prototype应该是要解耦的但是，因为java中clone只能自调用所以采用这种方法，其实可以在分装一层调用层，不过这样就增加了复杂度了，这里就不多说了。 方便理解；下面是程序的类图 原型模式例子client 的实现 商品接口的实现 商品接口的实现类 主函数","date":"2024-12-31","categories":["软件开发"],"tags":["设计模式"]},{"title":"GoF设计模式(7)-Builder","url":"/2024/12/31/GoF设计模式-7--Builder/","content":"建造者模式这个模式在我的个人感觉中感觉就像是升级版的模板方法模式，这个模式中的director起到了监控和调用各种builer类的方法（感觉相当于在），然后在director中编写对应的流程函数（construction）来调用指定的相关的各种函数。和模板方法中使用hashmap进行字符串到具体clone 的对应关系相同，在建造者模式中我也使用了HashMap进行获取对象的解耦，实现了统一入口 引申: 其实在director中指定相关的函数并不是一个很好的方法，其实可以实现一个模板接口，通过这个接口动态的实现相关的策略，以后再想想。 建造者模式类图 建造者模式实例Director builer接口 builer 实现类 client 或者主函数","date":"2024-12-31","categories":["软件开发"],"tags":["设计模式"]},{"title":"GoF设计模式(8)abstractFactory","url":"/2024/12/31/GoF设计模式-8-abstractFactory/","content":"","date":"2024-12-31","categories":["软件开发"],"tags":["设计模式"]},{"title":"GoF设计模式(9)-Bridge","url":"/2024/12/31/GoF设计模式-9--Bridge/","content":"桥接模式桥接模式我的感觉这个模式就是使用模板方法模式和适配器的模式的结合。 在GoF书中，将类的扩展分为，类的功能层次划分和类的结构层次划分 类的功能层次划分： 通过继承来对添加相关的功能，其实也就是外部调用的实际方法 类的实现层次划分：父类（接口或者抽象类），通过继承将这个作为一个特殊的实现类适配不同的情况，比如说一个类定义了一个操作系统底层的接口，其子类通过实现，支持了windowns和linux，这里就是类的实现层次划分，针对不同的情况事项不同的接口。 桥接模式本质上就是将类的功能层次和实现层次分离 桥接模式 桥接模式本质上就是将类的功能层次和实现层次分离 Abstraction： 这个类提供的功能是对外的功能，也就是说功能上的扩展就是使用这个类通过继承或者其他的方法就行扩展 Implementor ： 这个类提供的是底层的操作也就是说，这个类只能被Abstraction类使用。 注意：Abstraction的相关方发是使用适配器模式的思想进行调用的，其实本质上是因为Implement是使用引用的方法传入Abstraction类中的，不过这样也好，更加的提高了系统的可扩展性。 适配器模式的代码实现Abstraction 功能层次的扩展方法 Abstraction 的两个实现类，实现不同的功能（startToDisplay）方法 Implement 类的实现层次 类的实现层次的功能 主函数","date":"2024-12-31","categories":["软件开发"],"tags":["设计模式"]},{"title":"linux 终端使用代理","url":"/2024/12/31/linux-终端使用代理/","content":"核心思想 , 改变终端的环境变量终端输入命令","date":"2024-12-31","categories":["运维系统"]},{"title":"微观经济学1","url":"/2024/12/31/微观经济学1/","content":"亚当史密斯 , 国富论大卫李嘉图 , 政治经济学及赋税理论马歇尔 , 边际效应学派和马歇尔这种理论体系 中转 张伯伦,罗宾逊 垄断主义经济凯恩斯主义经济价值论和一般均衡论 货币注意和理性预期分支和扩展(新古典主义和新凯恩斯主义) 影响价格的因素有一个很特殊就是替代品价格 , 和她能进行对比的商品价格","date":"2024-12-31","categories":["通识"],"tags":["微观经济经济学"]},{"title":"categories","url":"/categories/index.html","content":"","date":"2026-02-18"},{"title":"about","url":"/about/index.html","content":"","date":"2025-02-22"},{"title":"tags","url":"/tags/index.html","content":"","date":"2026-02-18"}]